# Lesson 1ï¼šLLM åŸºç¡€ï¼ˆTokenizerã€Decoder-only Transformerã€Attentionã€å‚æ•°é‡ï¼‰

> **ğŸ“Œ è¯¾ç¨‹å®šä½è¯´æ˜**
>
> æœ¬è¯¾ç¨‹ä¸»è¦ä¾§é‡äº **AI Infraï¼ˆåŸºç¡€è®¾æ–½/ç³»ç»Ÿï¼‰** è§’åº¦ï¼Œè€Œéç®—æ³•ç ”ç©¶ã€‚å› æ­¤åœ¨ç®—æ³•åŸç†ä¸Šä¸ä¼šè®²å¾—ç‰¹åˆ«æ·±å…¥ï¼Œå¯¹äº AI Infra å·¥ç¨‹å¸ˆæ¥è¯´ï¼Œåªéœ€äº†è§£å®ç°åŸç†å³å¯ï¼Œä¸å¿…æ·±ç©¶æ•°å­¦æ¨å¯¼ç»†èŠ‚ã€‚

- **Tokenizer**ï¼šæ–‡æœ¬ â†’ token IDï¼ˆæ•°å­—ï¼‰
- **Decoder-only Transformer**ï¼štoken ID â†’ è¯è¡¨ä¸Šçš„ logits
- **ç”Ÿæˆï¼ˆGenerationï¼‰**ï¼šä¸æ–­äº§ç”Ÿä¸‹ä¸€ä¸ª token IDï¼Œå† decode å›æ–‡æœ¬

![LLM æ¨ç†/ç”Ÿæˆæµç¨‹ï¼šencode â†’ decoder-only transformer â†’ decode â†’ è‡ªå›å½’å¾ªç¯](images/llm_flow.png)

---

## æ ¸å¿ƒæ¦‚å¿µ

### 1) Tokenizationï¼štext â†’ tokens â†’ IDs

LLM ä¸æ˜¯ç›´æ¥â€œè¯»æ–‡å­—â€ï¼Œå®ƒè¯»çš„æ˜¯ **token ID**ï¼ˆæ•´æ•°åºåˆ—ï¼‰ã€‚

å…¸å‹æµç¨‹ï¼š

```
Text â”€â”€â–º (Tokenizer) â”€â”€â–º Tokensï¼ˆå­—ç¬¦ä¸²ç‰‡æ®µï¼‰ â”€â”€â–º Token IDsï¼ˆæ•´æ•°ï¼‰
```

ä¸ºä»€ä¹ˆ tokenizer å¾ˆé‡è¦ï¼š

- **ä¸Šä¸‹æ–‡é•¿åº¦**æŒ‰ token æ•°è®¡ç®—ï¼Œè€Œä¸æ˜¯å­—ç¬¦æ•°
- **é€Ÿåº¦/åå**å¸¸å¸¸æŒ‰â€œæ¯ç”Ÿæˆ 1 ä¸ª token çš„æˆæœ¬â€è¡¡é‡
- token çš„åˆ‡åˆ†è¾¹ç•Œä¼šå½±å“æ¨¡å‹è¡¨è¾¾ï¼ˆäººåã€ä»£ç ã€ä¸åŒè¯­è¨€ç­‰ï¼‰


ä½ å¯ä»¥æŠŠ tokenizer ç†è§£æˆä¸€ä¸ªâ€œ**å¯é€†çš„å­—å…¸å‹ç¼©å™¨**â€ï¼š

![Tokenizerï¼šencode / decode ç¤ºæ„å›¾](images/tokenizer.png)

- **Vocabularyï¼ˆè¯è¡¨ï¼‰**ï¼šä¸€ä¸ªå›ºå®šå­—å…¸ï¼Œåˆ—å‡ºå…è®¸å‡ºç°çš„â€œç‰‡æ®µâ€ï¼ˆtokenï¼‰
- **Encode**ï¼šæŠŠæ–‡æœ¬æ‹†æˆè¿™äº›ç‰‡æ®µï¼Œå¹¶æŠŠæ¯ä¸ªç‰‡æ®µæ˜ å°„æˆæ•°å­—ï¼ˆtoken IDï¼‰
- **Decode**ï¼šæŠŠ token ID å†æ˜ å°„å›ç‰‡æ®µï¼Œå¹¶æ‹¼å›æ–‡æœ¬ï¼ˆå› æ­¤å®ƒéœ€è¦å°½é‡å¯é€†ã€ç¨³å®šï¼‰

ä¸ºä»€ä¹ˆä¸ç›´æ¥æŒ‰â€œå­—ç¬¦â€æˆ–â€œè¯â€ï¼Ÿ

- **å­—ç¬¦çº§**ï¼šåºåˆ—ä¼šå˜å¾ˆé•¿ï¼ˆtoken æ•°æ›´å¤šï¼‰ï¼Œæ³¨æ„åŠ›è®¡ç®—æ›´è´µï¼Œç”Ÿæˆæ›´æ…¢
- **è¯çº§**ï¼šè¯è¡¨ä¼šçˆ†ç‚¸ï¼ˆæ–°è¯ã€æ‹¼å†™å˜åŒ–ã€äººåã€ä»£ç æ ‡è¯†ç¬¦ï¼‰ï¼Œè¿˜ä¼šé¢‘ç¹é‡åˆ° OOVï¼ˆè¯è¡¨å¤–ï¼‰é—®é¢˜

å®é™…å·¥ç¨‹é‡Œå¸¸ç”¨çš„æŠ˜ä¸­æ˜¯ **å­è¯/ç‰‡æ®µï¼ˆsubword piecesï¼‰**ï¼š

- å¸¸è§ç‰‡æ®µå˜æˆä¸€ä¸ª tokenï¼ˆæ›´çŸ­çš„åºåˆ—ï¼‰
- ç½•è§è¯å¯ä»¥ç”±å¤šä¸ªç‰‡æ®µç»„åˆï¼ˆé¿å…å®Œå…¨ OOVï¼‰
- è¯è¡¨è§„æ¨¡å¯æ§ï¼ˆå¸¸è§æ˜¯å‡ ä¸‡åˆ°åå‡ ä¸‡ï¼‰

æ­¤å¤–è¿˜æœ‰ä¸€ç±»éå¸¸é‡è¦ï¼š**ç‰¹æ®Š tokenï¼ˆspecial tokensï¼‰**ï¼Œæ¯”å¦‚ï¼š

- `<bos>` / `<eos>`ï¼šå¥å­å¼€å§‹/ç»“æŸ
- `<pad>`ï¼šbatch å¯¹é½ç”¨çš„å¡«å……
- ä»¥åŠèŠå¤©æ¨¡å‹æ¨¡æ¿ï¼ˆroleã€åˆ†éš”ç¬¦ç­‰ï¼‰

ä½ åé¢ä¼šåå¤é‡åˆ°ä¸€å¥è¯ï¼š**ä¸Šä¸‹æ–‡é•¿åº¦ã€KV cache å¤§å°ã€ååï¼ˆtokens/sï¼‰åŸºæœ¬éƒ½æŒ‰ token æ•°æ¥ç®—**ã€‚

---

### 2) Embeddingï¼štoken ID â†’ å‘é‡

Tokenizer è¾“å‡ºçš„æ˜¯æ•´æ•°åºåˆ—ï¼ˆtoken IDsï¼‰ï¼Œä½†ç¥ç»ç½‘ç»œéœ€è¦çš„æ˜¯**è¿ç»­å‘é‡**ã€‚Embedding å±‚å°±æ˜¯è¿™ä¸ªæ¡¥æ¢ã€‚

```
Token IDs: [1024, 5678, 42]
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embedding æŸ¥è¡¨                  â”‚
â”‚  (vocab_size Ã— hidden_size)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Hidden States: [[0.1, -0.2, ...], [0.3, 0.5, ...], [0.2, 0.1, ...]]
               å½¢çŠ¶: (seq_len, hidden_size)
```

**æ ¸å¿ƒå®ç°**ï¼š

```python
class Embedding(nn.Module):
    def __init__(self, vocab_size: int, hidden_size: int):
        super().__init__()
        # åˆ›å»ºä¸€ä¸ªå¯å­¦ä¹ çš„æŸ¥æ‰¾è¡¨
        self.weight = nn.Parameter(torch.randn(vocab_size, hidden_size))

    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:
        # token_ids: (batch_size, seq_len)
        # è¿”å›: (batch_size, seq_len, hidden_size)
        return self.weight[token_ids]
```

**å‚æ•°é‡**ï¼š`vocab_size Ã— hidden_size`

ä»¥ Qwen3-8B ä¸ºä¾‹ï¼š`151936 Ã— 4096 â‰ˆ 622M` å‚æ•°ï¼ˆçº¦å æ¨¡å‹çš„ 7%ï¼‰

**å…³é”®ç‚¹**ï¼š
- Embedding æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª"æŸ¥è¡¨"æ“ä½œï¼Œä¸æ¶‰åŠçŸ©é˜µä¹˜æ³•
- æ¯ä¸ª token ID å¯¹åº”ä¸€ä¸ªå›ºå®šçš„å‘é‡ï¼ˆè®­ç»ƒæ—¶å­¦ä¹ å¾—åˆ°ï¼‰
- è¾“å‡ºå½¢çŠ¶ä» `(batch, seq_len)` å˜ä¸º `(batch, seq_len, hidden_size)`

---

### 3) LayerNorm ä¸ RMSNormï¼šå±‚å½’ä¸€åŒ–

Normalizationï¼ˆå½’ä¸€åŒ–ï¼‰æ˜¯æ·±åº¦å­¦ä¹ è®­ç»ƒç¨³å®šçš„å…³é”®æŠ€æœ¯ã€‚æ²¡æœ‰å½’ä¸€åŒ–ï¼Œæ·±å±‚ç½‘ç»œå¾ˆå®¹æ˜“å‡ºç°æ¢¯åº¦çˆ†ç‚¸æˆ–æ¢¯åº¦æ¶ˆå¤±ã€‚

#### ä¸ºä»€ä¹ˆéœ€è¦å½’ä¸€åŒ–ï¼Ÿ

ç¥ç»ç½‘ç»œæ¯ä¸€å±‚çš„è¾“å‡ºåˆ†å¸ƒä¼šéšç€è®­ç»ƒä¸æ–­å˜åŒ–ï¼ˆInternal Covariate Shiftï¼‰ï¼Œè¿™ä¼šå¯¼è‡´ï¼š
- åç»­å±‚éœ€è¦ä¸æ–­é€‚åº”æ–°çš„è¾“å…¥åˆ†å¸ƒ
- è®­ç»ƒä¸ç¨³å®šï¼Œéœ€è¦æ›´å°çš„å­¦ä¹ ç‡
- æ”¶æ•›é€Ÿåº¦å˜æ…¢

å½’ä¸€åŒ–çš„ç›®æ ‡ï¼š**å°†æ¯ä¸€å±‚çš„è¾“å‡º"æ‹‰å›"åˆ°ç¨³å®šçš„åˆ†å¸ƒ**ï¼ˆå‡å€¼â‰ˆ0ï¼Œæ–¹å·®â‰ˆ1ï¼‰ã€‚

#### LayerNorm è¯¦è§£

**æ ¸å¿ƒå…¬å¼**ï¼š

å¯¹äºè¾“å…¥å‘é‡ $x = [x_1, x_2, ..., x_d]$ï¼ˆd æ˜¯ hidden_sizeï¼‰ï¼š

$$
\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$

å…¶ä¸­ï¼š
- $\mu = \frac{1}{d}\sum_{i=1}^{d} x_i$ï¼ˆå‡å€¼ï¼‰
- $\sigma^2 = \frac{1}{d}\sum_{i=1}^{d} (x_i - \mu)^2$ï¼ˆæ–¹å·®ï¼‰
- $\gamma$ï¼ˆscaleï¼‰å’Œ $\beta$ï¼ˆshiftï¼‰æ˜¯å¯å­¦ä¹ å‚æ•°
- $\epsilon$ æ˜¯é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°ï¼ˆå¦‚ 1e-6ï¼‰

**LayerNorm æ ¸å¿ƒå®ç°**ï¼š

```python
class LayerNorm(nn.Module):
    def __init__(self, hidden_size: int, eps: float = 1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(hidden_size))   # ç¼©æ”¾å‚æ•°
        self.beta = nn.Parameter(torch.zeros(hidden_size))   # å¹³ç§»å‚æ•°
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch, seq_len, hidden_size)

        # 1. è®¡ç®—å‡å€¼ (åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Š)
        mean = x.mean(dim=-1, keepdim=True)

        # 2. è®¡ç®—æ–¹å·®
        var = x.var(dim=-1, keepdim=True, unbiased=False)

        # 3. å½’ä¸€åŒ–: (x - mean) / sqrt(var + eps)
        x_norm = (x - mean) / torch.sqrt(var + self.eps)

        # 4. ç¼©æ”¾å’Œå¹³ç§»: gamma * x_norm + beta
        return self.gamma * x_norm + self.beta
```

**å‚æ•°é‡**ï¼š`2 Ã— hidden_size`ï¼ˆgamma å’Œ beta å„ hidden_size ä¸ªï¼‰

#### ä¸ºä»€ä¹ˆ Î³ å’Œ Î² å¾ˆé‡è¦ï¼Ÿ

å¦‚æœåªåšå½’ä¸€åŒ–ï¼ˆå¼ºåˆ¶å‡å€¼=0ï¼Œæ–¹å·®=1ï¼‰ï¼Œä¼šé™åˆ¶ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ã€‚é€šè¿‡å¯å­¦ä¹ çš„ Î³ å’Œ Î²ï¼š

- ç½‘ç»œå¯ä»¥"å­¦ä¹ "æ¢å¤åŸå§‹åˆ†å¸ƒï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
- å½“ Î³=Ïƒ, Î²=Î¼ æ—¶ï¼Œç›¸å½“äºæ’ç­‰å˜æ¢ï¼ˆä»€ä¹ˆéƒ½ä¸åšï¼‰
- ç½‘ç»œå¯ä»¥åœ¨"å½’ä¸€åŒ–"å’Œ"ä¿æŒåŸæ ·"ä¹‹é—´è‡ªç”±é€‰æ‹©

#### RMSNormï¼šæ›´ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆ

ç°ä»£ LLMï¼ˆå¦‚ Llamaã€Qwenã€Mistralï¼‰æ™®éä½¿ç”¨ **RMSNorm** æ›¿ä»£ LayerNormã€‚

**æ ¸å¿ƒå…¬å¼**ï¼š

\[
\text{RMSNorm}(x) = \gamma \cdot \frac{x}{\text{RMS}(x)}
\]

å…¶ä¸­ï¼š
\[
\text{RMS}(x) = \sqrt{\frac{1}{d}\sum_{i=1}^{d} x_i^2 + \epsilon}
\]

**å…³é”®åŒºåˆ«**ï¼š

| ç‰¹æ€§ | LayerNorm | RMSNorm |
|------|-----------|---------|
| å‡å‡å€¼ï¼ˆä¸­å¿ƒåŒ–ï¼‰ | âœ… æ˜¯ | âŒ å¦ |
| é™¤æ ‡å‡†å·® | âœ… æ˜¯ | âŒ å¦ï¼ˆé™¤ RMSï¼‰ |
| åŠ åç½® Î² | âœ… æ˜¯ | âŒ å¦ |
| å‚æ•°é‡ | 2d | d |
| è®¡ç®—é‡ | è¾ƒå¤š | è¾ƒå°‘ |

**ä¸ºä»€ä¹ˆ RMSNorm æœ‰æ•ˆ**ï¼š

ç ”ç©¶è¡¨æ˜ï¼ŒLayerNorm çš„ä¸»è¦ä½œç”¨æ¥è‡ª**ç¼©æ”¾**ï¼ˆé™¤ä»¥æŸä¸ªç»Ÿè®¡é‡ï¼‰ï¼Œè€Œä¸æ˜¯**ä¸­å¿ƒåŒ–**ï¼ˆå‡å‡å€¼ï¼‰ã€‚RMSNorm å»æ‰äº†ä¸­å¿ƒåŒ–æ­¥éª¤ï¼Œä½†ä¿ç•™äº†æ ¸å¿ƒçš„ç¼©æ”¾ä½œç”¨ï¼ŒåŒæ—¶ï¼š
- å‡å°‘ ~50% çš„è®¡ç®—é‡
- å‡å°‘ 50% çš„å‚æ•°é‡
- å®éªŒæ•ˆæœç›¸å½“ç”šè‡³æ›´å¥½

**RMSNorm æ ¸å¿ƒå®ç°**ï¼š

```python
class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization"""
    def __init__(self, hidden_size: int, eps: float = 1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))  # gamma
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch, seq_len, hidden_size)
        # 1. è®¡ç®— RMS: sqrt(mean(x^2))
        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)
        # 2. å½’ä¸€åŒ–å¹¶ç¼©æ”¾
        return (x / rms) * self.weight
```

**å‚æ•°é‡**ï¼š`hidden_size`ï¼ˆåªæœ‰ gammaï¼Œæ²¡æœ‰ betaï¼‰

---

### 4) Q/K/V ç”Ÿæˆï¼ˆLinear æŠ•å½±ï¼‰

Attention çš„æ ¸å¿ƒæ˜¯ Queryã€Keyã€Value ä¸‰ä¸ªå‘é‡ã€‚å®ƒä»¬é€šè¿‡**çº¿æ€§æŠ•å½±**ä»è¾“å…¥ hidden states ç”Ÿæˆã€‚

```
Hidden States (batch, seq_len, hidden_size)
        â”‚
        â”œâ”€â”€â–º Wq â”€â”€â–º Q (Query)   : "æˆ‘åœ¨æ‰¾ä»€ä¹ˆ"
        â”‚
        â”œâ”€â”€â–º Wk â”€â”€â–º K (Key)     : "æˆ‘æœ‰ä»€ä¹ˆ"
        â”‚
        â””â”€â”€â–º Wv â”€â”€â–º V (Value)   : "æˆ‘çš„å†…å®¹æ˜¯ä»€ä¹ˆ"
```

**æ ¸å¿ƒå®ç°**ï¼š

```python
# ç®€åŒ–ç‰ˆï¼ˆä¸è€ƒè™‘å¤šå¤´ï¼‰
self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
self.k_proj = nn.Linear(hidden_size, hidden_size, bias=False)
self.v_proj = nn.Linear(hidden_size, hidden_size, bias=False)

# å‰å‘ä¼ æ’­
Q = self.q_proj(hidden_states)  # (batch, seq_len, hidden_size)
K = self.k_proj(hidden_states)
V = self.v_proj(hidden_states)
```

**Multi-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰**ï¼š

å®é™…ä¸Šæˆ‘ä»¬ä¼šæŠŠ hidden_size æ‹†æˆå¤šä¸ª"å¤´"ï¼š

```python
# Qwen3-8B é…ç½®
hidden_size = 4096
num_heads = 32
head_dim = hidden_size // num_heads  # = 128

# Q å®é™…å½¢çŠ¶
Q: (batch, seq_len, hidden_size)
   â”€â”€reshapeâ”€â”€â–º (batch, seq_len, num_heads, head_dim)
   â”€â”€transposeâ”€â”€â–º (batch, num_heads, seq_len, head_dim)
```

**Grouped Query Attention (GQA)**ï¼š

ç°ä»£ LLMï¼ˆå¦‚ Llama2-70Bã€Qwen3ï¼‰ä½¿ç”¨ GQA æ¥å‡å°‘ KV cache å¤§å°ï¼š

```
MHA:  32 ä¸ª Q heads, 32 ä¸ª K heads, 32 ä¸ª V heads
MQA:  32 ä¸ª Q heads,  1 ä¸ª K head,   1 ä¸ª V head
GQA:  32 ä¸ª Q heads,  8 ä¸ª K heads,  8 ä¸ª V headsï¼ˆQwen3-8Bï¼‰
                      â†‘
              æ¯ 4 ä¸ª Q heads å…±äº«ä¸€ç»„ KV
```

```python
# GQA å®ç°
self.q_proj = nn.Linear(hidden_size, num_heads * head_dim)       # 32 * 128 = 4096
self.k_proj = nn.Linear(hidden_size, num_kv_heads * head_dim)    # 8 * 128 = 1024
self.v_proj = nn.Linear(hidden_size, num_kv_heads * head_dim)    # 8 * 128 = 1024
```

**å‚æ•°é‡**ï¼š
- Q: `hidden_size Ã— (num_heads Ã— head_dim)` = `4096 Ã— 4096` â‰ˆ 16.8M
- K: `hidden_size Ã— (num_kv_heads Ã— head_dim)` = `4096 Ã— 1024` â‰ˆ 4.2M
- V: `hidden_size Ã— (num_kv_heads Ã— head_dim)` = `4096 Ã— 1024` â‰ˆ 4.2M
- O (è¾“å‡ºæŠ•å½±): `4096 Ã— 4096` â‰ˆ 16.8M
- **æ¯å±‚ Attention æ€»è®¡**ï¼šçº¦ 42M å‚æ•°

---

### 5) RoPEï¼šæ—‹è½¬ä½ç½®ç¼–ç 

Transformer çš„æ ¸å¿ƒæ“ä½œï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰æœ¬èº«æ˜¯**ä½ç½®æ— å…³**çš„â€”â€”æ‰“ä¹±è¾“å…¥é¡ºåºï¼Œè¾“å‡ºä¹Ÿåªæ˜¯ç›¸åº”æ‰“ä¹±ã€‚ä¸ºäº†è®©æ¨¡å‹ç†è§£"è°åœ¨å‰ã€è°åœ¨å"ï¼Œæˆ‘ä»¬éœ€è¦æ³¨å…¥ä½ç½®ä¿¡æ¯ã€‚

**ä½ç½®ç¼–ç çš„æ¼”è¿›**ï¼š

```
ç»å¯¹ä½ç½®ç¼–ç  (GPT-1/2)     â†’  å­¦ä¹ å›ºå®šä½ç½®å‘é‡ï¼Œç›´æ¥åŠ åˆ° embedding
æ­£å¼¦ä½ç½®ç¼–ç  (Transformer)  â†’  ç”¨ sin/cos ç”Ÿæˆä½ç½®å‘é‡ï¼ŒåŠ åˆ° embedding
ç›¸å¯¹ä½ç½®ç¼–ç  (T5, ALiBi)    â†’  åœ¨ attention åˆ†æ•°ä¸ŠåŠ åç½®
RoPE (Llama, Qwen, ...)    â†’  æ—‹è½¬ Q å’Œ K å‘é‡ â† ç°ä»£ä¸»æµ
```

**RoPE çš„æ ¸å¿ƒæ€æƒ³**ï¼š

æŠŠä½ç½®ä¿¡æ¯"æ—‹è½¬"è¿› Q å’Œ K å‘é‡ä¸­ï¼Œä½¿å¾—ä¸¤ä¸ªä½ç½® m å’Œ n çš„å‘é‡ç‚¹ç§¯è‡ªç„¶åŒ…å«å®ƒä»¬çš„**ç›¸å¯¹è·ç¦»** (m-n)ã€‚

```
ä½ç½® 0 çš„å‘é‡ï¼šä¸æ—‹è½¬
ä½ç½® 1 çš„å‘é‡ï¼šæ—‹è½¬ Î¸ åº¦
ä½ç½® 2 çš„å‘é‡ï¼šæ—‹è½¬ 2Î¸ åº¦
ä½ç½® m çš„å‘é‡ï¼šæ—‹è½¬ mÃ—Î¸ åº¦
```

![RoPE](images/rope.png)

**æ•°å­¦åŸç†**ï¼š

å¯¹äº 2D å‘é‡ï¼Œæ—‹è½¬çŸ©é˜µä¸ºï¼š

```
[cos(mÎ¸)  -sin(mÎ¸)]   [xâ‚€]   [xâ‚€Â·cos(mÎ¸) - xâ‚Â·sin(mÎ¸)]
[sin(mÎ¸)   cos(mÎ¸)] Ã— [xâ‚] = [xâ‚€Â·sin(mÎ¸) + xâ‚Â·cos(mÎ¸)]
```

å¯¹äºé«˜ç»´å‘é‡ï¼ˆå¦‚ head_dim=128ï¼‰ï¼Œæˆ‘ä»¬æŠŠå®ƒåˆ†æˆ 64 å¯¹ï¼Œæ¯å¯¹ä½¿ç”¨ä¸åŒé¢‘ç‡çš„ Î¸ã€‚

**è®ºæ–‡ä¸­çš„æ•°å­¦æè¿°** vs **PyTorch å®é™…å®ç°**ï¼š

è®ºæ–‡ä¸­æè¿°çš„æ˜¯ç›¸é‚»ç»´åº¦é…å¯¹ï¼š
```
ç»´åº¦ 0,1:  ä½¿ç”¨ Î¸â‚€ = base^(-0/d)     â†’ é«˜é¢‘ï¼Œå˜åŒ–å¿«
ç»´åº¦ 2,3:  ä½¿ç”¨ Î¸â‚ = base^(-2/d)     â†’
ç»´åº¦ 4,5:  ä½¿ç”¨ Î¸â‚‚ = base^(-4/d)     â†’
   ...
ç»´åº¦ 126,127: ä½¿ç”¨ Î¸â‚†â‚ƒ = base^(-126/d) â†’ ä½é¢‘ï¼Œå˜åŒ–æ…¢
```

ä½† **PyTorch å®é™…å®ç°æ˜¯å‰ååŠé…å¯¹**ï¼ˆä»¥ head_dim=4 ä¸ºä¾‹ï¼‰ï¼š
```
è®ºæ–‡æè¿°:    (0,1) ç”¨ Î¸â‚€,  (2,3) ç”¨ Î¸â‚
PyTorchå®ç°: (0,2) ç”¨ Î¸â‚€,  (1,3) ç”¨ Î¸â‚
```

è¿™æ˜¯å› ä¸º `rotate_half` å‡½æ•°å°†å‘é‡åˆ†æˆå‰åä¸¤åŠï¼Œè€Œä¸æ˜¯äº¤é”™å–ç›¸é‚»ç»´åº¦ï¼š
```python
x1 = x[..., :2]   # [xâ‚€, xâ‚] å‰åŠ
x2 = x[..., 2:]   # [xâ‚‚, xâ‚ƒ] ååŠ
rotate_half(x) = [-xâ‚‚, -xâ‚ƒ, xâ‚€, xâ‚]
```

**ä¸¤ç§æ–¹å¼æ•°å­¦ä¸Šå®Œå…¨ç­‰ä»·**ï¼Œåªæ˜¯ç»´åº¦æ’åˆ—ä¸åŒã€‚PyTorch è¿™æ ·å®ç°æ˜¯ä¸ºäº†åˆ©ç”¨è¿ç»­å†…å­˜è®¿é—®ï¼Œé¿å…äº¤é”™ç´¢å¼•å¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚

å…¶ä¸­ `base` é€šå¸¸æ˜¯ 10000ï¼ˆåŸå§‹ Transformerï¼‰æˆ– 1000000ï¼ˆQwen3 é•¿ä¸Šä¸‹æ–‡ï¼‰ã€‚

**ä¸ºä»€ä¹ˆ RoPE èƒ½ç¼–ç ç›¸å¯¹ä½ç½®**ï¼š

å½“è®¡ç®— Q_m Â· K_n æ—¶ï¼ˆä½ç½® m çš„ Q å’Œä½ç½® n çš„ K çš„ç‚¹ç§¯ï¼‰ï¼š

```
Q_m Â· K_n = (R(mÎ¸) Â· q) Â· (R(nÎ¸) Â· k)
          = q Â· R((m-n)Î¸) Â· k    â† åªä¾èµ–ç›¸å¯¹è·ç¦» (m-n)ï¼
```

è¿™æ„å‘³ç€æ¨¡å‹"çœ‹åˆ°"çš„æ˜¯ä¸¤ä¸ª token ä¹‹é—´çš„è·ç¦»ï¼Œè€Œä¸æ˜¯å®ƒä»¬çš„ç»å¯¹ä½ç½®ã€‚

**æ ¸å¿ƒå®ç°**ï¼š

```python
class RotaryEmbedding(nn.Module):
    def __init__(self, head_dim: int, base: float = 10000.0):
        super().__init__()
        # è®¡ç®—æ¯å¯¹ç»´åº¦çš„é¢‘ç‡: Î¸_i = base^(-2i/d)
        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))
        self.register_buffer("inv_freq", inv_freq)

    def forward(self, seq_len: int, device: torch.device):
        # ä½ç½®ç´¢å¼•: [0, 1, 2, ..., seq_len-1]
        t = torch.arange(seq_len, device=device, dtype=torch.float32)

        # è®¡ç®—æ¯ä¸ªä½ç½®ã€æ¯ä¸ªé¢‘ç‡çš„è§’åº¦: (seq_len, head_dim/2)
        freqs = torch.outer(t, self.inv_freq)

        # å¤åˆ¶ä»¥åŒ¹é… head_dim: (seq_len, head_dim)
        emb = torch.cat((freqs, freqs), dim=-1)

        return emb.cos(), emb.sin()
```

**åº”ç”¨ RoPE åˆ° Q å’Œ K**ï¼š

```python
def apply_rotary_pos_emb(q, k, cos, sin):
    """
    q, k: (batch, num_heads, seq_len, head_dim)
    cos, sin: (seq_len, head_dim)
    """
    def rotate_half(x):
        """å°†ååŠéƒ¨åˆ†å–è´Ÿå¹¶ä¸å‰åŠéƒ¨åˆ†äº¤æ¢"""
        x1 = x[..., : x.shape[-1] // 2]
        x2 = x[..., x.shape[-1] // 2 :]
        return torch.cat((-x2, x1), dim=-1)

    # æ—‹è½¬å…¬å¼: x * cos + rotate_half(x) * sin
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)

    return q_embed, k_embed
```

**åœ¨ Attention ä¸­çš„ä½ç½®**ï¼š

```
Q_proj â”€â”€â–º Q â”€â”€â–º RoPE â”€â”€â”
                        â”œâ”€â”€â–º Attention Score â”€â”€â–º ...
K_proj â”€â”€â–º K â”€â”€â–º RoPE â”€â”€â”˜

V_proj â”€â”€â–º V â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º ...
                    ï¼ˆV ä¸éœ€è¦ RoPEï¼‰
```

**RoPE çš„ä¼˜åŠ¿**ï¼š

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **ç›¸å¯¹ä½ç½®** | ç‚¹ç§¯è‡ªç„¶åŒ…å«ç›¸å¯¹è·ç¦»ï¼Œæ— éœ€æ˜¾å¼è®¡ç®— |
| **å¤–æ¨èƒ½åŠ›** | ç†è®ºä¸Šå¯å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„é•¿åº¦ |
| **é›¶å‚æ•°é‡** | ä¸å¢åŠ å¯å­¦ä¹ å‚æ•° |
| **è®¡ç®—é«˜æ•ˆ** | åªéœ€ç®€å•çš„ä¹˜æ³•å’ŒåŠ æ³• |
| **å…¼å®¹æ€§å¥½** | å¯ä¸ Flash Attention ç­‰ä¼˜åŒ–æŠ€æœ¯ç»“åˆ |

**é•¿ä¸Šä¸‹æ–‡æ‰©å±•**ï¼š

Qwen3 ä½¿ç”¨ `base=1000000`ï¼ˆè€ŒéåŸå§‹çš„ 10000ï¼‰ï¼Œä½¿ä½é¢‘æˆåˆ†å˜åŒ–æ›´æ…¢ï¼Œä»è€Œæ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼ˆ40K+ tokensï¼‰ã€‚è¿™ç§æŠ€æœ¯ç§°ä¸º **NTK-aware scaling** æˆ– **Dynamic NTK**ã€‚

---

### 6) Attentionï¼šæ³¨æ„åŠ›è®¡ç®—

æœ‰äº† Qã€Kã€V ä¹‹åï¼Œå°±å¯ä»¥è®¡ç®—æ³¨æ„åŠ›äº†ï¼š

```
         Q        K^T           Softmax          V
      (seq, d) Ã— (d, seq)  â†’  (seq, seq)  Ã—  (seq, d)  â†’  (seq, d)
         â”‚          â”‚            â”‚             â”‚
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â”‚             â”‚
              â–¼                  â”‚             â”‚
           scores               â–¼             â”‚
         (seq, seq)          weights          â”‚
              â”‚             (seq, seq)         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                            output
                           (seq, d)
```

**æ ¸å¿ƒå…¬å¼**ï¼š

\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
\]

**æ ¸å¿ƒå®ç°**ï¼š

```python
def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Q, K, V: (batch, num_heads, seq_len, head_dim)
    """
    d_k = Q.shape[-1]

    # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    # scores: (batch, num_heads, seq_len, seq_len)

    # 2. åº”ç”¨å› æœæ©ç ï¼ˆautoregressive generationï¼‰
    if mask is not None:
        scores = scores + mask  # mask ä¸­æœªæ¥ä½ç½®æ˜¯ -inf

    # 3. Softmax å½’ä¸€åŒ–
    attn_weights = F.softmax(scores, dim=-1)
    # attn_weights: (batch, num_heads, seq_len, seq_len)

    # 4. åŠ æƒæ±‚å’Œ
    output = torch.matmul(attn_weights, V)
    # output: (batch, num_heads, seq_len, head_dim)

    return output, attn_weights
```

**å› æœæ©ç ï¼ˆCausal Maskï¼‰**ï¼š

å¯¹äºè‡ªå›å½’ç”Ÿæˆï¼Œä½ç½® t åªèƒ½çœ‹åˆ°ä½ç½® 0~tï¼Œä¸èƒ½çœ‹åˆ°æœªæ¥ï¼š

```python
def causal_mask(seq_len):
    """
    è¿”å›:
    [[  0, -inf, -inf, -inf],
     [  0,    0, -inf, -inf],
     [  0,    0,    0, -inf],
     [  0,    0,    0,    0]]
    """
    mask = torch.full((seq_len, seq_len), float("-inf"))
    mask = torch.triu(mask, diagonal=1)
    return mask
```

**ä¸ºä»€ä¹ˆè¦é™¤ä»¥ âˆšd_k**ï¼š

å¦‚æœä¸ç¼©æ”¾ï¼Œå½“ d_k å¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯å€¼ä¼šå¾ˆå¤§ï¼Œsoftmax ä¼šå˜å¾—éå¸¸"å°–é”"ï¼ˆæ¥è¿‘ one-hotï¼‰ï¼Œæ¢¯åº¦æ¥è¿‘ 0ã€‚é™¤ä»¥ âˆšd_k å¯ä»¥ç¨³å®šè®­ç»ƒã€‚

**å¤æ‚åº¦**ï¼šO(nÂ²) å…¶ä¸­ n æ˜¯åºåˆ—é•¿åº¦â€”â€”è¿™æ˜¯é•¿ä¸Šä¸‹æ–‡çš„ä¸»è¦ç“¶é¢ˆã€‚

---

### 7) MLPï¼ˆå‰é¦ˆç½‘ç»œï¼‰

æ¯ä¸ª Transformer å±‚çš„å¦ä¸€åŠæ˜¯ MLPï¼ˆä¹Ÿå« FFNï¼‰ã€‚å®ƒå¯¹æ¯ä¸ª token ä½ç½®ç‹¬ç«‹åšéçº¿æ€§å˜æ¢ã€‚

**ä¼ ç»Ÿ FFN**ï¼š

```
x â”€â”€â–º Linear(dâ†’4d) â”€â”€â–º ReLU â”€â”€â–º Linear(4dâ†’d) â”€â”€â–º output
```

**ç°ä»£ LLM ä½¿ç”¨ SwiGLU**ï¼ˆQwen3ã€Llamaï¼‰ï¼š

```
        â”Œâ”€â”€â–º gate_proj â”€â”€â–º SiLU â”€â”€â”€â”
        â”‚                          â”‚
x â”€â”€â”€â”€â”€â”€â”¤                          â”œâ”€â”€â–º é€å…ƒç´ ä¹˜ â”€â”€â–º down_proj â”€â”€â–º output
        â”‚                          â”‚
        â””â”€â”€â–º up_proj â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒå®ç°**ï¼š

```python
class MLP(nn.Module):
    def __init__(self, hidden_size: int, intermediate_size: int):
        super().__init__()
        # Qwen3-8B: hidden_size=4096, intermediate_size=14336
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)

    def forward(self, x):
        # SwiGLU: silu(gate(x)) * up(x)
        gate = F.silu(self.gate_proj(x))  # SiLU = x * sigmoid(x)
        up = self.up_proj(x)
        return self.down_proj(gate * up)
```

**å‚æ•°é‡**ï¼ˆæ¯å±‚ MLPï¼‰ï¼š
- gate_proj: `4096 Ã— 14336` â‰ˆ 58.7M
- up_proj: `4096 Ã— 14336` â‰ˆ 58.7M
- down_proj: `14336 Ã— 4096` â‰ˆ 58.7M
- **æ¯å±‚ MLP æ€»è®¡**ï¼šçº¦ 176M å‚æ•°

**å…³é”®ç‚¹**ï¼šMLP é€šå¸¸å æ¨¡å‹å‚æ•°é‡çš„ **60-70%**ï¼

---

### 8) å®Œæ•´çš„ Transformer å±‚

æŠŠä¸Šé¢çš„ç»„ä»¶ç»„åˆèµ·æ¥ï¼Œä¸€ä¸ªå®Œæ•´çš„ Transformer å±‚ï¼ˆPre-Norm æ¶æ„ï¼‰å¦‚ä¸‹ï¼š

```
Input Hidden States
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ RMSNorm â”‚ â†â”€â”€ input_layernorm
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚           Self-Attention             â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
   â”‚  â”‚ Q/K/V Proj â”€â”€â–º RoPE â”€â”€â–º Attn â”‚    â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
   â”‚               â”‚                      â”‚
   â”‚               â–¼ O Proj               â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€ + Residual â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                   â”‚
        â–¼                                   â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
   â”‚ RMSNorm â”‚ â†â”€â”€ post_attention_layernorm â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                              â”‚
        â”‚                                   â”‚
        â–¼                                   â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
   â”‚      MLP        â”‚                      â”‚
   â”‚ (SwiGLU: 3å±‚)   â”‚                      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
            â”‚                               â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€ + Residual â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
     Output Hidden States
```

**ä»£ç å®ç°**ï¼š

```python
class TransformerBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.input_layernorm = RMSNorm(config.hidden_size)
        self.self_attn = Attention(config)
        self.post_attention_layernorm = RMSNorm(config.hidden_size)
        self.mlp = MLP(config)

    def forward(self, x, attention_mask=None, position_ids=None):
        # 1. Self-Attention (with residual)
        residual = x
        x = self.input_layernorm(x)
        x = self.self_attn(x, attention_mask, position_ids)
        x = residual + x

        # 2. MLP (with residual)
        residual = x
        x = self.post_attention_layernorm(x)
        x = self.mlp(x)
        x = residual + x

        return x
```

---

### 9) å‚æ•°é‡ä»å“ªé‡Œæ¥ï¼Ÿ

ä»¥ Qwen3-8B ä¸ºä¾‹ï¼Œæ¯å±‚å‚æ•°é‡åˆ†å¸ƒï¼š

| ç»„ä»¶ | å‚æ•°é‡ | å æ¯” |
|------|--------|------|
| Q Proj | 16.8M | 7.5% |
| K Proj | 4.2M | 1.9% |
| V Proj | 4.2M | 1.9% |
| O Proj | 16.8M | 7.5% |
| **Attention å°è®¡** | **42M** | **18.8%** |
| Gate Proj | 58.7M | 26.3% |
| Up Proj | 58.7M | 26.3% |
| Down Proj | 58.7M | 26.3% |
| **MLP å°è®¡** | **176M** | **78.9%** |
| RMSNorm Ã—2 | 8K | ~0% |

**ç»“è®º**ï¼šMLP å äº†æ¯å±‚å‚æ•°çš„ ~80%ï¼

**å®Œæ•´ 8B æ¨¡å‹å‚æ•°åˆ†å¸ƒ**ï¼š
- Embedding: ~622M (7%)
- 36 Ã— Transformer Layer: ~7.8B (92%)
- Final RMSNorm + LM Head: ~626M (1%)

---
## å‚è€ƒèµ„æ–™

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)


