# Lesson 1ï¼šLLM åŸºç¡€ï¼ˆTokenizerã€Decoder-only Transformerã€Attentionã€å‚æ•°é‡ï¼‰

<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 12px; margin: 20px 0;">
<h2 style="color: white; margin: 0 0 12px 0; font-size: 1.5em;">ğŸ“Œ è¯¾ç¨‹å®šä½è¯´æ˜</h2>
<p style="color: white; font-size: 1.2em; line-height: 1.6; margin: 0;">
æœ¬è¯¾ç¨‹ä¸»è¦ä¾§é‡äº <strong>AI Infraï¼ˆåŸºç¡€è®¾æ–½/ç³»ç»Ÿï¼‰</strong> è§’åº¦ï¼Œè€Œéç®—æ³•ç ”ç©¶ã€‚å› æ­¤åœ¨ç®—æ³•åŸç†ä¸Šä¸ä¼šè®²å¾—ç‰¹åˆ«æ·±å…¥ï¼Œå¯¹äº AI Infra å·¥ç¨‹å¸ˆæ¥è¯´ï¼Œåªéœ€äº†è§£å®ç°åŸç†å³å¯ï¼Œä¸å¿…æ·±ç©¶æ•°å­¦æ¨å¯¼ç»†èŠ‚ã€‚
</p>
<p style="color: #e0e0e0; font-size: 1.1em; line-height: 1.6; margin: 12px 0 0 0;">
ğŸ”– æœ¬è¯¾ç¨‹ä»¥ <strong style="color: #ffd700;">Qwen3-8B</strong> çš„ç½‘ç»œæ¶æ„ä¸ºä¾‹è¿›è¡Œè®²è§£ï¼Œå¸®åŠ©å¤§å®¶ç†è§£ç°ä»£ LLM çš„æ ¸å¿ƒç»„ä»¶å’Œå®ç°ç»†èŠ‚ã€‚
</p>
</div>

- **Tokenizer**ï¼šæ–‡æœ¬ â†’ token IDï¼ˆæ•°å­—ï¼‰
- **Decoder-only Transformer**ï¼štoken ID â†’ è¯è¡¨ä¸Šçš„ logits
- **ç”Ÿæˆï¼ˆGenerationï¼‰**ï¼šä¸æ–­äº§ç”Ÿä¸‹ä¸€ä¸ª token IDï¼Œå† decode å›æ–‡æœ¬

![LLM æ¨ç†/ç”Ÿæˆæµç¨‹ï¼šencode â†’ decoder-only transformer â†’ decode â†’ è‡ªå›å½’å¾ªç¯](images/llm_flow.png)

---

## æ ¸å¿ƒæ¦‚å¿µ

### 1) Tokenizationï¼štext â†’ tokens â†’ IDs

LLM ä¸æ˜¯ç›´æ¥â€œè¯»æ–‡å­—â€ï¼Œå®ƒè¯»çš„æ˜¯ **token ID**ï¼ˆæ•´æ•°åºåˆ—ï¼‰ã€‚

å…¸å‹æµç¨‹ï¼š

```
Text â”€â”€â–º (Tokenizer) â”€â”€â–º Tokensï¼ˆå­—ç¬¦ä¸²ç‰‡æ®µï¼‰ â”€â”€â–º Token IDsï¼ˆæ•´æ•°ï¼‰
```

ä¸ºä»€ä¹ˆ tokenizer å¾ˆé‡è¦ï¼š

- **ä¸Šä¸‹æ–‡é•¿åº¦**æŒ‰ token æ•°è®¡ç®—ï¼Œè€Œä¸æ˜¯å­—ç¬¦æ•°
- **é€Ÿåº¦/åå**å¸¸å¸¸æŒ‰â€œæ¯ç”Ÿæˆ 1 ä¸ª token çš„æˆæœ¬â€è¡¡é‡
- token çš„åˆ‡åˆ†è¾¹ç•Œä¼šå½±å“æ¨¡å‹è¡¨è¾¾ï¼ˆäººåã€ä»£ç ã€ä¸åŒè¯­è¨€ç­‰ï¼‰


ä½ å¯ä»¥æŠŠ tokenizer ç†è§£æˆä¸€ä¸ªâ€œ**å¯é€†çš„å­—å…¸å‹ç¼©å™¨**â€ï¼š

![Tokenizerï¼šencode / decode ç¤ºæ„å›¾](images/tokenizer.png)

- **Vocabularyï¼ˆè¯è¡¨ï¼‰**ï¼šä¸€ä¸ªå›ºå®šå­—å…¸ï¼Œåˆ—å‡ºå…è®¸å‡ºç°çš„â€œç‰‡æ®µâ€ï¼ˆtokenï¼‰
- **Encode**ï¼šæŠŠæ–‡æœ¬æ‹†æˆè¿™äº›ç‰‡æ®µï¼Œå¹¶æŠŠæ¯ä¸ªç‰‡æ®µæ˜ å°„æˆæ•°å­—ï¼ˆtoken IDï¼‰
- **Decode**ï¼šæŠŠ token ID å†æ˜ å°„å›ç‰‡æ®µï¼Œå¹¶æ‹¼å›æ–‡æœ¬ï¼ˆå› æ­¤å®ƒéœ€è¦å°½é‡å¯é€†ã€ç¨³å®šï¼‰

ä¸ºä»€ä¹ˆä¸ç›´æ¥æŒ‰â€œå­—ç¬¦â€æˆ–â€œè¯â€ï¼Ÿ

- **å­—ç¬¦çº§**ï¼šåºåˆ—ä¼šå˜å¾ˆé•¿ï¼ˆtoken æ•°æ›´å¤šï¼‰ï¼Œæ³¨æ„åŠ›è®¡ç®—æ›´è´µï¼Œç”Ÿæˆæ›´æ…¢
- **è¯çº§**ï¼šè¯è¡¨ä¼šçˆ†ç‚¸ï¼ˆæ–°è¯ã€æ‹¼å†™å˜åŒ–ã€äººåã€ä»£ç æ ‡è¯†ç¬¦ï¼‰ï¼Œè¿˜ä¼šé¢‘ç¹é‡åˆ° OOVï¼ˆè¯è¡¨å¤–ï¼‰é—®é¢˜

å®é™…å·¥ç¨‹é‡Œå¸¸ç”¨çš„æŠ˜ä¸­æ˜¯ **å­è¯/ç‰‡æ®µï¼ˆsubword piecesï¼‰**ï¼š

- å¸¸è§ç‰‡æ®µå˜æˆä¸€ä¸ª tokenï¼ˆæ›´çŸ­çš„åºåˆ—ï¼‰
- ç½•è§è¯å¯ä»¥ç”±å¤šä¸ªç‰‡æ®µç»„åˆï¼ˆé¿å…å®Œå…¨ OOVï¼‰
- è¯è¡¨è§„æ¨¡å¯æ§ï¼ˆå¸¸è§æ˜¯å‡ ä¸‡åˆ°åå‡ ä¸‡ï¼‰

æ­¤å¤–è¿˜æœ‰ä¸€ç±»éå¸¸é‡è¦ï¼š**ç‰¹æ®Š tokenï¼ˆspecial tokensï¼‰**ï¼Œæ¯”å¦‚ï¼š

- `<bos>` / `<eos>`ï¼šå¥å­å¼€å§‹/ç»“æŸ
- `<pad>`ï¼šbatch å¯¹é½ç”¨çš„å¡«å……
- ä»¥åŠèŠå¤©æ¨¡å‹æ¨¡æ¿ï¼ˆroleã€åˆ†éš”ç¬¦ç­‰ï¼‰

ä½ åé¢ä¼šåå¤é‡åˆ°ä¸€å¥è¯ï¼š**ä¸Šä¸‹æ–‡é•¿åº¦ã€KV cache å¤§å°ã€ååï¼ˆtokens/sï¼‰åŸºæœ¬éƒ½æŒ‰ token æ•°æ¥ç®—**ã€‚

---

### 2) Embeddingï¼štoken ID â†’ å‘é‡

Tokenizer è¾“å‡ºçš„æ˜¯æ•´æ•°åºåˆ—ï¼ˆtoken IDsï¼‰ï¼Œä½†ç¥ç»ç½‘ç»œéœ€è¦çš„æ˜¯**è¿ç»­å‘é‡**ã€‚Embedding å±‚å°±æ˜¯è¿™ä¸ªæ¡¥æ¢ã€‚

```
Token IDs: [1024, 5678, 42]
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embedding æŸ¥è¡¨                  â”‚
â”‚  (vocab_size Ã— hidden_size)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Hidden States: [[0.1, -0.2, ...], [0.3, 0.5, ...], [0.2, 0.1, ...]]
               å½¢çŠ¶: (seq_len, hidden_size)
```

**æ ¸å¿ƒå®ç°**ï¼š

```python
class Embedding(nn.Module):
    def __init__(self, vocab_size: int, hidden_size: int):
        super().__init__()
        # åˆ›å»ºä¸€ä¸ªå¯å­¦ä¹ çš„æŸ¥æ‰¾è¡¨
        self.weight = nn.Parameter(torch.randn(vocab_size, hidden_size))

    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:
        # token_ids: (batch_size, seq_len)
        # è¿”å›: (batch_size, seq_len, hidden_size)
        return self.weight[token_ids]
```

**å‚æ•°é‡**ï¼š`vocab_size Ã— hidden_size`

ä»¥ Qwen3-8B ä¸ºä¾‹ï¼š`151936 Ã— 4096 â‰ˆ 622M` å‚æ•°ï¼ˆçº¦å æ¨¡å‹çš„ 7%ï¼‰

**å…³é”®ç‚¹**ï¼š
- Embedding æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª"æŸ¥è¡¨"æ“ä½œï¼Œä¸æ¶‰åŠçŸ©é˜µä¹˜æ³•
- æ¯ä¸ª token ID å¯¹åº”ä¸€ä¸ªå›ºå®šçš„å‘é‡ï¼ˆè®­ç»ƒæ—¶å­¦ä¹ å¾—åˆ°ï¼‰
- è¾“å‡ºå½¢çŠ¶ä» `(batch, seq_len)` å˜ä¸º `(batch, seq_len, hidden_size)`

---

### 3) LayerNorm ä¸ RMSNormï¼šå±‚å½’ä¸€åŒ–

Normalizationï¼ˆå½’ä¸€åŒ–ï¼‰æ˜¯æ·±åº¦å­¦ä¹ è®­ç»ƒç¨³å®šçš„å…³é”®æŠ€æœ¯ã€‚æ²¡æœ‰å½’ä¸€åŒ–ï¼Œæ·±å±‚ç½‘ç»œå¾ˆå®¹æ˜“å‡ºç°æ¢¯åº¦çˆ†ç‚¸æˆ–æ¢¯åº¦æ¶ˆå¤±ã€‚

#### ä¸ºä»€ä¹ˆéœ€è¦å½’ä¸€åŒ–ï¼Ÿ

ç¥ç»ç½‘ç»œæ¯ä¸€å±‚çš„è¾“å‡ºåˆ†å¸ƒä¼šéšç€è®­ç»ƒä¸æ–­å˜åŒ–ï¼ˆInternal Covariate Shiftï¼‰ï¼Œè¿™ä¼šå¯¼è‡´ï¼š
- åç»­å±‚éœ€è¦ä¸æ–­é€‚åº”æ–°çš„è¾“å…¥åˆ†å¸ƒ
- è®­ç»ƒä¸ç¨³å®šï¼Œéœ€è¦æ›´å°çš„å­¦ä¹ ç‡
- æ”¶æ•›é€Ÿåº¦å˜æ…¢

å½’ä¸€åŒ–çš„ç›®æ ‡ï¼š**å°†æ¯ä¸€å±‚çš„è¾“å‡º"æ‹‰å›"åˆ°ç¨³å®šçš„åˆ†å¸ƒ**ï¼ˆå‡å€¼â‰ˆ0ï¼Œæ–¹å·®â‰ˆ1ï¼‰ã€‚

#### LayerNorm è¯¦è§£

**æ ¸å¿ƒå…¬å¼**ï¼š

å¯¹äºè¾“å…¥å‘é‡ $x = [x_1, x_2, ..., x_d]$ï¼ˆd æ˜¯ hidden_sizeï¼‰ï¼š

$$
\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$

å…¶ä¸­ï¼š
- $\mu = \frac{1}{d}\sum_{i=1}^{d} x_i$ï¼ˆå‡å€¼ï¼‰
- $\sigma^2 = \frac{1}{d}\sum_{i=1}^{d} (x_i - \mu)^2$ï¼ˆæ–¹å·®ï¼‰
- $\gamma$ï¼ˆscaleï¼‰å’Œ $\beta$ï¼ˆshiftï¼‰æ˜¯å¯å­¦ä¹ å‚æ•°
- $\epsilon$ æ˜¯é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°ï¼ˆå¦‚ 1e-6ï¼‰

**LayerNorm æ ¸å¿ƒå®ç°**ï¼š

```python
class LayerNorm(nn.Module):
    def __init__(self, hidden_size: int, eps: float = 1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(hidden_size))   # ç¼©æ”¾å‚æ•°
        self.beta = nn.Parameter(torch.zeros(hidden_size))   # å¹³ç§»å‚æ•°
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch, seq_len, hidden_size)

        # 1. è®¡ç®—å‡å€¼ (åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Š)
        mean = x.mean(dim=-1, keepdim=True)

        # 2. è®¡ç®—æ–¹å·®
        var = x.var(dim=-1, keepdim=True, unbiased=False)

        # 3. å½’ä¸€åŒ–: (x - mean) / sqrt(var + eps)
        x_norm = (x - mean) / torch.sqrt(var + self.eps)

        # 4. ç¼©æ”¾å’Œå¹³ç§»: gamma * x_norm + beta
        return self.gamma * x_norm + self.beta
```

**å‚æ•°é‡**ï¼š`2 Ã— hidden_size`ï¼ˆgamma å’Œ beta å„ hidden_size ä¸ªï¼‰

#### ä¸ºä»€ä¹ˆ Î³ å’Œ Î² å¾ˆé‡è¦ï¼Ÿ

å¦‚æœåªåšå½’ä¸€åŒ–ï¼ˆå¼ºåˆ¶å‡å€¼=0ï¼Œæ–¹å·®=1ï¼‰ï¼Œä¼šé™åˆ¶ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ã€‚é€šè¿‡å¯å­¦ä¹ çš„ Î³ å’Œ Î²ï¼š

- ç½‘ç»œå¯ä»¥"å­¦ä¹ "æ¢å¤åŸå§‹åˆ†å¸ƒï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
- å½“ Î³=Ïƒ, Î²=Î¼ æ—¶ï¼Œç›¸å½“äºæ’ç­‰å˜æ¢ï¼ˆä»€ä¹ˆéƒ½ä¸åšï¼‰
- ç½‘ç»œå¯ä»¥åœ¨"å½’ä¸€åŒ–"å’Œ"ä¿æŒåŸæ ·"ä¹‹é—´è‡ªç”±é€‰æ‹©

#### RMSNormï¼šæ›´ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆ

ç°ä»£ LLMï¼ˆå¦‚ Llamaã€Qwenã€Mistralï¼‰æ™®éä½¿ç”¨ **RMSNorm** æ›¿ä»£ LayerNormã€‚

**æ ¸å¿ƒå…¬å¼**ï¼š

$$
\text{RMSNorm}(x) = \gamma \cdot \frac{x}{\text{RMS}(x)}
$$

å…¶ä¸­ï¼š

$$
\text{RMS}(x) = \sqrt{\frac{1}{d}\sum_{i=1}^{d} x_i^2 + \epsilon}
$$

**å…³é”®åŒºåˆ«**ï¼š

| ç‰¹æ€§ | LayerNorm | RMSNorm |
|------|-----------|---------|
| å‡å‡å€¼ï¼ˆä¸­å¿ƒåŒ–ï¼‰ | âœ… æ˜¯ | âŒ å¦ |
| é™¤æ ‡å‡†å·® | âœ… æ˜¯ | âŒ å¦ï¼ˆé™¤ RMSï¼‰ |
| åŠ åç½® Î² | âœ… æ˜¯ | âŒ å¦ |
| å‚æ•°é‡ | 2d | d |
| è®¡ç®—é‡ | è¾ƒå¤š | è¾ƒå°‘ |

**ä¸ºä»€ä¹ˆ RMSNorm æœ‰æ•ˆ**ï¼š

ç ”ç©¶è¡¨æ˜ï¼ŒLayerNorm çš„ä¸»è¦ä½œç”¨æ¥è‡ª**ç¼©æ”¾**ï¼ˆé™¤ä»¥æŸä¸ªç»Ÿè®¡é‡ï¼‰ï¼Œè€Œä¸æ˜¯**ä¸­å¿ƒåŒ–**ï¼ˆå‡å‡å€¼ï¼‰ã€‚RMSNorm å»æ‰äº†ä¸­å¿ƒåŒ–æ­¥éª¤ï¼Œä½†ä¿ç•™äº†æ ¸å¿ƒçš„ç¼©æ”¾ä½œç”¨ï¼ŒåŒæ—¶ï¼š
- å‡å°‘ ~50% çš„è®¡ç®—é‡
- å‡å°‘ 50% çš„å‚æ•°é‡
- å®éªŒæ•ˆæœç›¸å½“ç”šè‡³æ›´å¥½

**RMSNorm æ ¸å¿ƒå®ç°**ï¼š

```python
class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization"""
    def __init__(self, hidden_size: int, eps: float = 1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))  # gamma
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch, seq_len, hidden_size)
        # 1. è®¡ç®— RMS: sqrt(mean(x^2))
        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)
        # 2. å½’ä¸€åŒ–å¹¶ç¼©æ”¾
        return (x / rms) * self.weight
```

**å‚æ•°é‡**ï¼š`hidden_size`ï¼ˆåªæœ‰ gammaï¼Œæ²¡æœ‰ betaï¼‰

---

### 4) Q/K/V ç”Ÿæˆï¼ˆLinear æŠ•å½±ï¼‰

Attention çš„æ ¸å¿ƒæ˜¯ Queryã€Keyã€Value ä¸‰ä¸ªå‘é‡ã€‚å®ƒä»¬é€šè¿‡**çº¿æ€§æŠ•å½±**ä»è¾“å…¥ hidden states ç”Ÿæˆã€‚

```
Hidden States (batch, seq_len, hidden_size)
        â”‚
        â”œâ”€â”€â–º Wq â”€â”€â–º Q (Query)   : "æˆ‘åœ¨æ‰¾ä»€ä¹ˆ"
        â”‚
        â”œâ”€â”€â–º Wk â”€â”€â–º K (Key)     : "æˆ‘æœ‰ä»€ä¹ˆ"
        â”‚
        â””â”€â”€â–º Wv â”€â”€â–º V (Value)   : "æˆ‘çš„å†…å®¹æ˜¯ä»€ä¹ˆ"
```

#### Linear å±‚åŸºç¡€ï¼šçŸ©é˜µä¹˜æ³•

åœ¨æ·±å…¥ Q/K/V ä¹‹å‰ï¼Œå…ˆç†è§£ **Linear å±‚**ï¼ˆä¹Ÿå«å…¨è¿æ¥å±‚ã€Dense å±‚ï¼‰çš„æœ¬è´¨ã€‚

**æ•°å­¦å®šä¹‰**ï¼š

$$
y = xW^T + b
$$

å…¶ä¸­ï¼š
- $x$ï¼šè¾“å…¥å‘é‡ï¼Œå½¢çŠ¶ `(*, in_features)`
- $W$ï¼šæƒé‡çŸ©é˜µï¼Œå½¢çŠ¶ `(out_features, in_features)`
- $b$ï¼šåç½®å‘é‡ï¼Œå½¢çŠ¶ `(out_features)`ï¼ˆå¯é€‰ï¼‰
- $y$ï¼šè¾“å‡ºå‘é‡ï¼Œå½¢çŠ¶ `(*, out_features)`

**çŸ©é˜µä¹˜æ³•å›¾è§£**ï¼š

```
è¾“å…¥ x              æƒé‡ W^T              è¾“å‡º y
(1, in_features)   (in_features, out)   (1, out_features)

[xâ‚€ xâ‚ xâ‚‚ xâ‚ƒ]  Ã—  [wâ‚€â‚€ wâ‚€â‚ wâ‚€â‚‚]   =   [yâ‚€ yâ‚ yâ‚‚]
                   [wâ‚â‚€ wâ‚â‚ wâ‚â‚‚]
                   [wâ‚‚â‚€ wâ‚‚â‚ wâ‚‚â‚‚]
                   [wâ‚ƒâ‚€ wâ‚ƒâ‚ wâ‚ƒâ‚‚]

æ¯ä¸ªè¾“å‡ºå…ƒç´ :
yâ‚€ = xâ‚€Â·wâ‚€â‚€ + xâ‚Â·wâ‚â‚€ + xâ‚‚Â·wâ‚‚â‚€ + xâ‚ƒÂ·wâ‚ƒâ‚€  (è¾“å…¥ä¸ç¬¬ 0 åˆ—ç‚¹ç§¯)
yâ‚ = xâ‚€Â·wâ‚€â‚ + xâ‚Â·wâ‚â‚ + xâ‚‚Â·wâ‚‚â‚ + xâ‚ƒÂ·wâ‚ƒâ‚  (è¾“å…¥ä¸ç¬¬ 1 åˆ—ç‚¹ç§¯)
yâ‚‚ = xâ‚€Â·wâ‚€â‚‚ + xâ‚Â·wâ‚â‚‚ + xâ‚‚Â·wâ‚‚â‚‚ + xâ‚ƒÂ·wâ‚ƒâ‚‚  (è¾“å…¥ä¸ç¬¬ 2 åˆ—ç‚¹ç§¯)
```

**PyTorch å®ç°**ï¼š

```python
import torch.nn as nn

# åˆ›å»ºä¸€ä¸ª Linear å±‚ï¼š4 ç»´è¾“å…¥ â†’ 3 ç»´è¾“å‡º
linear = nn.Linear(in_features=4, out_features=3, bias=False)

# æŸ¥çœ‹æƒé‡å½¢çŠ¶
print(linear.weight.shape)  # torch.Size([3, 4])  å³ (out_features, in_features)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 5, 4)    # (batch=2, seq_len=5, in_features=4)
y = linear(x)               # (batch=2, seq_len=5, out_features=3)
```

**å…³é”®ç†è§£**ï¼š

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| **çº¿æ€§å˜æ¢** | Linear å±‚æœ¬è´¨æ˜¯å¯¹è¾“å…¥åšçº¿æ€§å˜æ¢ï¼ˆæ—‹è½¬ã€ç¼©æ”¾ã€æŠ•å½±ï¼‰ |
| **å‚æ•°é‡** | `in_features Ã— out_features`ï¼ˆ+ out_features å¦‚æœæœ‰ biasï¼‰ |
| **æ— æ¿€æ´»å‡½æ•°** | Linear æœ¬èº«ä¸å«éçº¿æ€§ï¼Œéœ€é…åˆ ReLU/SiLU ç­‰ä½¿ç”¨ |
| **æ‰¹é‡å¤„ç†** | å¯¹ batch å’Œ seq_len ç»´åº¦ç‹¬ç«‹ä½œç”¨ï¼Œåªå˜æ¢æœ€åä¸€ç»´ |

**åœ¨ LLM ä¸­çš„åº”ç”¨**ï¼š

```python
# Q/K/V æŠ•å½±å°±æ˜¯ 3 ä¸ª Linear å±‚
self.q_proj = nn.Linear(4096, 4096, bias=False)  # hidden â†’ Q
self.k_proj = nn.Linear(4096, 1024, bias=False)  # hidden â†’ K (GQA: æ›´å°)
self.v_proj = nn.Linear(4096, 1024, bias=False)  # hidden â†’ V (GQA: æ›´å°)

# è®¡ç®—é‡ï¼šæ¯ä¸ª token éœ€è¦ in Ã— out æ¬¡ä¹˜åŠ æ“ä½œ
# Q: 4096 Ã— 4096 = 16.7M FLOPs/token
```

**æ ¸å¿ƒå®ç°**ï¼š

```python
# ç®€åŒ–ç‰ˆï¼ˆä¸è€ƒè™‘å¤šå¤´ï¼‰
self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
self.k_proj = nn.Linear(hidden_size, hidden_size, bias=False)
self.v_proj = nn.Linear(hidden_size, hidden_size, bias=False)

# å‰å‘ä¼ æ’­
Q = self.q_proj(hidden_states)  # (batch, seq_len, hidden_size)
K = self.k_proj(hidden_states)
V = self.v_proj(hidden_states)
```

**Multi-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰**ï¼š

å®é™…ä¸Šæˆ‘ä»¬ä¼šæŠŠ hidden_size æ‹†æˆå¤šä¸ª"å¤´"ï¼š

```python
# Qwen3-8B é…ç½®
hidden_size = 4096
num_heads = 32
head_dim = hidden_size // num_heads  # = 128

# Q å®é™…å½¢çŠ¶
Q: (batch, seq_len, hidden_size)
   â”€â”€reshapeâ”€â”€â–º (batch, seq_len, num_heads, head_dim)
   â”€â”€transposeâ”€â”€â–º (batch, num_heads, seq_len, head_dim)
```

**Grouped Query Attention (GQA)**ï¼š

ç°ä»£ LLMï¼ˆå¦‚ Llama2-70Bã€Qwen3ï¼‰ä½¿ç”¨ GQA æ¥å‡å°‘ KV cache å¤§å°ï¼š

```
MHA:  32 ä¸ª Q heads, 32 ä¸ª K heads, 32 ä¸ª V heads
MQA:  32 ä¸ª Q heads,  1 ä¸ª K head,   1 ä¸ª V head
GQA:  32 ä¸ª Q heads,  8 ä¸ª K heads,  8 ä¸ª V headsï¼ˆQwen3-8Bï¼‰
                      â†‘
              æ¯ 4 ä¸ª Q heads å…±äº«ä¸€ç»„ KV
```

```python
# GQA å®ç°
self.q_proj = nn.Linear(hidden_size, num_heads * head_dim)       # 32 * 128 = 4096
self.k_proj = nn.Linear(hidden_size, num_kv_heads * head_dim)    # 8 * 128 = 1024
self.v_proj = nn.Linear(hidden_size, num_kv_heads * head_dim)    # 8 * 128 = 1024
```

**å‚æ•°é‡**ï¼š
- Q: `hidden_size Ã— (num_heads Ã— head_dim)` = `4096 Ã— 4096` â‰ˆ 16.8M
- K: `hidden_size Ã— (num_kv_heads Ã— head_dim)` = `4096 Ã— 1024` â‰ˆ 4.2M
- V: `hidden_size Ã— (num_kv_heads Ã— head_dim)` = `4096 Ã— 1024` â‰ˆ 4.2M
- O (è¾“å‡ºæŠ•å½±): `4096 Ã— 4096` â‰ˆ 16.8M
- **æ¯å±‚ Attention æ€»è®¡**ï¼šçº¦ 42M å‚æ•°

---

### 5) RoPEï¼šæ—‹è½¬ä½ç½®ç¼–ç 

Transformer çš„æ ¸å¿ƒæ“ä½œï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰æœ¬èº«æ˜¯**ä½ç½®æ— å…³**çš„â€”â€”æ‰“ä¹±è¾“å…¥é¡ºåºï¼Œè¾“å‡ºä¹Ÿåªæ˜¯ç›¸åº”æ‰“ä¹±ã€‚ä¸ºäº†è®©æ¨¡å‹ç†è§£"è°åœ¨å‰ã€è°åœ¨å"ï¼Œæˆ‘ä»¬éœ€è¦æ³¨å…¥ä½ç½®ä¿¡æ¯ã€‚

**ä½ç½®ç¼–ç çš„æ¼”è¿›**ï¼š

```
ç»å¯¹ä½ç½®ç¼–ç  (GPT-1/2)     â†’  å­¦ä¹ å›ºå®šä½ç½®å‘é‡ï¼Œç›´æ¥åŠ åˆ° embedding
æ­£å¼¦ä½ç½®ç¼–ç  (Transformer)  â†’  ç”¨ sin/cos ç”Ÿæˆä½ç½®å‘é‡ï¼ŒåŠ åˆ° embedding
ç›¸å¯¹ä½ç½®ç¼–ç  (T5, ALiBi)    â†’  åœ¨ attention åˆ†æ•°ä¸ŠåŠ åç½®
RoPE (Llama, Qwen, ...)    â†’  æ—‹è½¬ Q å’Œ K å‘é‡ â† ç°ä»£ä¸»æµ
```

**RoPE çš„æ ¸å¿ƒæ€æƒ³**ï¼š

æŠŠä½ç½®ä¿¡æ¯"æ—‹è½¬"è¿› Q å’Œ K å‘é‡ä¸­ï¼Œä½¿å¾—ä¸¤ä¸ªä½ç½® m å’Œ n çš„å‘é‡ç‚¹ç§¯è‡ªç„¶åŒ…å«å®ƒä»¬çš„**ç›¸å¯¹è·ç¦»** (m-n)ã€‚

```
ä½ç½® 0 çš„å‘é‡ï¼šä¸æ—‹è½¬
ä½ç½® 1 çš„å‘é‡ï¼šæ—‹è½¬ Î¸ åº¦
ä½ç½® 2 çš„å‘é‡ï¼šæ—‹è½¬ 2Î¸ åº¦
ä½ç½® m çš„å‘é‡ï¼šæ—‹è½¬ mÃ—Î¸ åº¦
```

![RoPE](images/rope.png)

**æ•°å­¦åŸç†**ï¼š

å¯¹äº 2D å‘é‡ï¼Œæ—‹è½¬çŸ©é˜µä¸ºï¼š

```
[cos(mÎ¸)  -sin(mÎ¸)]   [xâ‚€]   [xâ‚€Â·cos(mÎ¸) - xâ‚Â·sin(mÎ¸)]
[sin(mÎ¸)   cos(mÎ¸)] Ã— [xâ‚] = [xâ‚€Â·sin(mÎ¸) + xâ‚Â·cos(mÎ¸)]
```

å¯¹äºé«˜ç»´å‘é‡ï¼ˆå¦‚ head_dim=128ï¼‰ï¼Œæˆ‘ä»¬æŠŠå®ƒåˆ†æˆ 64 å¯¹ï¼Œæ¯å¯¹ä½¿ç”¨ä¸åŒé¢‘ç‡çš„ Î¸ã€‚

**è®ºæ–‡ä¸­çš„æ•°å­¦æè¿°** vs **PyTorch å®é™…å®ç°**ï¼š

è®ºæ–‡ä¸­æè¿°çš„æ˜¯ç›¸é‚»ç»´åº¦é…å¯¹ï¼š
```
ç»´åº¦ 0,1:  ä½¿ç”¨ Î¸â‚€ = base^(-0/d)     â†’ é«˜é¢‘ï¼Œå˜åŒ–å¿«
ç»´åº¦ 2,3:  ä½¿ç”¨ Î¸â‚ = base^(-2/d)     â†’
ç»´åº¦ 4,5:  ä½¿ç”¨ Î¸â‚‚ = base^(-4/d)     â†’
   ...
ç»´åº¦ 126,127: ä½¿ç”¨ Î¸â‚†â‚ƒ = base^(-126/d) â†’ ä½é¢‘ï¼Œå˜åŒ–æ…¢
```

ä½† **PyTorch å®é™…å®ç°æ˜¯å‰ååŠé…å¯¹**ï¼ˆä»¥ head_dim=4 ä¸ºä¾‹ï¼‰ï¼š
```
è®ºæ–‡æè¿°:    (0,1) ç”¨ Î¸â‚€,  (2,3) ç”¨ Î¸â‚
PyTorchå®ç°: (0,2) ç”¨ Î¸â‚€,  (1,3) ç”¨ Î¸â‚
```

è¿™æ˜¯å› ä¸º `rotate_half` å‡½æ•°å°†å‘é‡åˆ†æˆå‰åä¸¤åŠï¼Œè€Œä¸æ˜¯äº¤é”™å–ç›¸é‚»ç»´åº¦ï¼š
```python
x1 = x[..., :2]   # [xâ‚€, xâ‚] å‰åŠ
x2 = x[..., 2:]   # [xâ‚‚, xâ‚ƒ] ååŠ
rotate_half(x) = [-xâ‚‚, -xâ‚ƒ, xâ‚€, xâ‚]
```

**ä¸¤ç§æ–¹å¼æ•°å­¦ä¸Šå®Œå…¨ç­‰ä»·**ï¼Œåªæ˜¯ç»´åº¦æ’åˆ—ä¸åŒã€‚PyTorch è¿™æ ·å®ç°æ˜¯ä¸ºäº†åˆ©ç”¨è¿ç»­å†…å­˜è®¿é—®ï¼Œé¿å…äº¤é”™ç´¢å¼•å¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚

å…¶ä¸­ `base` é€šå¸¸æ˜¯ 10000ï¼ˆåŸå§‹ Transformerï¼‰æˆ– 1000000ï¼ˆQwen3 é•¿ä¸Šä¸‹æ–‡ï¼‰ã€‚

**ä¸ºä»€ä¹ˆ RoPE èƒ½ç¼–ç ç›¸å¯¹ä½ç½®**ï¼š

å½“è®¡ç®— Q_m Â· K_n æ—¶ï¼ˆä½ç½® m çš„ Q å’Œä½ç½® n çš„ K çš„ç‚¹ç§¯ï¼‰ï¼š

```
Q_m Â· K_n = (R(mÎ¸) Â· q) Â· (R(nÎ¸) Â· k)
          = q Â· R((m-n)Î¸) Â· k    â† åªä¾èµ–ç›¸å¯¹è·ç¦» (m-n)ï¼
```

è¿™æ„å‘³ç€æ¨¡å‹"çœ‹åˆ°"çš„æ˜¯ä¸¤ä¸ª token ä¹‹é—´çš„è·ç¦»ï¼Œè€Œä¸æ˜¯å®ƒä»¬çš„ç»å¯¹ä½ç½®ã€‚

**æ ¸å¿ƒå®ç°**ï¼š

```python
class RotaryEmbedding(nn.Module):
    def __init__(self, head_dim: int, base: float = 10000.0):
        super().__init__()
        # è®¡ç®—æ¯å¯¹ç»´åº¦çš„é¢‘ç‡: Î¸_i = base^(-2i/d)
        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))
        self.register_buffer("inv_freq", inv_freq)

    def forward(self, seq_len: int, device: torch.device):
        # ä½ç½®ç´¢å¼•: [0, 1, 2, ..., seq_len-1]
        t = torch.arange(seq_len, device=device, dtype=torch.float32)

        # è®¡ç®—æ¯ä¸ªä½ç½®ã€æ¯ä¸ªé¢‘ç‡çš„è§’åº¦: (seq_len, head_dim/2)
        freqs = torch.outer(t, self.inv_freq)

        # å¤åˆ¶ä»¥åŒ¹é… head_dim: (seq_len, head_dim)
        emb = torch.cat((freqs, freqs), dim=-1)

        return emb.cos(), emb.sin()
```

**åº”ç”¨ RoPE åˆ° Q å’Œ K**ï¼š

```python
def apply_rotary_pos_emb(q, k, cos, sin):
    """
    q, k: (batch, num_heads, seq_len, head_dim)
    cos, sin: (seq_len, head_dim)
    """
    def rotate_half(x):
        """å°†ååŠéƒ¨åˆ†å–è´Ÿå¹¶ä¸å‰åŠéƒ¨åˆ†äº¤æ¢"""
        x1 = x[..., : x.shape[-1] // 2]
        x2 = x[..., x.shape[-1] // 2 :]
        return torch.cat((-x2, x1), dim=-1)

    # æ—‹è½¬å…¬å¼: x * cos + rotate_half(x) * sin
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)

    return q_embed, k_embed
```

**åœ¨ Attention ä¸­çš„ä½ç½®**ï¼š

```
Q_proj â”€â”€â–º Q â”€â”€â–º RoPE â”€â”€â”
                        â”œâ”€â”€â–º Attention Score â”€â”€â–º ...
K_proj â”€â”€â–º K â”€â”€â–º RoPE â”€â”€â”˜

V_proj â”€â”€â–º V â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º ...
                    ï¼ˆV ä¸éœ€è¦ RoPEï¼‰
```

**RoPE çš„ä¼˜åŠ¿**ï¼š

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **ç›¸å¯¹ä½ç½®** | ç‚¹ç§¯è‡ªç„¶åŒ…å«ç›¸å¯¹è·ç¦»ï¼Œæ— éœ€æ˜¾å¼è®¡ç®— |
| **å¤–æ¨èƒ½åŠ›** | ç†è®ºä¸Šå¯å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„é•¿åº¦ |
| **é›¶å‚æ•°é‡** | ä¸å¢åŠ å¯å­¦ä¹ å‚æ•° |
| **è®¡ç®—é«˜æ•ˆ** | åªéœ€ç®€å•çš„ä¹˜æ³•å’ŒåŠ æ³• |
| **å…¼å®¹æ€§å¥½** | å¯ä¸ Flash Attention ç­‰ä¼˜åŒ–æŠ€æœ¯ç»“åˆ |

**é•¿ä¸Šä¸‹æ–‡æ‰©å±•**ï¼š

Qwen3 ä½¿ç”¨ `base=1000000`ï¼ˆè€ŒéåŸå§‹çš„ 10000ï¼‰ï¼Œä½¿ä½é¢‘æˆåˆ†å˜åŒ–æ›´æ…¢ï¼Œä»è€Œæ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼ˆ40K+ tokensï¼‰ã€‚è¿™ç§æŠ€æœ¯ç§°ä¸º **NTK-aware scaling** æˆ– **Dynamic NTK**ã€‚

---

### 6) Softmax å‡½æ•°ï¼šæ¦‚ç‡å½’ä¸€åŒ–

åœ¨è¿›å…¥ Attention ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç†è§£ **Softmax** å‡½æ•°â€”â€”å®ƒæ˜¯ Attention æœºåˆ¶çš„æ ¸å¿ƒç»„ä»¶ã€‚

#### Softmax æ˜¯ä»€ä¹ˆï¼Ÿ

Softmax å‡½æ•°å°†ä»»æ„å®æ•°å‘é‡è½¬æ¢ä¸º**æ¦‚ç‡åˆ†å¸ƒ**ï¼ˆæ‰€æœ‰å…ƒç´ éè´Ÿä¸”å’Œä¸º 1ï¼‰ã€‚

**æ•°å­¦å®šä¹‰**ï¼š

å¯¹äºè¾“å…¥å‘é‡ $z = [z_1, z_2, ..., z_n]$ï¼š

$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
$$

**ç›´è§‚ç†è§£**ï¼š

```
è¾“å…¥ scores:  [2.0,  1.0,  0.1]
              â†“ å–æŒ‡æ•° e^x
æŒ‡æ•°åŒ–:       [7.39, 2.72, 1.11]
              â†“ é™¤ä»¥æ€»å’Œ (7.39+2.72+1.11=11.22)
æ¦‚ç‡åˆ†å¸ƒ:     [0.66, 0.24, 0.10]  â† å’Œä¸º 1.0
```

**æ ¸å¿ƒæ€§è´¨**ï¼š

| æ€§è´¨ | è¯´æ˜ |
|------|------|
| **å½’ä¸€åŒ–** | è¾“å‡ºå’Œä¸º 1ï¼Œå¯è§£é‡Šä¸ºæ¦‚ç‡ |
| **ä¿åº** | è¾“å…¥è¶Šå¤§ï¼Œè¾“å‡ºæ¦‚ç‡è¶Šé«˜ |
| **å¯å¾®åˆ†** | æ¢¯åº¦å‹å¥½ï¼Œé€‚åˆåå‘ä¼ æ’­ |
| **æ”¾å¤§å·®å¼‚** | æŒ‡æ•°å‡½æ•°æ”¾å¤§å¤§å€¼ã€æŠ‘åˆ¶å°å€¼ |

**Softmax çš„"æ¸©åº¦"æ•ˆåº”**ï¼š

```python
# æ¸©åº¦å‚æ•°æ§åˆ¶åˆ†å¸ƒçš„"å°–é”ç¨‹åº¦"
def softmax_with_temperature(x, temperature=1.0):
    return F.softmax(x / temperature, dim=-1)

# ç¤ºä¾‹ï¼šx = [2.0, 1.0, 0.0]
# T=1.0 (é»˜è®¤):  [0.67, 0.24, 0.09]  â† æ­£å¸¸åˆ†å¸ƒ
# T=0.5 (ä½æ¸©):  [0.84, 0.11, 0.04]  â† æ›´å°–é”ï¼Œæ¥è¿‘ argmax
# T=2.0 (é«˜æ¸©):  [0.51, 0.31, 0.19]  â† æ›´å¹³æ»‘ï¼Œæ¥è¿‘å‡åŒ€åˆ†å¸ƒ
```

**æ•°å€¼ç¨³å®šæ€§é—®é¢˜**ï¼š

ç›´æ¥è®¡ç®— $e^{z_i}$ å¯èƒ½å¯¼è‡´æ•°å€¼æº¢å‡ºï¼ˆå½“ $z_i$ å¾ˆå¤§æ—¶ï¼‰ã€‚å®é™…å®ç°ä¼šå‡å»æœ€å¤§å€¼ï¼š

```python
def stable_softmax(x, dim=-1):
    # å‡å»æœ€å¤§å€¼ï¼Œé˜²æ­¢ exp æº¢å‡º
    x_max = x.max(dim=dim, keepdim=True).values
    exp_x = torch.exp(x - x_max)
    return exp_x / exp_x.sum(dim=dim, keepdim=True)
```

**ä¸ºä»€ä¹ˆè¿™æ ·åšæ˜¯å®‰å…¨çš„**ï¼š

$$
\frac{e^{z_i}}{\sum_j e^{z_j}} = \frac{e^{z_i - c}}{\sum_j e^{z_j - c}}
$$

å‡å»å¸¸æ•° $c = \max(z)$ åï¼Œæœ€å¤§çš„æŒ‡æ•°å˜æˆ $e^0 = 1$ï¼Œå…¶ä»–éƒ½æ˜¯å°äº 1 çš„æ­£æ•°ï¼Œé¿å…äº†æº¢å‡ºã€‚

**PyTorch å®ç°æ·±å…¥**ï¼š

```python
# PyTorch çš„ F.softmax å·²ç»å¤„ç†äº†æ•°å€¼ç¨³å®šæ€§
import torch.nn.functional as F

x = torch.tensor([1000.0, 1000.1, 1000.2])  # å¾ˆå¤§çš„æ•°
probs = F.softmax(x, dim=0)
print(probs)  # tensor([0.0900, 0.2447, 0.6652]) â† æ­£å¸¸å·¥ä½œ

# æ‰‹åŠ¨å®ç°ï¼ˆä¸å®‰å…¨ï¼‰ä¼šæº¢å‡º
# exp(1000) = inf!
```

#### Softmax åœ¨ Attention ä¸­çš„ä½œç”¨

åœ¨ Attention ä¸­ï¼ŒSoftmax å°†**æ³¨æ„åŠ›åˆ†æ•°ï¼ˆscoresï¼‰**è½¬æ¢ä¸º**æ³¨æ„åŠ›æƒé‡ï¼ˆweightsï¼‰**ï¼š

```
QÂ·K^T åˆ†æ•°:     [2.1,  -0.5,  1.3,  0.8]
              â†“ softmax
æ³¨æ„åŠ›æƒé‡:     [0.42,  0.03,  0.29,  0.26]  â† æ¦‚ç‡åˆ†å¸ƒ
              â”‚
              â–¼ åŠ æƒæ±‚å’Œ V
è¾“å‡º:          weighted sum of V vectors
```

è¿™æ„å‘³ç€ï¼š
- **é«˜åˆ†ä½ç½®**ï¼šæ¨¡å‹"æ³¨æ„"è¿™äº›ä½ç½®ï¼Œç»™äºˆæ›´é«˜æƒé‡
- **ä½åˆ†ä½ç½®**ï¼šè¢«"å¿½ç•¥"ï¼Œæƒé‡æ¥è¿‘ 0
- **æƒé‡å’Œä¸º 1**ï¼šè¾“å‡ºæ˜¯ V çš„å‡¸ç»„åˆ

---

### 7) Attentionï¼šæ³¨æ„åŠ›æœºåˆ¶çš„ç®—æ³•åŸç†

#### ä¸ºä»€ä¹ˆéœ€è¦ Attentionï¼Ÿ

ä¼ ç»Ÿçš„åºåˆ—æ¨¡å‹ï¼ˆå¦‚ RNNï¼‰æœ‰ä¸€ä¸ªæ ¹æœ¬é—®é¢˜ï¼š**ä¿¡æ¯ç“¶é¢ˆ**ã€‚

```
RNN: æ‰€æœ‰å†å²ä¿¡æ¯å¿…é¡»å‹ç¼©åˆ°å›ºå®šå¤§å°çš„éšè—çŠ¶æ€
      åºåˆ—å¾ˆé•¿æ—¶ï¼Œæ—©æœŸä¿¡æ¯ä¼šè¢«"é—å¿˜"

Attention: æ¯ä¸ªä½ç½®å¯ä»¥ç›´æ¥è®¿é—®ä»»æ„å…¶ä»–ä½ç½®
           æ²¡æœ‰ä¿¡æ¯å‹ç¼©æŸå¤±
```

**Attention çš„æ ¸å¿ƒæ€æƒ³**ï¼šå¯¹äºæ¯ä¸ªæŸ¥è¯¢ï¼ˆQueryï¼‰ï¼Œåœ¨æ‰€æœ‰é”®å€¼å¯¹ï¼ˆKey-Valueï¼‰ä¸­æ‰¾åˆ°ç›¸å…³çš„å†…å®¹ï¼Œç„¶ååŠ æƒæ±‡æ€»ã€‚

#### Qã€Kã€V çš„ç›´è§‚ç†è§£

æŠŠ Attention æƒ³è±¡æˆä¸€ä¸ª**ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿ**ï¼š

```
   ä½ åœ¨å›¾ä¹¦é¦†æ‰¾ä¹¦
        â”‚
        â–¼
   Query (Q): ä½ çš„é—®é¢˜/éœ€æ±‚
        "æˆ‘æƒ³æ‰¾å…³äº Python çš„å…¥é—¨ä¹¦"
        â”‚
        â–¼
   Key (K): æ¯æœ¬ä¹¦çš„ç´¢å¼•/æ ‡ç­¾
        ["Pythonå…¥é—¨", "Javaé«˜çº§", "æ•°æ®ç»“æ„", ...]
        â”‚
        â–¼
   åŒ¹é…ç¨‹åº¦ = Q Â· K^T
        [0.95, 0.1, 0.3, ...]  â† ç›¸å…³æ€§åˆ†æ•°
        â”‚
        â–¼
   Softmax â†’ æ³¨æ„åŠ›æƒé‡
        [0.7, 0.01, 0.15, ...]  â† å½’ä¸€åŒ–å
        â”‚
        â–¼
   Value (V): æ¯æœ¬ä¹¦çš„å®é™…å†…å®¹
        å–å‡ºç›¸å…³ä¹¦ç±ï¼Œæ ¹æ®ç›¸å…³æ€§åŠ æƒç»„åˆ
```

**Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰**ï¼š

åœ¨ Transformer ä¸­ï¼ŒQã€Kã€V éƒ½æ¥è‡ª**åŒä¸€ä¸ªè¾“å…¥åºåˆ—**ï¼ˆç»è¿‡ä¸åŒçš„çº¿æ€§æŠ•å½±ï¼‰ã€‚

```python
# è¾“å…¥: "The cat sat on the mat"
# æ¯ä¸ªè¯éƒ½ç”Ÿæˆè‡ªå·±çš„ Q, K, V

# å½“å¤„ç† "sat" è¿™ä¸ªè¯æ—¶:
Q_sat = "sat æƒ³è¦æŸ¥æ‰¾ä»€ä¹ˆä¿¡æ¯ï¼Ÿ"
K_*   = "æ‰€æœ‰è¯åœ¨'è¢«æŸ¥æ‰¾'æ—¶çš„è¡¨ç¤º"
V_*   = "æ‰€æœ‰è¯çš„å®é™…è¯­ä¹‰å†…å®¹"

# sat çš„è¾“å‡º = åŠ æƒç»„åˆæ‰€æœ‰è¯çš„ V
#   å¦‚æœ sat ä¸ cat ç›¸å…³ï¼Œcat çš„ V æƒé‡ä¼šè¾ƒé«˜
```

#### Attention å®Œæ•´è®¡ç®—æµç¨‹

```
         Q        K^T           Softmax          V
      (seq, d) Ã— (d, seq)  â†’  (seq, seq)  Ã—  (seq, d)  â†’  (seq, d)
         â”‚          â”‚            â”‚             â”‚
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â”‚             â”‚
              â–¼                  â”‚             â”‚
           scores               â–¼             â”‚
         (seq, seq)          weights          â”‚
              â”‚             (seq, seq)         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                            output
                           (seq, d)
```

**åˆ†æ­¥è¯¦è§£**ï¼š

**Step 1: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼ˆScoresï¼‰**

$$
\text{scores} = QK^T
$$

```python
# Q: (batch, heads, seq_len, head_dim) = (1, 1, 4, 3)
# K: (batch, heads, seq_len, head_dim) = (1, 1, 4, 3)

# çŸ©é˜µä¹˜æ³•: Q @ K.T
# å½¢çŠ¶å˜åŒ–: (4, 3) @ (3, 4) = (4, 4)

# ç»“æœ scores[i][j] = Q[i] ä¸ K[j] çš„ç‚¹ç§¯
#                   = ä½ç½® i å¯¹ä½ç½® j çš„"åŸå§‹æ³¨æ„åŠ›"
```

**ç‚¹ç§¯å¦‚ä½•è¡¡é‡ç›¸ä¼¼åº¦**ï¼š

```
å‘é‡ A = [1, 0]
å‘é‡ B = [1, 0]  â†’ AÂ·B = 1  (ç›¸åŒæ–¹å‘ï¼Œé«˜ç›¸ä¼¼)
å‘é‡ C = [0, 1]  â†’ AÂ·C = 0  (æ­£äº¤ï¼Œä¸ç›¸å…³)
å‘é‡ D = [-1, 0] â†’ AÂ·D = -1 (ç›¸åæ–¹å‘ï¼Œè´Ÿç›¸å…³)
```

**Step 2: ç¼©æ”¾ï¼ˆScalingï¼‰**

$$
\text{scaled\_scores} = \frac{QK^T}{\sqrt{d_k}}
$$

```python
d_k = head_dim  # 128 for Qwen3
scores = scores / math.sqrt(d_k)
```

**ä¸ºä»€ä¹ˆè¦é™¤ä»¥ âˆšd_k**ï¼š

å½“ $d_k$ å¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯çš„**æ–¹å·®**ä¼šå˜å¤§ï¼š

```
å‡è®¾ Q å’Œ K çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ ~N(0,1)

ç‚¹ç§¯ = sum(Q[i] * K[i] for i in range(d_k))
     = d_k ä¸ªç‹¬ç«‹éšæœºå˜é‡çš„å’Œ

æ–¹å·® = d_k  (æ¯é¡¹æ–¹å·®=1)
æ ‡å‡†å·® = âˆšd_k

å½“ d_k=128 æ—¶ï¼Œç‚¹ç§¯å€¼å¯èƒ½åœ¨ [-20, 20] èŒƒå›´
Softmax ä¼šå˜å¾—éå¸¸"å°–é”" â†’ æ¢¯åº¦æ¶ˆå¤±
```

é™¤ä»¥ âˆšd_k åï¼Œæ–¹å·®å›åˆ° ~1ï¼Œsoftmax è¾“å‡ºæ›´å¹³æ»‘ã€‚

**Step 3: åº”ç”¨æ©ç ï¼ˆMaskingï¼‰**

å¯¹äºå› æœè¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTï¼‰ï¼Œä½ç½® t ä¸èƒ½çœ‹åˆ° t+1, t+2, ... çš„ä¿¡æ¯ï¼š

```python
# æ©ç çŸ©é˜µï¼ˆ4x4 çš„ä¾‹å­ï¼‰
mask = [
    [  0, -âˆ, -âˆ, -âˆ],   # ä½ç½®0 åªèƒ½çœ‹ä½ç½®0
    [  0,  0, -âˆ, -âˆ],   # ä½ç½®1 èƒ½çœ‹ 0,1
    [  0,  0,  0, -âˆ],   # ä½ç½®2 èƒ½çœ‹ 0,1,2
    [  0,  0,  0,  0],   # ä½ç½®3 èƒ½çœ‹ 0,1,2,3
]

scores = scores + mask
# -âˆ çš„ä½ç½®åœ¨ softmax åä¼šå˜æˆ 0
```

**Step 4: Softmax å½’ä¸€åŒ–**

$$
\text{weights} = \text{softmax}(\text{scaled\_scores})
$$

```python
attn_weights = F.softmax(scores, dim=-1)
# æ¯ä¸€è¡Œå’Œä¸º 1
# attn_weights[i] è¡¨ç¤ºä½ç½® i å¯¹æ‰€æœ‰ä½ç½®çš„æ³¨æ„åŠ›åˆ†å¸ƒ
```

**Step 5: åŠ æƒæ±‚å’Œ Value**

$$
\text{output} = \text{weights} \cdot V
$$

```python
output = torch.matmul(attn_weights, V)
# output[i] = sum(weights[i][j] * V[j] for all j)
#           = æ‰€æœ‰ä½ç½® V çš„åŠ æƒç»„åˆ
```

#### æ ¸å¿ƒå…¬å¼

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

#### æ ¸å¿ƒå®ç°

```python
def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Q, K, V: (batch, num_heads, seq_len, head_dim)
    """
    d_k = Q.shape[-1]

    # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    # scores: (batch, num_heads, seq_len, seq_len)

    # 2. åº”ç”¨å› æœæ©ç ï¼ˆautoregressive generationï¼‰
    if mask is not None:
        scores = scores + mask  # mask ä¸­æœªæ¥ä½ç½®æ˜¯ -inf

    # 3. Softmax å½’ä¸€åŒ–
    attn_weights = F.softmax(scores, dim=-1)
    # attn_weights: (batch, num_heads, seq_len, seq_len)

    # 4. åŠ æƒæ±‚å’Œ
    output = torch.matmul(attn_weights, V)
    # output: (batch, num_heads, seq_len, head_dim)

    return output, attn_weights
```

#### å› æœæ©ç ï¼ˆCausal Maskï¼‰

å¯¹äºè‡ªå›å½’ç”Ÿæˆï¼Œä½ç½® t åªèƒ½çœ‹åˆ°ä½ç½® 0~tï¼Œä¸èƒ½çœ‹åˆ°æœªæ¥ï¼š

```python
def causal_mask(seq_len):
    """
    è¿”å›:
    [[  0, -inf, -inf, -inf],
     [  0,    0, -inf, -inf],
     [  0,    0,    0, -inf],
     [  0,    0,    0,    0]]
    """
    mask = torch.full((seq_len, seq_len), float("-inf"))
    mask = torch.triu(mask, diagonal=1)
    return mask
```

#### Attention çš„å¯è§†åŒ–ç†è§£

```
è¾“å…¥å¥å­: "The cat sat on the mat"

Attention æƒé‡çŸ©é˜µï¼ˆæŸä¸€å±‚æŸä¸€å¤´ï¼‰:

          The   cat   sat   on   the   mat
    The  [0.9   0.05  0.02  0.01  0.01  0.01]
    cat  [0.3   0.5   0.1   0.05  0.03  0.02]
    sat  [0.1   0.4   0.3   0.1   0.05  0.05]  â† sat ä¸»è¦å…³æ³¨ cat
    on   [0.1   0.1   0.2   0.4   0.1   0.1 ]
    the  [0.05  0.1   0.1   0.1   0.3   0.35]
    mat  [0.05  0.2   0.1   0.1   0.15  0.4 ]  â† mat å…³æ³¨è‡ªå·±å’Œ cat
```

**ä¸åŒå±‚çš„ Attention æ¨¡å¼**ï¼š

- **æµ…å±‚**ï¼šé€šå¸¸å…³æ³¨å±€éƒ¨ä¸Šä¸‹æ–‡ã€è¯­æ³•ç»“æ„
- **æ·±å±‚**ï¼šæ•æ‰è¯­ä¹‰å…³ç³»ã€é•¿è·ç¦»ä¾èµ–
- **æŸäº›å¤´**ï¼šä¸“é—¨å…³æ³¨"å‰ä¸€ä¸ªè¯"æˆ–"æ ‡ç‚¹"
- **æŸäº›å¤´**ï¼šä¸“é—¨å…³æ³¨"ä¸»è°“å…³ç³»"æˆ–"æŒ‡ä»£"

#### å¤æ‚åº¦åˆ†æ

| æ“ä½œ | å¤æ‚åº¦ | è¯´æ˜ |
|------|--------|------|
| QÂ·K^T | O(nÂ² Â· d) | n=åºåˆ—é•¿åº¦ï¼Œd=head_dim |
| Softmax | O(nÂ²) | æ¯è¡Œ n ä¸ªå…ƒç´  |
| WeightsÂ·V | O(nÂ² Â· d) | çŸ©é˜µä¹˜æ³• |
| **æ€»è®¡** | **O(nÂ² Â· d)** | è¿™æ˜¯é•¿ä¸Šä¸‹æ–‡çš„ä¸»è¦ç“¶é¢ˆ |

**ä¸ºä»€ä¹ˆ O(nÂ²) æ˜¯é—®é¢˜**ï¼š

```
n = 1K   â†’ nÂ² = 1M æ¬¡æ“ä½œ    â† å¯æ¥å—
n = 4K   â†’ nÂ² = 16M æ¬¡æ“ä½œ   â† å¼€å§‹å˜æ…¢
n = 32K  â†’ nÂ² = 1B æ¬¡æ“ä½œ    â† å¾ˆæ…¢
n = 128K â†’ nÂ² = 16B æ¬¡æ“ä½œ   â† éå¸¸æ…¢ï¼Œæ˜¾å­˜çˆ†ç‚¸
```

**Flash Attention ç­‰ä¼˜åŒ–**ï¼šé€šè¿‡å·§å¦™çš„åˆ†å—è®¡ç®—ï¼Œåœ¨ä¿æŒ O(nÂ²) è®¡ç®—é‡çš„åŒæ—¶ï¼Œå¤§å¹…å‡å°‘**æ˜¾å­˜å ç”¨**ï¼ˆä» O(nÂ²) é™åˆ° O(n)ï¼‰ã€‚

---

### 8) MLPï¼ˆå‰é¦ˆç½‘ç»œï¼‰

æ¯ä¸ª Transformer å±‚çš„å¦ä¸€åŠæ˜¯ MLPï¼ˆä¹Ÿå« FFNï¼‰ã€‚å®ƒå¯¹æ¯ä¸ª token ä½ç½®ç‹¬ç«‹åšéçº¿æ€§å˜æ¢ã€‚

**ä¼ ç»Ÿ FFN**ï¼š

```
x â”€â”€â–º Linear(dâ†’4d) â”€â”€â–º ReLU â”€â”€â–º Linear(4dâ†’d) â”€â”€â–º output
```

**ç°ä»£ LLM ä½¿ç”¨ SwiGLU**ï¼ˆQwen3ã€Llamaï¼‰ï¼š

```
        â”Œâ”€â”€â–º gate_proj â”€â”€â–º SiLU â”€â”€â”€â”
        â”‚                          â”‚
x â”€â”€â”€â”€â”€â”€â”¤                          â”œâ”€â”€â–º é€å…ƒç´ ä¹˜ â”€â”€â–º down_proj â”€â”€â–º output
        â”‚                          â”‚
        â””â”€â”€â–º up_proj â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ ¸å¿ƒå®ç°**ï¼š

```python
class MLP(nn.Module):
    def __init__(self, hidden_size: int, intermediate_size: int):
        super().__init__()
        # Qwen3-8B: hidden_size=4096, intermediate_size=14336
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)

    def forward(self, x):
        # SwiGLU: silu(gate(x)) * up(x)
        gate = F.silu(self.gate_proj(x))  # SiLU = x * sigmoid(x)
        up = self.up_proj(x)
        return self.down_proj(gate * up)
```

**å‚æ•°é‡**ï¼ˆæ¯å±‚ MLPï¼‰ï¼š
- gate_proj: `4096 Ã— 14336` â‰ˆ 58.7M
- up_proj: `4096 Ã— 14336` â‰ˆ 58.7M
- down_proj: `14336 Ã— 4096` â‰ˆ 58.7M
- **æ¯å±‚ MLP æ€»è®¡**ï¼šçº¦ 176M å‚æ•°

**å…³é”®ç‚¹**ï¼šMLP é€šå¸¸å æ¨¡å‹å‚æ•°é‡çš„ **60-70%**ï¼

---

### 9) å®Œæ•´çš„ Transformer å±‚

æŠŠä¸Šé¢çš„ç»„ä»¶ç»„åˆèµ·æ¥ï¼Œä¸€ä¸ªå®Œæ•´çš„ Transformer å±‚ï¼ˆPre-Norm æ¶æ„ï¼‰å¦‚ä¸‹ï¼š

```
Input Hidden States
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ RMSNorm â”‚ â†â”€â”€ input_layernorm
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚           Self-Attention             â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
   â”‚  â”‚ Q/K/V Proj â”€â”€â–º RoPE â”€â”€â–º Attn â”‚    â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
   â”‚               â”‚                      â”‚
   â”‚               â–¼ O Proj               â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€ + Residual â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                   â”‚
        â–¼                                   â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
   â”‚ RMSNorm â”‚ â†â”€â”€ post_attention_layernorm â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                              â”‚
        â”‚                                   â”‚
        â–¼                                   â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
   â”‚      MLP        â”‚                      â”‚
   â”‚ (SwiGLU: 3å±‚)   â”‚                      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
            â”‚                               â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€ + Residual â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
     Output Hidden States
```

**ä»£ç å®ç°**ï¼š

```python
class TransformerBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.input_layernorm = RMSNorm(config.hidden_size)
        self.self_attn = Attention(config)
        self.post_attention_layernorm = RMSNorm(config.hidden_size)
        self.mlp = MLP(config)

    def forward(self, x, attention_mask=None, position_ids=None):
        # 1. Self-Attention (with residual)
        residual = x
        x = self.input_layernorm(x)
        x = self.self_attn(x, attention_mask, position_ids)
        x = residual + x

        # 2. MLP (with residual)
        residual = x
        x = self.post_attention_layernorm(x)
        x = self.mlp(x)
        x = residual + x

        return x
```

---

### 10) å‚æ•°é‡ä»å“ªé‡Œæ¥ï¼Ÿ

ä»¥ Qwen3-8B ä¸ºä¾‹ï¼Œæ¯å±‚å‚æ•°é‡åˆ†å¸ƒï¼š

| ç»„ä»¶ | å‚æ•°é‡ | å æ¯” |
|------|--------|------|
| Q Proj | 16.8M | 7.5% |
| K Proj | 4.2M | 1.9% |
| V Proj | 4.2M | 1.9% |
| O Proj | 16.8M | 7.5% |
| **Attention å°è®¡** | **42M** | **18.8%** |
| Gate Proj | 58.7M | 26.3% |
| Up Proj | 58.7M | 26.3% |
| Down Proj | 58.7M | 26.3% |
| **MLP å°è®¡** | **176M** | **78.9%** |
| RMSNorm Ã—2 | 8K | ~0% |

**ç»“è®º**ï¼šMLP å äº†æ¯å±‚å‚æ•°çš„ ~80%ï¼

**å®Œæ•´ 8B æ¨¡å‹å‚æ•°åˆ†å¸ƒ**ï¼š
- Embedding: ~622M (7%)
- 36 Ã— Transformer Layer: ~7.8B (92%)
- Final RMSNorm + LM Head: ~626M (1%)

---
## å‚è€ƒèµ„æ–™

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)


