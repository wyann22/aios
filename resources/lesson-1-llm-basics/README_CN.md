# Lesson 1ï¼šLLM åŸºç¡€ï¼ˆTokenizerã€Decoder-only Transformerã€Attentionã€å‚æ•°é‡ï¼‰

<div style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 12px; margin: 20px 0;">
<h2 style="color: white; margin: 0 0 12px 0; font-size: 1.5em;">ğŸ“Œ è¯¾ç¨‹å®šä½è¯´æ˜</h2>
<p style="color: white; font-size: 1.2em; line-height: 1.6; margin: 0;">
æœ¬è¯¾ç¨‹ä¸»è¦ä¾§é‡äº <strong>AI Infraï¼ˆåŸºç¡€è®¾æ–½/ç³»ç»Ÿï¼‰</strong> è§’åº¦ï¼Œè€Œéç®—æ³•ç ”ç©¶ã€‚å› æ­¤åœ¨ç®—æ³•åŸç†ä¸Šä¸ä¼šè®²å¾—ç‰¹åˆ«æ·±å…¥ï¼Œå¯¹äº AI Infra å·¥ç¨‹å¸ˆæ¥è¯´ï¼Œåªéœ€äº†è§£å®ç°åŸç†å³å¯ï¼Œä¸å¿…æ·±ç©¶æ•°å­¦æ¨å¯¼ç»†èŠ‚ã€‚
</p>
<p style="color: #e0e0e0; font-size: 1.1em; line-height: 1.6; margin: 12px 0 0 0;">
ğŸ”– æœ¬è¯¾ç¨‹ä»¥ <strong style="color: #ffd700;">Qwen3-8B</strong> çš„ç½‘ç»œæ¶æ„ä¸ºä¾‹è¿›è¡Œè®²è§£ï¼Œå¸®åŠ©å¤§å®¶ç†è§£ç°ä»£ LLM çš„æ ¸å¿ƒç»„ä»¶å’Œå®ç°ç»†èŠ‚ã€‚
</p>
</div>

- **Tokenizer**ï¼šæ–‡æœ¬ â†’ token IDï¼ˆæ•°å­—ï¼‰
- **Decoder-only Transformer**ï¼štoken ID â†’ è¯è¡¨ä¸Šçš„ logits
- **ç”Ÿæˆï¼ˆGenerationï¼‰**ï¼šä¸æ–­äº§ç”Ÿä¸‹ä¸€ä¸ª token IDï¼Œå† decode å›æ–‡æœ¬

![LLM æ¨ç†/ç”Ÿæˆæµç¨‹ï¼šencode â†’ decoder-only transformer â†’ decode â†’ è‡ªå›å½’å¾ªç¯](images/llm_flow.png)

<img src="images/qwen3.png" alt="Qwen3 æ¨¡å‹æ¶æ„" width="60%">

---

## æ ¸å¿ƒæ¦‚å¿µ

### 1) Tokenizationï¼štext â†’ tokens â†’ IDs

LLM ä¸æ˜¯ç›´æ¥â€œè¯»æ–‡å­—â€ï¼Œå®ƒè¯»çš„æ˜¯ **token ID**ï¼ˆæ•´æ•°åºåˆ—ï¼‰ã€‚

å…¸å‹æµç¨‹ï¼š

```
Text â”€â”€â–º (Tokenizer) â”€â”€â–º Tokensï¼ˆå­—ç¬¦ä¸²ç‰‡æ®µï¼‰ â”€â”€â–º Token IDsï¼ˆæ•´æ•°ï¼‰
```

ä¸ºä»€ä¹ˆ tokenizer å¾ˆé‡è¦ï¼š

- **ä¸Šä¸‹æ–‡é•¿åº¦**æŒ‰ token æ•°è®¡ç®—ï¼Œè€Œä¸æ˜¯å­—ç¬¦æ•°
- **é€Ÿåº¦/åå**å¸¸å¸¸æŒ‰â€œæ¯ç”Ÿæˆ 1 ä¸ª token çš„æˆæœ¬â€è¡¡é‡
- token çš„åˆ‡åˆ†è¾¹ç•Œä¼šå½±å“æ¨¡å‹è¡¨è¾¾ï¼ˆäººåã€ä»£ç ã€ä¸åŒè¯­è¨€ç­‰ï¼‰


ä½ å¯ä»¥æŠŠ tokenizer ç†è§£æˆä¸€ä¸ªâ€œ**å¯é€†çš„å­—å…¸å‹ç¼©å™¨**â€ï¼š

![Tokenizerï¼šencode / decode ç¤ºæ„å›¾](images/tokenizer.png)

- **Vocabularyï¼ˆè¯è¡¨ï¼‰**ï¼šä¸€ä¸ªå›ºå®šå­—å…¸ï¼Œåˆ—å‡ºå…è®¸å‡ºç°çš„â€œç‰‡æ®µâ€ï¼ˆtokenï¼‰
- **Encode**ï¼šæŠŠæ–‡æœ¬æ‹†æˆè¿™äº›ç‰‡æ®µï¼Œå¹¶æŠŠæ¯ä¸ªç‰‡æ®µæ˜ å°„æˆæ•°å­—ï¼ˆtoken IDï¼‰
- **Decode**ï¼šæŠŠ token ID å†æ˜ å°„å›ç‰‡æ®µï¼Œå¹¶æ‹¼å›æ–‡æœ¬ï¼ˆå› æ­¤å®ƒéœ€è¦å°½é‡å¯é€†ã€ç¨³å®šï¼‰

ä¸ºä»€ä¹ˆä¸ç›´æ¥æŒ‰â€œå­—ç¬¦â€æˆ–â€œè¯â€ï¼Ÿ

- **å­—ç¬¦çº§**ï¼šåºåˆ—ä¼šå˜å¾ˆé•¿ï¼ˆtoken æ•°æ›´å¤šï¼‰ï¼Œæ³¨æ„åŠ›è®¡ç®—æ›´è´µï¼Œç”Ÿæˆæ›´æ…¢
- **è¯çº§**ï¼šè¯è¡¨ä¼šçˆ†ç‚¸ï¼ˆæ–°è¯ã€æ‹¼å†™å˜åŒ–ã€äººåã€ä»£ç æ ‡è¯†ç¬¦ï¼‰ï¼Œè¿˜ä¼šé¢‘ç¹é‡åˆ° OOVï¼ˆè¯è¡¨å¤–ï¼‰é—®é¢˜

å®é™…å·¥ç¨‹é‡Œå¸¸ç”¨çš„æŠ˜ä¸­æ˜¯ **å­è¯/ç‰‡æ®µï¼ˆsubword piecesï¼‰**ï¼š

- å¸¸è§ç‰‡æ®µå˜æˆä¸€ä¸ª tokenï¼ˆæ›´çŸ­çš„åºåˆ—ï¼‰
- ç½•è§è¯å¯ä»¥ç”±å¤šä¸ªç‰‡æ®µç»„åˆï¼ˆé¿å…å®Œå…¨ OOVï¼‰
- è¯è¡¨è§„æ¨¡å¯æ§ï¼ˆå¸¸è§æ˜¯å‡ ä¸‡åˆ°åå‡ ä¸‡ï¼‰

æ­¤å¤–è¿˜æœ‰ä¸€ç±»éå¸¸é‡è¦ï¼š**ç‰¹æ®Š tokenï¼ˆspecial tokensï¼‰**ï¼Œæ¯”å¦‚ï¼š

- `<bos>` / `<eos>`ï¼šå¥å­å¼€å§‹/ç»“æŸ
- `<pad>`ï¼šbatch å¯¹é½ç”¨çš„å¡«å……
- ä»¥åŠèŠå¤©æ¨¡å‹æ¨¡æ¿ï¼ˆroleã€åˆ†éš”ç¬¦ç­‰ï¼‰

ä½ åé¢ä¼šåå¤é‡åˆ°ä¸€å¥è¯ï¼š**ä¸Šä¸‹æ–‡é•¿åº¦ã€KV cache å¤§å°ã€ååï¼ˆtokens/sï¼‰åŸºæœ¬éƒ½æŒ‰ token æ•°æ¥ç®—**ã€‚

---

### 2) Embeddingï¼štoken ID â†’ å‘é‡

Tokenizer è¾“å‡ºçš„æ˜¯æ•´æ•°åºåˆ—ï¼ˆtoken IDsï¼‰ï¼Œä½†ç¥ç»ç½‘ç»œéœ€è¦çš„æ˜¯**è¿ç»­å‘é‡**ã€‚Embedding å±‚å°±æ˜¯è¿™ä¸ªæ¡¥æ¢ã€‚

```
Token IDs: [1024, 5678, 42]
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Embedding æŸ¥è¡¨                  â”‚
â”‚  (vocab_size Ã— hidden_size)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
Hidden States: [[0.1, -0.2, ...], [0.3, 0.5, ...], [0.2, 0.1, ...]]
               å½¢çŠ¶: (seq_len, hidden_size)
```

**æ ¸å¿ƒå®ç°**ï¼š

```python
class Embedding(nn.Module):
    def __init__(self, vocab_size: int, hidden_size: int):
        super().__init__()
        # åˆ›å»ºä¸€ä¸ªå¯å­¦ä¹ çš„æŸ¥æ‰¾è¡¨
        self.weight = nn.Parameter(torch.randn(vocab_size, hidden_size))

    def forward(self, token_ids: torch.Tensor) -> torch.Tensor:
        # token_ids: (batch_size, seq_len)
        # è¿”å›: (batch_size, seq_len, hidden_size)
        return self.weight[token_ids]
```

**å‚æ•°é‡**ï¼š`vocab_size Ã— hidden_size`

ä»¥ Qwen3-8B ä¸ºä¾‹ï¼š`151936 Ã— 4096 â‰ˆ 622M` å‚æ•°ï¼ˆçº¦å æ¨¡å‹çš„ 7%ï¼‰

**å…³é”®ç‚¹**ï¼š
- Embedding æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª"æŸ¥è¡¨"æ“ä½œï¼Œä¸æ¶‰åŠçŸ©é˜µä¹˜æ³•
- æ¯ä¸ª token ID å¯¹åº”ä¸€ä¸ªå›ºå®šçš„å‘é‡ï¼ˆè®­ç»ƒæ—¶å­¦ä¹ å¾—åˆ°ï¼‰
- è¾“å‡ºå½¢çŠ¶ä» `(batch, seq_len)` å˜ä¸º `(batch, seq_len, hidden_size)`

---

### 3) LayerNorm ä¸ RMSNormï¼šå±‚å½’ä¸€åŒ–

Normalizationï¼ˆå½’ä¸€åŒ–ï¼‰æ˜¯æ·±åº¦å­¦ä¹ è®­ç»ƒç¨³å®šçš„å…³é”®æŠ€æœ¯ã€‚æ²¡æœ‰å½’ä¸€åŒ–ï¼Œæ·±å±‚ç½‘ç»œå¾ˆå®¹æ˜“å‡ºç°æ¢¯åº¦çˆ†ç‚¸æˆ–æ¢¯åº¦æ¶ˆå¤±ã€‚

#### ä¸ºä»€ä¹ˆéœ€è¦å½’ä¸€åŒ–ï¼Ÿ

ç¥ç»ç½‘ç»œæ¯ä¸€å±‚çš„è¾“å‡ºåˆ†å¸ƒä¼šéšç€è®­ç»ƒä¸æ–­å˜åŒ–ï¼ˆInternal Covariate Shiftï¼‰ï¼Œè¿™ä¼šå¯¼è‡´ï¼š
- åç»­å±‚éœ€è¦ä¸æ–­é€‚åº”æ–°çš„è¾“å…¥åˆ†å¸ƒ
- è®­ç»ƒä¸ç¨³å®šï¼Œéœ€è¦æ›´å°çš„å­¦ä¹ ç‡
- æ”¶æ•›é€Ÿåº¦å˜æ…¢

å½’ä¸€åŒ–çš„ç›®æ ‡ï¼š**å°†æ¯ä¸€å±‚çš„è¾“å‡º"æ‹‰å›"åˆ°ç¨³å®šçš„åˆ†å¸ƒ**ï¼ˆå‡å€¼â‰ˆ0ï¼Œæ–¹å·®â‰ˆ1ï¼‰ã€‚

#### LayerNorm è¯¦è§£

**æ ¸å¿ƒå…¬å¼**ï¼š

å¯¹äºè¾“å…¥å‘é‡ $x = [x_1, x_2, ..., x_d]$ï¼ˆd æ˜¯ hidden_sizeï¼‰ï¼š

$$
\text{LayerNorm}(x) = \gamma \cdot \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
$$

å…¶ä¸­ï¼š
- $\mu = \frac{1}{d}\sum_{i=1}^{d} x_i$ï¼ˆå‡å€¼ï¼‰
- $\sigma^2 = \frac{1}{d}\sum_{i=1}^{d} (x_i - \mu)^2$ï¼ˆæ–¹å·®ï¼‰
- $\gamma$ï¼ˆscaleï¼‰å’Œ $\beta$ï¼ˆshiftï¼‰æ˜¯å¯å­¦ä¹ å‚æ•°
- $\epsilon$ æ˜¯é˜²æ­¢é™¤é›¶çš„å°å¸¸æ•°ï¼ˆå¦‚ 1e-6ï¼‰

**LayerNorm æ ¸å¿ƒå®ç°**ï¼š

```python
class LayerNorm(nn.Module):
    def __init__(self, hidden_size: int, eps: float = 1e-6):
        super().__init__()
        self.gamma = nn.Parameter(torch.ones(hidden_size))   # ç¼©æ”¾å‚æ•°
        self.beta = nn.Parameter(torch.zeros(hidden_size))   # å¹³ç§»å‚æ•°
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch, seq_len, hidden_size)

        # 1. è®¡ç®—å‡å€¼ (åœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Š)
        mean = x.mean(dim=-1, keepdim=True)

        # 2. è®¡ç®—æ–¹å·®
        var = x.var(dim=-1, keepdim=True, unbiased=False)

        # 3. å½’ä¸€åŒ–: (x - mean) / sqrt(var + eps)
        x_norm = (x - mean) / torch.sqrt(var + self.eps)

        # 4. ç¼©æ”¾å’Œå¹³ç§»: gamma * x_norm + beta
        return self.gamma * x_norm + self.beta
```

**å‚æ•°é‡**ï¼š`2 Ã— hidden_size`ï¼ˆgamma å’Œ beta å„ hidden_size ä¸ªï¼‰

#### ä¸ºä»€ä¹ˆ Î³ å’Œ Î² å¾ˆé‡è¦ï¼Ÿ

å¦‚æœåªåšå½’ä¸€åŒ–ï¼ˆå¼ºåˆ¶å‡å€¼=0ï¼Œæ–¹å·®=1ï¼‰ï¼Œä¼šé™åˆ¶ç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ã€‚é€šè¿‡å¯å­¦ä¹ çš„ Î³ å’Œ Î²ï¼š

- ç½‘ç»œå¯ä»¥"å­¦ä¹ "æ¢å¤åŸå§‹åˆ†å¸ƒï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰
- å½“ Î³=Ïƒ, Î²=Î¼ æ—¶ï¼Œç›¸å½“äºæ’ç­‰å˜æ¢ï¼ˆä»€ä¹ˆéƒ½ä¸åšï¼‰
- ç½‘ç»œå¯ä»¥åœ¨"å½’ä¸€åŒ–"å’Œ"ä¿æŒåŸæ ·"ä¹‹é—´è‡ªç”±é€‰æ‹©

#### RMSNormï¼šæ›´ç®€å•çš„æ›¿ä»£æ–¹æ¡ˆ

ç°ä»£ LLMï¼ˆå¦‚ Llamaã€Qwenã€Mistralï¼‰æ™®éä½¿ç”¨ **RMSNorm** æ›¿ä»£ LayerNormã€‚

**æ ¸å¿ƒå…¬å¼**ï¼š

$$
\text{RMSNorm}(x) = \gamma \cdot \frac{x}{\text{RMS}(x)}
$$

å…¶ä¸­ï¼š

$$
\text{RMS}(x) = \sqrt{\frac{1}{d}\sum_{i=1}^{d} x_i^2 + \epsilon}
$$

**å…³é”®åŒºåˆ«**ï¼š

| ç‰¹æ€§ | LayerNorm | RMSNorm |
|------|-----------|---------|
| å‡å‡å€¼ï¼ˆä¸­å¿ƒåŒ–ï¼‰ | âœ… æ˜¯ | âŒ å¦ |
| é™¤æ ‡å‡†å·® | âœ… æ˜¯ | âŒ å¦ï¼ˆé™¤ RMSï¼‰ |
| åŠ åç½® Î² | âœ… æ˜¯ | âŒ å¦ |
| å‚æ•°é‡ | 2d | d |
| è®¡ç®—é‡ | è¾ƒå¤š | è¾ƒå°‘ |

**ä¸ºä»€ä¹ˆ RMSNorm æœ‰æ•ˆ**ï¼š

ç ”ç©¶è¡¨æ˜ï¼ŒLayerNorm çš„ä¸»è¦ä½œç”¨æ¥è‡ª**ç¼©æ”¾**ï¼ˆé™¤ä»¥æŸä¸ªç»Ÿè®¡é‡ï¼‰ï¼Œè€Œä¸æ˜¯**ä¸­å¿ƒåŒ–**ï¼ˆå‡å‡å€¼ï¼‰ã€‚RMSNorm å»æ‰äº†ä¸­å¿ƒåŒ–æ­¥éª¤ï¼Œä½†ä¿ç•™äº†æ ¸å¿ƒçš„ç¼©æ”¾ä½œç”¨ï¼ŒåŒæ—¶ï¼š
- å‡å°‘ ~50% çš„è®¡ç®—é‡
- å‡å°‘ 50% çš„å‚æ•°é‡
- å®éªŒæ•ˆæœç›¸å½“ç”šè‡³æ›´å¥½

**RMSNorm æ ¸å¿ƒå®ç°**ï¼š

```python
class RMSNorm(nn.Module):
    """Root Mean Square Layer Normalization"""
    def __init__(self, hidden_size: int, eps: float = 1e-6):
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))  # gamma
        self.eps = eps

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (batch, seq_len, hidden_size)
        # 1. è®¡ç®— RMS: sqrt(mean(x^2))
        rms = torch.sqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)
        # 2. å½’ä¸€åŒ–å¹¶ç¼©æ”¾
        return (x / rms) * self.weight
```

**å‚æ•°é‡**ï¼š`hidden_size`ï¼ˆåªæœ‰ gammaï¼Œæ²¡æœ‰ betaï¼‰

---

### 4) Q/K/V ç”Ÿæˆï¼ˆLinear æŠ•å½±ï¼‰

Attention çš„æ ¸å¿ƒæ˜¯ Queryã€Keyã€Value ä¸‰ä¸ªå‘é‡ã€‚å®ƒä»¬é€šè¿‡**çº¿æ€§æŠ•å½±**ä»è¾“å…¥ hidden states ç”Ÿæˆã€‚

```
Hidden States (batch, seq_len, hidden_size)
        â”‚
        â”œâ”€â”€â–º Wq â”€â”€â–º Q (Query)   : "æˆ‘åœ¨æ‰¾ä»€ä¹ˆ"
        â”‚
        â”œâ”€â”€â–º Wk â”€â”€â–º K (Key)     : "æˆ‘æœ‰ä»€ä¹ˆ"
        â”‚
        â””â”€â”€â–º Wv â”€â”€â–º V (Value)   : "æˆ‘çš„å†…å®¹æ˜¯ä»€ä¹ˆ"
```

#### Linear å±‚åŸºç¡€ï¼šçŸ©é˜µä¹˜æ³•

åœ¨æ·±å…¥ Q/K/V ä¹‹å‰ï¼Œå…ˆç†è§£ **Linear å±‚**ï¼ˆä¹Ÿå«å…¨è¿æ¥å±‚ã€Dense å±‚ï¼‰çš„æœ¬è´¨ã€‚

**æ•°å­¦å®šä¹‰**ï¼š

$$
y = xW^T + b
$$

å…¶ä¸­ï¼š
- $x$ï¼šè¾“å…¥å‘é‡ï¼Œå½¢çŠ¶ `(*, in_features)`
- $W$ï¼šæƒé‡çŸ©é˜µï¼Œå½¢çŠ¶ `(out_features, in_features)`
- $b$ï¼šåç½®å‘é‡ï¼Œå½¢çŠ¶ `(out_features)`ï¼ˆå¯é€‰ï¼‰
- $y$ï¼šè¾“å‡ºå‘é‡ï¼Œå½¢çŠ¶ `(*, out_features)`

**çŸ©é˜µä¹˜æ³•å›¾è§£**ï¼š

```
è¾“å…¥ x              æƒé‡ W^T              è¾“å‡º y
(1, in_features)   (in_features, out)   (1, out_features)

[xâ‚€ xâ‚ xâ‚‚ xâ‚ƒ]  Ã—  [wâ‚€â‚€ wâ‚€â‚ wâ‚€â‚‚]   =   [yâ‚€ yâ‚ yâ‚‚]
                   [wâ‚â‚€ wâ‚â‚ wâ‚â‚‚]
                   [wâ‚‚â‚€ wâ‚‚â‚ wâ‚‚â‚‚]
                   [wâ‚ƒâ‚€ wâ‚ƒâ‚ wâ‚ƒâ‚‚]

æ¯ä¸ªè¾“å‡ºå…ƒç´ :
yâ‚€ = xâ‚€Â·wâ‚€â‚€ + xâ‚Â·wâ‚â‚€ + xâ‚‚Â·wâ‚‚â‚€ + xâ‚ƒÂ·wâ‚ƒâ‚€  (è¾“å…¥ä¸ç¬¬ 0 åˆ—ç‚¹ç§¯)
yâ‚ = xâ‚€Â·wâ‚€â‚ + xâ‚Â·wâ‚â‚ + xâ‚‚Â·wâ‚‚â‚ + xâ‚ƒÂ·wâ‚ƒâ‚  (è¾“å…¥ä¸ç¬¬ 1 åˆ—ç‚¹ç§¯)
yâ‚‚ = xâ‚€Â·wâ‚€â‚‚ + xâ‚Â·wâ‚â‚‚ + xâ‚‚Â·wâ‚‚â‚‚ + xâ‚ƒÂ·wâ‚ƒâ‚‚  (è¾“å…¥ä¸ç¬¬ 2 åˆ—ç‚¹ç§¯)
```

**PyTorch å®ç°**ï¼š

```python
import torch.nn as nn

# åˆ›å»ºä¸€ä¸ª Linear å±‚ï¼š4 ç»´è¾“å…¥ â†’ 3 ç»´è¾“å‡º
linear = nn.Linear(in_features=4, out_features=3, bias=False)

# æŸ¥çœ‹æƒé‡å½¢çŠ¶
print(linear.weight.shape)  # torch.Size([3, 4])  å³ (out_features, in_features)

# å‰å‘ä¼ æ’­
x = torch.randn(2, 5, 4)    # (batch=2, seq_len=5, in_features=4)
y = linear(x)               # (batch=2, seq_len=5, out_features=3)
```

**å…³é”®ç†è§£**ï¼š

| æ¦‚å¿µ | è¯´æ˜ |
|------|------|
| **çº¿æ€§å˜æ¢** | Linear å±‚æœ¬è´¨æ˜¯å¯¹è¾“å…¥åšçº¿æ€§å˜æ¢ï¼ˆæ—‹è½¬ã€ç¼©æ”¾ã€æŠ•å½±ï¼‰ |
| **å‚æ•°é‡** | `in_features Ã— out_features`ï¼ˆ+ out_features å¦‚æœæœ‰ biasï¼‰ |
| **æ— æ¿€æ´»å‡½æ•°** | Linear æœ¬èº«ä¸å«éçº¿æ€§ï¼Œéœ€é…åˆ ReLU/SiLU ç­‰ä½¿ç”¨ |
| **æ‰¹é‡å¤„ç†** | å¯¹ batch å’Œ seq_len ç»´åº¦ç‹¬ç«‹ä½œç”¨ï¼Œåªå˜æ¢æœ€åä¸€ç»´ |

**åœ¨ LLM ä¸­çš„åº”ç”¨**ï¼š

```python
# Q/K/V æŠ•å½±å°±æ˜¯ 3 ä¸ª Linear å±‚
self.q_proj = nn.Linear(4096, 4096, bias=False)  # hidden â†’ Q
self.k_proj = nn.Linear(4096, 1024, bias=False)  # hidden â†’ K (GQA: æ›´å°)
self.v_proj = nn.Linear(4096, 1024, bias=False)  # hidden â†’ V (GQA: æ›´å°)

# è®¡ç®—é‡ï¼šæ¯ä¸ª token éœ€è¦ in Ã— out æ¬¡ä¹˜åŠ æ“ä½œ
# Q: 4096 Ã— 4096 = 16.7M FLOPs/token
```

**æ ¸å¿ƒå®ç°**ï¼š

```python
# ç®€åŒ–ç‰ˆï¼ˆä¸è€ƒè™‘å¤šå¤´ï¼‰
self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
self.k_proj = nn.Linear(hidden_size, hidden_size, bias=False)
self.v_proj = nn.Linear(hidden_size, hidden_size, bias=False)

# å‰å‘ä¼ æ’­
Q = self.q_proj(hidden_states)  # (batch, seq_len, hidden_size)
K = self.k_proj(hidden_states)
V = self.v_proj(hidden_states)
```

**Multi-Head Attentionï¼ˆå¤šå¤´æ³¨æ„åŠ›ï¼‰**ï¼š

å®é™…ä¸Šæˆ‘ä»¬ä¼šæŠŠ hidden_size æ‹†æˆå¤šä¸ª"å¤´"ï¼š

```python
# Qwen3-8B é…ç½®
hidden_size = 4096
num_heads = 32
head_dim = hidden_size // num_heads  # = 128

# Q å®é™…å½¢çŠ¶
Q: (batch, seq_len, hidden_size)
   â”€â”€reshapeâ”€â”€â–º (batch, seq_len, num_heads, head_dim)
   â”€â”€transposeâ”€â”€â–º (batch, num_heads, seq_len, head_dim)
```

**Grouped Query Attention (GQA)**ï¼š

ç°ä»£ LLMï¼ˆå¦‚ Llama2-70Bã€Qwen3ï¼‰ä½¿ç”¨ GQA æ¥å‡å°‘ KV cache å¤§å°ï¼š

```
MHA:  32 ä¸ª Q heads, 32 ä¸ª K heads, 32 ä¸ª V heads
MQA:  32 ä¸ª Q heads,  1 ä¸ª K head,   1 ä¸ª V head
GQA:  32 ä¸ª Q heads,  8 ä¸ª K heads,  8 ä¸ª V headsï¼ˆQwen3-8Bï¼‰
                      â†‘
              æ¯ 4 ä¸ª Q heads å…±äº«ä¸€ç»„ KV
```

```python
# GQA å®ç°
self.q_proj = nn.Linear(hidden_size, num_heads * head_dim)       # 32 * 128 = 4096
self.k_proj = nn.Linear(hidden_size, num_kv_heads * head_dim)    # 8 * 128 = 1024
self.v_proj = nn.Linear(hidden_size, num_kv_heads * head_dim)    # 8 * 128 = 1024
```

**å‚æ•°é‡**ï¼š
- Q: `hidden_size Ã— (num_heads Ã— head_dim)` = `4096 Ã— 4096` â‰ˆ 16.8M
- K: `hidden_size Ã— (num_kv_heads Ã— head_dim)` = `4096 Ã— 1024` â‰ˆ 4.2M
- V: `hidden_size Ã— (num_kv_heads Ã— head_dim)` = `4096 Ã— 1024` â‰ˆ 4.2M
- O (è¾“å‡ºæŠ•å½±): `4096 Ã— 4096` â‰ˆ 16.8M
- **æ¯å±‚ Attention æ€»è®¡**ï¼šçº¦ 42M å‚æ•°

---

### 5) RoPEï¼šæ—‹è½¬ä½ç½®ç¼–ç 

Transformer çš„æ ¸å¿ƒæ“ä½œï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰æœ¬èº«æ˜¯**ä½ç½®æ— å…³**çš„â€”â€”æ‰“ä¹±è¾“å…¥é¡ºåºï¼Œè¾“å‡ºä¹Ÿåªæ˜¯ç›¸åº”æ‰“ä¹±ã€‚ä¸ºäº†è®©æ¨¡å‹ç†è§£"è°åœ¨å‰ã€è°åœ¨å"ï¼Œæˆ‘ä»¬éœ€è¦æ³¨å…¥ä½ç½®ä¿¡æ¯ã€‚

**ä½ç½®ç¼–ç çš„æ¼”è¿›**ï¼š

```
ç»å¯¹ä½ç½®ç¼–ç  (GPT-1/2)     â†’  å­¦ä¹ å›ºå®šä½ç½®å‘é‡ï¼Œç›´æ¥åŠ åˆ° embedding
æ­£å¼¦ä½ç½®ç¼–ç  (Transformer)  â†’  ç”¨ sin/cos ç”Ÿæˆä½ç½®å‘é‡ï¼ŒåŠ åˆ° embedding
ç›¸å¯¹ä½ç½®ç¼–ç  (T5, ALiBi)    â†’  åœ¨ attention åˆ†æ•°ä¸ŠåŠ åç½®
RoPE (Llama, Qwen, ...)    â†’  æ—‹è½¬ Q å’Œ K å‘é‡ â† ç°ä»£ä¸»æµ
```

**RoPE çš„æ ¸å¿ƒæ€æƒ³**ï¼š

æŠŠä½ç½®ä¿¡æ¯"æ—‹è½¬"è¿› Q å’Œ K å‘é‡ä¸­ï¼Œä½¿å¾—ä¸¤ä¸ªä½ç½® m å’Œ n çš„å‘é‡ç‚¹ç§¯è‡ªç„¶åŒ…å«å®ƒä»¬çš„**ç›¸å¯¹è·ç¦»** (m-n)ã€‚

```
ä½ç½® 0 çš„å‘é‡ï¼šä¸æ—‹è½¬
ä½ç½® 1 çš„å‘é‡ï¼šæ—‹è½¬ Î¸ åº¦
ä½ç½® 2 çš„å‘é‡ï¼šæ—‹è½¬ 2Î¸ åº¦
ä½ç½® m çš„å‘é‡ï¼šæ—‹è½¬ mÃ—Î¸ åº¦
```

![RoPE](images/rope.png)

**æ•°å­¦åŸç†**ï¼š

å¯¹äº 2D å‘é‡ï¼Œæ—‹è½¬çŸ©é˜µä¸ºï¼š

```
[cos(mÎ¸)  -sin(mÎ¸)]   [xâ‚€]   [xâ‚€Â·cos(mÎ¸) - xâ‚Â·sin(mÎ¸)]
[sin(mÎ¸)   cos(mÎ¸)] Ã— [xâ‚] = [xâ‚€Â·sin(mÎ¸) + xâ‚Â·cos(mÎ¸)]
```

å¯¹äºé«˜ç»´å‘é‡ï¼ˆå¦‚ head_dim=128ï¼‰ï¼Œæˆ‘ä»¬æŠŠå®ƒåˆ†æˆ 64 å¯¹ï¼Œæ¯å¯¹ä½¿ç”¨ä¸åŒé¢‘ç‡çš„ Î¸ã€‚

**è®ºæ–‡ä¸­çš„æ•°å­¦æè¿°** vs **PyTorch å®é™…å®ç°**ï¼š

è®ºæ–‡ä¸­æè¿°çš„æ˜¯ç›¸é‚»ç»´åº¦é…å¯¹ï¼š
```
ç»´åº¦ 0,1:  ä½¿ç”¨ Î¸â‚€ = base^(-0/d)     â†’ é«˜é¢‘ï¼Œå˜åŒ–å¿«
ç»´åº¦ 2,3:  ä½¿ç”¨ Î¸â‚ = base^(-2/d)     â†’
ç»´åº¦ 4,5:  ä½¿ç”¨ Î¸â‚‚ = base^(-4/d)     â†’
   ...
ç»´åº¦ 126,127: ä½¿ç”¨ Î¸â‚†â‚ƒ = base^(-126/d) â†’ ä½é¢‘ï¼Œå˜åŒ–æ…¢
```

ä½† **PyTorch å®é™…å®ç°æ˜¯å‰ååŠé…å¯¹**ï¼ˆä»¥ head_dim=4 ä¸ºä¾‹ï¼‰ï¼š
```
è®ºæ–‡æè¿°:    (0,1) ç”¨ Î¸â‚€,  (2,3) ç”¨ Î¸â‚
PyTorchå®ç°: (0,2) ç”¨ Î¸â‚€,  (1,3) ç”¨ Î¸â‚
```

è¿™æ˜¯å› ä¸º `rotate_half` å‡½æ•°å°†å‘é‡åˆ†æˆå‰åä¸¤åŠï¼Œè€Œä¸æ˜¯äº¤é”™å–ç›¸é‚»ç»´åº¦ï¼š
```python
x1 = x[..., :2]   # [xâ‚€, xâ‚] å‰åŠ
x2 = x[..., 2:]   # [xâ‚‚, xâ‚ƒ] ååŠ
rotate_half(x) = [-xâ‚‚, -xâ‚ƒ, xâ‚€, xâ‚]
```

**ä¸¤ç§æ–¹å¼æ•°å­¦ä¸Šå®Œå…¨ç­‰ä»·**ï¼Œåªæ˜¯ç»´åº¦æ’åˆ—ä¸åŒã€‚PyTorch è¿™æ ·å®ç°æ˜¯ä¸ºäº†åˆ©ç”¨è¿ç»­å†…å­˜è®¿é—®ï¼Œé¿å…äº¤é”™ç´¢å¼•å¸¦æ¥çš„æ€§èƒ½æŸå¤±ã€‚

å…¶ä¸­ `base` é€šå¸¸æ˜¯ 10000ï¼ˆåŸå§‹ Transformerï¼‰æˆ– 1000000ï¼ˆQwen3 é•¿ä¸Šä¸‹æ–‡ï¼‰ã€‚

**ä¸ºä»€ä¹ˆ RoPE èƒ½ç¼–ç ç›¸å¯¹ä½ç½®**ï¼š

å½“è®¡ç®— Q_m Â· K_n æ—¶ï¼ˆä½ç½® m çš„ Q å’Œä½ç½® n çš„ K çš„ç‚¹ç§¯ï¼‰ï¼š

```
Q_m Â· K_n = (R(mÎ¸) Â· q) Â· (R(nÎ¸) Â· k)
          = q Â· R((m-n)Î¸) Â· k    â† åªä¾èµ–ç›¸å¯¹è·ç¦» (m-n)ï¼
```

è¿™æ„å‘³ç€æ¨¡å‹"çœ‹åˆ°"çš„æ˜¯ä¸¤ä¸ª token ä¹‹é—´çš„è·ç¦»ï¼Œè€Œä¸æ˜¯å®ƒä»¬çš„ç»å¯¹ä½ç½®ã€‚

**æ ¸å¿ƒå®ç°**ï¼š

```python
class RotaryEmbedding(nn.Module):
    def __init__(self, head_dim: int, base: float = 10000.0):
        super().__init__()
        # è®¡ç®—æ¯å¯¹ç»´åº¦çš„é¢‘ç‡: Î¸_i = base^(-2i/d)
        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))
        self.register_buffer("inv_freq", inv_freq)

    def forward(self, seq_len: int, device: torch.device):
        # ä½ç½®ç´¢å¼•: [0, 1, 2, ..., seq_len-1]
        t = torch.arange(seq_len, device=device, dtype=torch.float32)

        # è®¡ç®—æ¯ä¸ªä½ç½®ã€æ¯ä¸ªé¢‘ç‡çš„è§’åº¦: (seq_len, head_dim/2)
        freqs = torch.outer(t, self.inv_freq)

        # å¤åˆ¶ä»¥åŒ¹é… head_dim: (seq_len, head_dim)
        emb = torch.cat((freqs, freqs), dim=-1)

        return emb.cos(), emb.sin()
```

**åº”ç”¨ RoPE åˆ° Q å’Œ K**ï¼š

```python
def apply_rotary_pos_emb(q, k, cos, sin):
    """
    q, k: (batch, num_heads, seq_len, head_dim)
    cos, sin: (seq_len, head_dim)
    """
    def rotate_half(x):
        """å°†ååŠéƒ¨åˆ†å–è´Ÿå¹¶ä¸å‰åŠéƒ¨åˆ†äº¤æ¢"""
        x1 = x[..., : x.shape[-1] // 2]
        x2 = x[..., x.shape[-1] // 2 :]
        return torch.cat((-x2, x1), dim=-1)

    # æ—‹è½¬å…¬å¼: x * cos + rotate_half(x) * sin
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)

    return q_embed, k_embed
```

**åœ¨ Attention ä¸­çš„ä½ç½®**ï¼š

```
Q_proj â”€â”€â–º Q â”€â”€â–º RoPE â”€â”€â”
                        â”œâ”€â”€â–º Attention Score â”€â”€â–º ...
K_proj â”€â”€â–º K â”€â”€â–º RoPE â”€â”€â”˜

V_proj â”€â”€â–º V â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º ...
                    ï¼ˆV ä¸éœ€è¦ RoPEï¼‰
```

**RoPE çš„ä¼˜åŠ¿**ï¼š

| ç‰¹æ€§ | è¯´æ˜ |
|------|------|
| **ç›¸å¯¹ä½ç½®** | ç‚¹ç§¯è‡ªç„¶åŒ…å«ç›¸å¯¹è·ç¦»ï¼Œæ— éœ€æ˜¾å¼è®¡ç®— |
| **å¤–æ¨èƒ½åŠ›** | ç†è®ºä¸Šå¯å¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„é•¿åº¦ |
| **é›¶å‚æ•°é‡** | ä¸å¢åŠ å¯å­¦ä¹ å‚æ•° |
| **è®¡ç®—é«˜æ•ˆ** | åªéœ€ç®€å•çš„ä¹˜æ³•å’ŒåŠ æ³• |
| **å…¼å®¹æ€§å¥½** | å¯ä¸ Flash Attention ç­‰ä¼˜åŒ–æŠ€æœ¯ç»“åˆ |

**é•¿ä¸Šä¸‹æ–‡æ‰©å±•**ï¼š

Qwen3 ä½¿ç”¨ `base=1000000`ï¼ˆè€ŒéåŸå§‹çš„ 10000ï¼‰ï¼Œä½¿ä½é¢‘æˆåˆ†å˜åŒ–æ›´æ…¢ï¼Œä»è€Œæ”¯æŒæ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼ˆ40K+ tokensï¼‰ã€‚è¿™ç§æŠ€æœ¯ç§°ä¸º **NTK-aware scaling** æˆ– **Dynamic NTK**ã€‚

---

### 6) Softmax å‡½æ•°ï¼šæ¦‚ç‡å½’ä¸€åŒ–

åœ¨è¿›å…¥ Attention ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç†è§£ **Softmax** å‡½æ•°â€”â€”å®ƒæ˜¯ Attention æœºåˆ¶çš„æ ¸å¿ƒç»„ä»¶ã€‚

#### Softmax æ˜¯ä»€ä¹ˆï¼Ÿ

Softmax å‡½æ•°å°†ä»»æ„å®æ•°å‘é‡è½¬æ¢ä¸º**æ¦‚ç‡åˆ†å¸ƒ**ï¼ˆæ‰€æœ‰å…ƒç´ éè´Ÿä¸”å’Œä¸º 1ï¼‰ã€‚

**æ•°å­¦å®šä¹‰**ï¼š

å¯¹äºè¾“å…¥å‘é‡ $z = [z_1, z_2, ..., z_n]$ï¼š

$$
\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}
$$

**ç›´è§‚ç†è§£**ï¼š

```
è¾“å…¥ scores:  [2.0,  1.0,  0.1]
              â†“ å–æŒ‡æ•° e^x
æŒ‡æ•°åŒ–:       [7.39, 2.72, 1.11]
              â†“ é™¤ä»¥æ€»å’Œ (7.39+2.72+1.11=11.22)
æ¦‚ç‡åˆ†å¸ƒ:     [0.66, 0.24, 0.10]  â† å’Œä¸º 1.0
```

**æ ¸å¿ƒæ€§è´¨**ï¼š

| æ€§è´¨ | è¯´æ˜ |
|------|------|
| **å½’ä¸€åŒ–** | è¾“å‡ºå’Œä¸º 1ï¼Œå¯è§£é‡Šä¸ºæ¦‚ç‡ |
| **ä¿åº** | è¾“å…¥è¶Šå¤§ï¼Œè¾“å‡ºæ¦‚ç‡è¶Šé«˜ |
| **å¯å¾®åˆ†** | æ¢¯åº¦å‹å¥½ï¼Œé€‚åˆåå‘ä¼ æ’­ |
| **æ”¾å¤§å·®å¼‚** | æŒ‡æ•°å‡½æ•°æ”¾å¤§å¤§å€¼ã€æŠ‘åˆ¶å°å€¼ |

**Softmax çš„"æ¸©åº¦"æ•ˆåº”**ï¼š

```python
# æ¸©åº¦å‚æ•°æ§åˆ¶åˆ†å¸ƒçš„"å°–é”ç¨‹åº¦"
def softmax_with_temperature(x, temperature=1.0):
    return F.softmax(x / temperature, dim=-1)

# ç¤ºä¾‹ï¼šx = [2.0, 1.0, 0.0]
# T=1.0 (é»˜è®¤):  [0.67, 0.24, 0.09]  â† æ­£å¸¸åˆ†å¸ƒ
# T=0.5 (ä½æ¸©):  [0.84, 0.11, 0.04]  â† æ›´å°–é”ï¼Œæ¥è¿‘ argmax
# T=2.0 (é«˜æ¸©):  [0.51, 0.31, 0.19]  â† æ›´å¹³æ»‘ï¼Œæ¥è¿‘å‡åŒ€åˆ†å¸ƒ
```

**æ•°å€¼ç¨³å®šæ€§é—®é¢˜**ï¼š

ç›´æ¥è®¡ç®— $e^{z_i}$ å¯èƒ½å¯¼è‡´æ•°å€¼æº¢å‡ºï¼ˆå½“ $z_i$ å¾ˆå¤§æ—¶ï¼‰ã€‚å®é™…å®ç°ä¼šå‡å»æœ€å¤§å€¼ï¼š

```python
def stable_softmax(x, dim=-1):
    # å‡å»æœ€å¤§å€¼ï¼Œé˜²æ­¢ exp æº¢å‡º
    x_max = x.max(dim=dim, keepdim=True).values
    exp_x = torch.exp(x - x_max)
    return exp_x / exp_x.sum(dim=dim, keepdim=True)
```

**ä¸ºä»€ä¹ˆè¿™æ ·åšæ˜¯å®‰å…¨çš„**ï¼š

$$
\frac{e^{z_i}}{\sum_j e^{z_j}} = \frac{e^{z_i - c}}{\sum_j e^{z_j - c}}
$$

å‡å»å¸¸æ•° $c = \max(z)$ åï¼Œæœ€å¤§çš„æŒ‡æ•°å˜æˆ $e^0 = 1$ï¼Œå…¶ä»–éƒ½æ˜¯å°äº 1 çš„æ­£æ•°ï¼Œé¿å…äº†æº¢å‡ºã€‚

**PyTorch å®ç°æ·±å…¥**ï¼š

```python
# PyTorch çš„ F.softmax å·²ç»å¤„ç†äº†æ•°å€¼ç¨³å®šæ€§
import torch.nn.functional as F

x = torch.tensor([1000.0, 1000.1, 1000.2])  # å¾ˆå¤§çš„æ•°
probs = F.softmax(x, dim=0)
print(probs)  # tensor([0.0900, 0.2447, 0.6652]) â† æ­£å¸¸å·¥ä½œ

# æ‰‹åŠ¨å®ç°ï¼ˆä¸å®‰å…¨ï¼‰ä¼šæº¢å‡º
# exp(1000) = inf!
```

#### Softmax åœ¨ Attention ä¸­çš„ä½œç”¨

åœ¨ Attention ä¸­ï¼ŒSoftmax å°†**æ³¨æ„åŠ›åˆ†æ•°ï¼ˆscoresï¼‰**è½¬æ¢ä¸º**æ³¨æ„åŠ›æƒé‡ï¼ˆweightsï¼‰**ï¼š

```
QÂ·K^T åˆ†æ•°:     [2.1,  -0.5,  1.3,  0.8]
              â†“ softmax
æ³¨æ„åŠ›æƒé‡:     [0.42,  0.03,  0.29,  0.26]  â† æ¦‚ç‡åˆ†å¸ƒ
              â”‚
              â–¼ åŠ æƒæ±‚å’Œ V
è¾“å‡º:          weighted sum of V vectors
```

è¿™æ„å‘³ç€ï¼š
- **é«˜åˆ†ä½ç½®**ï¼šæ¨¡å‹"æ³¨æ„"è¿™äº›ä½ç½®ï¼Œç»™äºˆæ›´é«˜æƒé‡
- **ä½åˆ†ä½ç½®**ï¼šè¢«"å¿½ç•¥"ï¼Œæƒé‡æ¥è¿‘ 0
- **æƒé‡å’Œä¸º 1**ï¼šè¾“å‡ºæ˜¯ V çš„å‡¸ç»„åˆ

---

### 7) Attentionï¼šæ³¨æ„åŠ›æœºåˆ¶
**æ ¸å¿ƒå…¬å¼**ï¼š

$$
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
$$

å…¶ä¸­ï¼š
- $Q$ï¼ˆQueryï¼‰ï¼šæŸ¥è¯¢çŸ©é˜µï¼Œè¡¨ç¤º"æˆ‘åœ¨æ‰¾ä»€ä¹ˆ"
- $K$ï¼ˆKeyï¼‰ï¼šé”®çŸ©é˜µï¼Œè¡¨ç¤º"æˆ‘æœ‰ä»€ä¹ˆ"
- $V$ï¼ˆValueï¼‰ï¼šå€¼çŸ©é˜µï¼Œè¡¨ç¤º"æˆ‘çš„å†…å®¹æ˜¯ä»€ä¹ˆ"
- $d_k$ï¼šKey å‘é‡çš„ç»´åº¦ï¼ˆç”¨äºç¼©æ”¾ï¼‰

<img src="images/attn.png" alt="Attention æœºåˆ¶ç¤ºæ„å›¾" width="60%">

---

#### ä¸ºä»€ä¹ˆéœ€è¦ Attentionï¼Ÿ

ä¼ ç»Ÿçš„åºåˆ—æ¨¡å‹ï¼ˆå¦‚ RNNï¼‰æœ‰ä¸€ä¸ªæ ¹æœ¬é—®é¢˜ï¼š**ä¿¡æ¯ç“¶é¢ˆ**ã€‚

```
RNN: æ‰€æœ‰å†å²ä¿¡æ¯å¿…é¡»å‹ç¼©åˆ°å›ºå®šå¤§å°çš„éšè—çŠ¶æ€
      åºåˆ—å¾ˆé•¿æ—¶ï¼Œæ—©æœŸä¿¡æ¯ä¼šè¢«"é—å¿˜"

Attention: æ¯ä¸ªä½ç½®å¯ä»¥ç›´æ¥è®¿é—®ä»»æ„å…¶ä»–ä½ç½®
           æ²¡æœ‰ä¿¡æ¯å‹ç¼©æŸå¤±
```

**Attention çš„æ ¸å¿ƒæ€æƒ³**ï¼šå¯¹äºæ¯ä¸ªæŸ¥è¯¢ï¼ˆQueryï¼‰ï¼Œåœ¨æ‰€æœ‰é”®å€¼å¯¹ï¼ˆKey-Valueï¼‰ä¸­æ‰¾åˆ°ç›¸å…³çš„å†…å®¹ï¼Œç„¶ååŠ æƒæ±‡æ€»ã€‚

#### Qã€Kã€V çš„ç›´è§‚ç†è§£

æŠŠ Attention æƒ³è±¡æˆä¸€ä¸ª**ä¿¡æ¯æ£€ç´¢ç³»ç»Ÿ**ï¼š

```
   ä½ åœ¨å›¾ä¹¦é¦†æ‰¾ä¹¦
        â”‚
        â–¼
   Query (Q): ä½ çš„é—®é¢˜/éœ€æ±‚
        "æˆ‘æƒ³æ‰¾å…³äº Python çš„å…¥é—¨ä¹¦"
        â”‚
        â–¼
   Key (K): æ¯æœ¬ä¹¦çš„ç´¢å¼•/æ ‡ç­¾
        ["Pythonå…¥é—¨", "Javaé«˜çº§", "æ•°æ®ç»“æ„", ...]
        â”‚
        â–¼
   åŒ¹é…ç¨‹åº¦ = Q Â· K^T
        [0.95, 0.1, 0.3, ...]  â† ç›¸å…³æ€§åˆ†æ•°
        â”‚
        â–¼
   Softmax â†’ æ³¨æ„åŠ›æƒé‡
        [0.7, 0.01, 0.15, ...]  â† å½’ä¸€åŒ–å
        â”‚
        â–¼
   Value (V): æ¯æœ¬ä¹¦çš„å®é™…å†…å®¹
        å–å‡ºç›¸å…³ä¹¦ç±ï¼Œæ ¹æ®ç›¸å…³æ€§åŠ æƒç»„åˆ
```

**Self-Attentionï¼ˆè‡ªæ³¨æ„åŠ›ï¼‰**ï¼š

åœ¨ Transformer ä¸­ï¼ŒQã€Kã€V éƒ½æ¥è‡ª**åŒä¸€ä¸ªè¾“å…¥åºåˆ—**ï¼ˆç»è¿‡ä¸åŒçš„çº¿æ€§æŠ•å½±ï¼‰ã€‚

```python
# è¾“å…¥: "The cat sat on the mat"
# æ¯ä¸ªè¯éƒ½ç”Ÿæˆè‡ªå·±çš„ Q, K, V

# å½“å¤„ç† "sat" è¿™ä¸ªè¯æ—¶:
Q_sat = "sat æƒ³è¦æŸ¥æ‰¾ä»€ä¹ˆä¿¡æ¯ï¼Ÿ"
K_*   = "æ‰€æœ‰è¯åœ¨'è¢«æŸ¥æ‰¾'æ—¶çš„è¡¨ç¤º"
V_*   = "æ‰€æœ‰è¯çš„å®é™…è¯­ä¹‰å†…å®¹"

# sat çš„è¾“å‡º = åŠ æƒç»„åˆæ‰€æœ‰è¯çš„ V
#   å¦‚æœ sat ä¸ cat ç›¸å…³ï¼Œcat çš„ V æƒé‡ä¼šè¾ƒé«˜
```

#### Attention å®Œæ•´è®¡ç®—æµç¨‹

```
         Q        K^T           Softmax          V
      (seq, d) Ã— (d, seq)  â†’  (seq, seq)  Ã—  (seq, d)  â†’  (seq, d)
         â”‚          â”‚            â”‚             â”‚
         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜            â”‚             â”‚
              â–¼                  â”‚             â”‚
           scores               â–¼             â”‚
         (seq, seq)          weights          â”‚
              â”‚             (seq, seq)         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                            output
                           (seq, d)
```

**åˆ†æ­¥è¯¦è§£**ï¼š

**Step 1: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼ˆScoresï¼‰**

$$
\text{scores} = QK^T
$$

```python
# Q: (batch, heads, seq_len, head_dim) = (1, 1, 4, 3)
# K: (batch, heads, seq_len, head_dim) = (1, 1, 4, 3)

# çŸ©é˜µä¹˜æ³•: Q @ K.T
# å½¢çŠ¶å˜åŒ–: (4, 3) @ (3, 4) = (4, 4)

# ç»“æœ scores[i][j] = Q[i] ä¸ K[j] çš„ç‚¹ç§¯
#                   = ä½ç½® i å¯¹ä½ç½® j çš„"åŸå§‹æ³¨æ„åŠ›"
```

**ç‚¹ç§¯å¦‚ä½•è¡¡é‡ç›¸ä¼¼åº¦**ï¼š

```
å‘é‡ A = [1, 0]
å‘é‡ B = [1, 0]  â†’ AÂ·B = 1  (ç›¸åŒæ–¹å‘ï¼Œé«˜ç›¸ä¼¼)
å‘é‡ C = [0, 1]  â†’ AÂ·C = 0  (æ­£äº¤ï¼Œä¸ç›¸å…³)
å‘é‡ D = [-1, 0] â†’ AÂ·D = -1 (ç›¸åæ–¹å‘ï¼Œè´Ÿç›¸å…³)
```

**Step 2: ç¼©æ”¾ï¼ˆScalingï¼‰**

$$
\text{ScaledScores} = \frac{QK^T}{\sqrt{d_k}}
$$

```python
d_k = head_dim  # 128 for Qwen3
scores = scores / math.sqrt(d_k)
```

**ä¸ºä»€ä¹ˆè¦é™¤ä»¥ âˆšd_k**ï¼š

å½“ $d_k$ å¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯çš„**æ–¹å·®**ä¼šå˜å¤§ï¼š

```
å‡è®¾ Q å’Œ K çš„æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ ~N(0,1)

ç‚¹ç§¯ = sum(Q[i] * K[i] for i in range(d_k))
     = d_k ä¸ªç‹¬ç«‹éšæœºå˜é‡çš„å’Œ

æ–¹å·® = d_k  (æ¯é¡¹æ–¹å·®=1)
æ ‡å‡†å·® = âˆšd_k

å½“ d_k=128 æ—¶ï¼Œç‚¹ç§¯å€¼å¯èƒ½åœ¨ [-20, 20] èŒƒå›´
Softmax ä¼šå˜å¾—éå¸¸"å°–é”" â†’ æ¢¯åº¦æ¶ˆå¤±
```

é™¤ä»¥ âˆšd_k åï¼Œæ–¹å·®å›åˆ° ~1ï¼Œsoftmax è¾“å‡ºæ›´å¹³æ»‘ã€‚

**Step 3: åº”ç”¨æ©ç ï¼ˆMaskingï¼‰**

å¯¹äºå› æœè¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTï¼‰ï¼Œä½ç½® t ä¸èƒ½çœ‹åˆ° t+1, t+2, ... çš„ä¿¡æ¯ï¼š

```python
# æ©ç çŸ©é˜µï¼ˆ4x4 çš„ä¾‹å­ï¼‰
mask = [
    [  0, -âˆ, -âˆ, -âˆ],   # ä½ç½®0 åªèƒ½çœ‹ä½ç½®0
    [  0,  0, -âˆ, -âˆ],   # ä½ç½®1 èƒ½çœ‹ 0,1
    [  0,  0,  0, -âˆ],   # ä½ç½®2 èƒ½çœ‹ 0,1,2
    [  0,  0,  0,  0],   # ä½ç½®3 èƒ½çœ‹ 0,1,2,3
]

scores = scores + mask
# -âˆ çš„ä½ç½®åœ¨ softmax åä¼šå˜æˆ 0
```

**Step 4: Softmax å½’ä¸€åŒ–**

$$
\text{Weights} = \text{softmax}(\text{ScaledScores})
$$

```python
attn_weights = F.softmax(scores, dim=-1)
# æ¯ä¸€è¡Œå’Œä¸º 1
# attn_weights[i] è¡¨ç¤ºä½ç½® i å¯¹æ‰€æœ‰ä½ç½®çš„æ³¨æ„åŠ›åˆ†å¸ƒ
```

**Step 5: åŠ æƒæ±‚å’Œ Value**

$$
\text{Output} = \text{Weights} \cdot V
$$

```python
output = torch.matmul(attn_weights, V)
# output[i] = sum(weights[i][j] * V[j] for all j)
#           = æ‰€æœ‰ä½ç½® V çš„åŠ æƒç»„åˆ
```

#### æ ¸å¿ƒå®ç°

```python
def scaled_dot_product_attention(Q, K, V, mask=None):
    """
    Q, K, V: (batch, num_heads, seq_len, head_dim)
    """
    d_k = Q.shape[-1]

    # 1. è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
    # scores: (batch, num_heads, seq_len, seq_len)

    # 2. åº”ç”¨å› æœæ©ç ï¼ˆautoregressive generationï¼‰
    if mask is not None:
        scores = scores + mask  # mask ä¸­æœªæ¥ä½ç½®æ˜¯ -inf

    # 3. Softmax å½’ä¸€åŒ–
    attn_weights = F.softmax(scores, dim=-1)
    # attn_weights: (batch, num_heads, seq_len, seq_len)

    # 4. åŠ æƒæ±‚å’Œ
    output = torch.matmul(attn_weights, V)
    # output: (batch, num_heads, seq_len, head_dim)

    return output, attn_weights
```

#### å› æœæ©ç ï¼ˆCausal Maskï¼‰

å¯¹äºè‡ªå›å½’ç”Ÿæˆï¼Œä½ç½® t åªèƒ½çœ‹åˆ°ä½ç½® 0~tï¼Œä¸èƒ½çœ‹åˆ°æœªæ¥ï¼š

```python
def causal_mask(seq_len):
    """
    è¿”å›:
    [[  0, -inf, -inf, -inf],
     [  0,    0, -inf, -inf],
     [  0,    0,    0, -inf],
     [  0,    0,    0,    0]]
    """
    mask = torch.full((seq_len, seq_len), float("-inf"))
    mask = torch.triu(mask, diagonal=1)
    return mask
```

#### Attention çš„å¯è§†åŒ–ç†è§£

```
è¾“å…¥å¥å­: "The cat sat on the mat"

Attention æƒé‡çŸ©é˜µï¼ˆæŸä¸€å±‚æŸä¸€å¤´ï¼‰:

          The   cat   sat   on   the   mat
    The  [0.9   0.05  0.02  0.01  0.01  0.01]
    cat  [0.3   0.5   0.1   0.05  0.03  0.02]
    sat  [0.1   0.4   0.3   0.1   0.05  0.05]  â† sat ä¸»è¦å…³æ³¨ cat
    on   [0.1   0.1   0.2   0.4   0.1   0.1 ]
    the  [0.05  0.1   0.1   0.1   0.3   0.35]
    mat  [0.05  0.2   0.1   0.1   0.15  0.4 ]  â† mat å…³æ³¨è‡ªå·±å’Œ cat
```

**ä¸åŒå±‚çš„ Attention æ¨¡å¼**ï¼š

- **æµ…å±‚**ï¼šé€šå¸¸å…³æ³¨å±€éƒ¨ä¸Šä¸‹æ–‡ã€è¯­æ³•ç»“æ„
- **æ·±å±‚**ï¼šæ•æ‰è¯­ä¹‰å…³ç³»ã€é•¿è·ç¦»ä¾èµ–
- **æŸäº›å¤´**ï¼šä¸“é—¨å…³æ³¨"å‰ä¸€ä¸ªè¯"æˆ–"æ ‡ç‚¹"
- **æŸäº›å¤´**ï¼šä¸“é—¨å…³æ³¨"ä¸»è°“å…³ç³»"æˆ–"æŒ‡ä»£"

#### å¤æ‚åº¦åˆ†æ

| æ“ä½œ | å¤æ‚åº¦ | è¯´æ˜ |
|------|--------|------|
| QÂ·K^T | O(nÂ² Â· d) | n=åºåˆ—é•¿åº¦ï¼Œd=head_dim |
| Softmax | O(nÂ²) | æ¯è¡Œ n ä¸ªå…ƒç´  |
| WeightsÂ·V | O(nÂ² Â· d) | çŸ©é˜µä¹˜æ³• |
| **æ€»è®¡** | **O(nÂ² Â· d)** | è¿™æ˜¯é•¿ä¸Šä¸‹æ–‡çš„ä¸»è¦ç“¶é¢ˆ |

**ä¸ºä»€ä¹ˆ O(nÂ²) æ˜¯é—®é¢˜**ï¼š

```
n = 1K   â†’ nÂ² = 1M æ¬¡æ“ä½œ    â† å¯æ¥å—
n = 4K   â†’ nÂ² = 16M æ¬¡æ“ä½œ   â† å¼€å§‹å˜æ…¢
n = 32K  â†’ nÂ² = 1B æ¬¡æ“ä½œ    â† å¾ˆæ…¢
n = 128K â†’ nÂ² = 16B æ¬¡æ“ä½œ   â† éå¸¸æ…¢ï¼Œæ˜¾å­˜çˆ†ç‚¸
```

**Flash Attention ç­‰ä¼˜åŒ–**ï¼šé€šè¿‡å·§å¦™çš„åˆ†å—è®¡ç®—ï¼Œåœ¨ä¿æŒ O(nÂ²) è®¡ç®—é‡çš„åŒæ—¶ï¼Œå¤§å¹…å‡å°‘**æ˜¾å­˜å ç”¨**ï¼ˆä» O(nÂ²) é™åˆ° O(n)ï¼‰ã€‚

---

### 8) æ¿€æ´»å‡½æ•°ï¼šç¥ç»ç½‘ç»œçš„"éçº¿æ€§é­”æ³•"

åœ¨è®² MLP ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦ç†è§£**æ¿€æ´»å‡½æ•°**â€”â€”å®ƒæ˜¯ç¥ç»ç½‘ç»œè¡¨è¾¾èƒ½åŠ›çš„å…³é”®ã€‚

#### ä¸ºä»€ä¹ˆéœ€è¦æ¿€æ´»å‡½æ•°ï¼Ÿ

å¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œç¥ç»ç½‘ç»œæ— è®ºå¤šå°‘å±‚ï¼Œæœ¬è´¨ä¸Šéƒ½æ˜¯**çº¿æ€§å˜æ¢**ï¼š

```
çº¿æ€§å±‚1: yâ‚ = Wâ‚x + bâ‚
çº¿æ€§å±‚2: yâ‚‚ = Wâ‚‚yâ‚ + bâ‚‚ = Wâ‚‚(Wâ‚x + bâ‚) + bâ‚‚ = (Wâ‚‚Wâ‚)x + (Wâ‚‚bâ‚ + bâ‚‚)
                                                â†‘           â†‘
                                           ç­‰æ•ˆå•å±‚æƒé‡   ç­‰æ•ˆåç½®
```

**å¤šå±‚çº¿æ€§ç½‘ç»œ = å•å±‚çº¿æ€§ç½‘ç»œ**ï¼æ— æ³•å­¦ä¹ å¤æ‚çš„éçº¿æ€§æ¨¡å¼ã€‚

æ¿€æ´»å‡½æ•°åœ¨æ¯å±‚ä¹‹é—´å¼•å…¥**éçº¿æ€§**ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿé€¼è¿‘ä»»æ„å¤æ‚çš„å‡½æ•°ã€‚

#### å¸¸è§æ¿€æ´»å‡½æ•°

**1. ReLUï¼ˆRectified Linear Unitï¼‰**

æœ€ç®€å•ä¹Ÿæœ€å¸¸ç”¨çš„æ¿€æ´»å‡½æ•°ï¼š

$$
\text{ReLU}(x) = \max(0, x)
$$

```python
def relu(x):
    return torch.maximum(x, torch.zeros_like(x))
```

```
è¾“å…¥:  [-2, -1, 0, 1, 2]
è¾“å‡º:  [ 0,  0, 0, 1, 2]
```

**ä¼˜ç‚¹**ï¼šè®¡ç®—ç®€å•ã€æ¢¯åº¦ä¸ä¼šé¥±å’Œï¼ˆæ­£åŒºé—´ï¼‰
**ç¼ºç‚¹**ï¼šè´Ÿæ•°åŒºåŸŸæ¢¯åº¦ä¸º0ï¼ˆ"æ­»äº¡ReLU"é—®é¢˜ï¼‰

**2. GELUï¼ˆGaussian Error Linear Unitï¼‰**

BERTã€GPT-2 ç­‰æ—©æœŸ Transformer æ¨¡å‹ä½¿ç”¨ï¼š

$$
\text{GELU}(x) = x \cdot \Phi(x)
$$

å…¶ä¸­ $\Phi(x)$ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•°ã€‚

```python
def gelu(x):
    # è¿‘ä¼¼å®ç°
    return 0.5 * x * (1 + torch.tanh(math.sqrt(2/math.pi) * (x + 0.044715 * x**3)))
```

**ç‰¹ç‚¹**ï¼šæ¯” ReLU æ›´å¹³æ»‘ï¼Œæ¦‚ç‡æ€§åœ°"æŠ‘åˆ¶"è¾“å…¥

**3. SiLU / Swishï¼ˆç°ä»£ LLM ä¸»æµï¼‰**

Llamaã€Qwenã€Mistral ç­‰ç°ä»£ LLM ä½¿ç”¨ï¼š

$$
\text{SiLU}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}
$$

```python
def silu(x):
    return x * torch.sigmoid(x)
```

```
è¾“å…¥:  [-2,   -1,    0,   1,     2   ]
è¾“å‡º:  [-0.24, -0.27, 0,   0.73,  1.76]
```

**å¯è§†åŒ–å¯¹æ¯”**ï¼š

![æ¿€æ´»å‡½æ•°å¯¹æ¯”ï¼šReLUã€GELUã€SiLU](images/activation.png)

**ä¸ºä»€ä¹ˆ SiLU æˆä¸ºä¸»æµ**ï¼š

| ç‰¹æ€§ | ReLU | GELU | SiLU |
|------|------|------|------|
| å¹³æ»‘æ€§ | âŒ ä¸å¹³æ»‘ | âœ… å¹³æ»‘ | âœ… å¹³æ»‘ |
| è´Ÿå€¼åŒºåŸŸ | å…¨éƒ¨å½’é›¶ | å¤§éƒ¨åˆ†å½’é›¶ | ä¿ç•™å°‘é‡è´Ÿå€¼ |
| è®¡ç®—æ•ˆç‡ | â­â­â­ | â­â­ | â­â­â­ |
| å®é™…æ€§èƒ½ | è‰¯å¥½ | æ›´å¥½ | æœ€ä½³ |


#### æ¿€æ´»å‡½æ•°åœ¨ LLM ä¸­çš„ä½ç½®

æ¿€æ´»å‡½æ•°ä¸»è¦å‡ºç°åœ¨ **MLP å±‚**ä¸­ï¼š

```
MLP ç»“æ„:
input â”€â”€â–º Linear â”€â”€â–º æ¿€æ´»å‡½æ•° â”€â”€â–º Linear â”€â”€â–º output
                        â†‘
                   å¼•å…¥éçº¿æ€§
```

---

### 9) MLPï¼ˆå‰é¦ˆç½‘ç»œï¼‰

æ¯ä¸ª Transformer å±‚çš„å¦ä¸€åŠæ˜¯ MLPï¼ˆä¹Ÿå« FFNï¼‰ã€‚å®ƒå¯¹æ¯ä¸ª token ä½ç½®ç‹¬ç«‹åšéçº¿æ€§å˜æ¢ã€‚

**ä¼ ç»Ÿ FFN**ï¼š

```
x â”€â”€â–º Linear(dâ†’4d) â”€â”€â–º ReLU â”€â”€â–º Linear(4dâ†’d) â”€â”€â–º output
```

**ç°ä»£ LLM ä½¿ç”¨ SwiGLU**ï¼ˆQwen3ã€Llamaï¼‰ï¼š

```
        â”Œâ”€â”€â–º gate_proj â”€â”€â–º SiLU â”€â”€â”€â”
        â”‚                          â”‚
x â”€â”€â”€â”€â”€â”€â”¤                          â”œâ”€â”€â–º é€å…ƒç´ ä¹˜ â”€â”€â–º down_proj â”€â”€â–º output
        â”‚                          â”‚
        â””â”€â”€â–º up_proj â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**SwiGLU ç®—æ³•åŸç†**ï¼š

SwiGLUï¼ˆSwish-Gated Linear Unitï¼‰ç”± Noam Shazeer åœ¨ 2020 å¹´æå‡ºï¼Œæ˜¯ GLUï¼ˆGated Linear Unitï¼‰å˜ä½“å®¶æ—ä¸­çš„ä¸€ç§ã€‚å…¶æ ¸å¿ƒæ€æƒ³æ˜¯ç”¨**é—¨æ§æœºåˆ¶**æ›¿ä»£ä¼ ç»Ÿ FFN ä¸­çš„å•ä¸€æ¿€æ´»å‡½æ•°ï¼š

$$\text{SwiGLU}(x) = \text{SiLU}(xW_{\text{gate}}) \otimes (xW_{\text{up}})$$

å…¶ä¸­ï¼š
- $\text{SiLU}(x) = x \cdot \sigma(x)$ï¼ˆä¹Ÿå« Swishï¼‰ï¼Œæ˜¯ä¸€ä¸ª**å¹³æ»‘çš„éå•è°ƒ**æ¿€æ´»å‡½æ•°
- $W_{\text{gate}}$ å’Œ $W_{\text{up}}$ æ˜¯ä¸¤ä¸ªç‹¬ç«‹çš„çº¿æ€§æŠ•å½±ï¼Œå°†è¾“å…¥ä» $d$ ç»´æ˜ å°„åˆ°ä¸­é—´ç»´åº¦
- $\otimes$ è¡¨ç¤ºé€å…ƒç´ ç›¸ä¹˜ï¼Œå³ gate åˆ†æ”¯æ§åˆ¶ up åˆ†æ”¯ä¸­å“ªäº›ä¿¡æ¯è¢«ä¿ç•™æˆ–æŠ‘åˆ¶
- æœ€åé€šè¿‡ $W_{\text{down}}$ å°†ä¸­é—´ç»´åº¦æ˜ å°„å› $d$ ç»´

**ä¸ä¼ ç»Ÿ FFN çš„åŒºåˆ«**ï¼šä¼ ç»Ÿ FFN åªæœ‰ä¸€æ¡è·¯å¾„ + ä¸€ä¸ªæ¿€æ´»å‡½æ•°ï¼ˆå¦‚ ReLUï¼‰ï¼Œè€Œ SwiGLU å°†è¾“å…¥åˆ†æˆä¸¤æ¡è·¯å¾„â€”â€”ä¸€æ¡æä¾›å†…å®¹ï¼ˆup_projï¼‰ï¼Œä¸€æ¡æä¾›é—¨æ§ä¿¡å·ï¼ˆgate_proj + SiLUï¼‰ï¼Œä¸¤è€…ç›¸ä¹˜åšä¿¡æ¯ç­›é€‰ã€‚è¿™ä½¿æ¨¡å‹èƒ½æ›´ç²¾ç»†åœ°æ§åˆ¶ä¿¡æ¯æµåŠ¨ï¼Œå®éªŒè¡¨æ˜åœ¨ç›¸åŒå‚æ•°é‡ä¸‹æ€§èƒ½ä¼˜äº ReLU/GELU ç­‰ä¼ ç»Ÿæ–¹æ¡ˆã€‚

**æ ¸å¿ƒå®ç°**ï¼š

```python
class MLP(nn.Module):
    def __init__(self, hidden_size: int, intermediate_size: int):
        super().__init__()
        # Qwen3-8B: hidden_size=4096, intermediate_size=14336
        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)
        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)

    def forward(self, x):
        # SwiGLU: silu(gate(x)) * up(x)
        gate = F.silu(self.gate_proj(x))  # SiLU = x * sigmoid(x)
        up = self.up_proj(x)
        return self.down_proj(gate * up)
```

**å‚æ•°é‡**ï¼ˆæ¯å±‚ MLPï¼‰ï¼š
- gate_proj: `4096 Ã— 14336` â‰ˆ 58.7M
- up_proj: `4096 Ã— 14336` â‰ˆ 58.7M
- down_proj: `14336 Ã— 4096` â‰ˆ 58.7M
- **æ¯å±‚ MLP æ€»è®¡**ï¼šçº¦ 176M å‚æ•°

**å…³é”®ç‚¹**ï¼šMLP é€šå¸¸å æ¨¡å‹å‚æ•°é‡çš„ **60-70%**ï¼

---

### 10) Attention vs MLPï¼šåˆ†å·¥ä¸åä½œ

Attention å’Œ MLP æ˜¯ Transformer çš„ä¸¤å¤§æ ¸å¿ƒç»„ä»¶ï¼Œå®ƒä»¬**åŠŸèƒ½äº’è¡¥**ï¼Œç¼ºä¸€ä¸å¯ã€‚

#### æ ¸å¿ƒåŠŸèƒ½å¯¹æ¯”

| ç»´åº¦ | Attention | MLP |
|------|-----------|-----|
| **ä¸»è¦åŠŸèƒ½** | Token é—´çš„ä¿¡æ¯äº¤äº’ | æ¯ä¸ª Token çš„ç‰¹å¾å˜æ¢ |
| **ä½œç”¨èŒƒå›´** | è·¨ä½ç½®ï¼ˆå…¨å±€ï¼‰ | å•ä½ç½®ï¼ˆå±€éƒ¨ï¼‰ |
| **æ ¸å¿ƒæ“ä½œ** | åŠ æƒèšåˆå…¶ä»–ä½ç½®çš„ä¿¡æ¯ | éçº¿æ€§ç‰¹å¾æ˜ å°„ |
| **ç±»æ¯”** | "å¼€ä¼šè®¨è®º" â€” æ”¶é›†ä»–äººæ„è§ | "ç‹¬ç«‹æ€è€ƒ" â€” å¤„ç†æ¶ˆåŒ–ä¿¡æ¯ |
| **å‚æ•°å æ¯”** | ~20-30% | ~70-80% |


#### ä¸¤è€…å¦‚ä½•åä½œï¼Ÿ

åœ¨æ¯ä¸ª Transformer å±‚ä¸­ï¼ŒAttention å’Œ MLP **ä¸²è¡Œé…åˆ**ï¼š

```
è¾“å…¥ Token è¡¨ç¤º
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Attention: "çœ‹çœ‹å‘¨å›´çš„ Token"           â”‚
â”‚                                          â”‚
â”‚  - Q: æˆ‘éœ€è¦ä»€ä¹ˆä¿¡æ¯ï¼Ÿ                    â”‚
â”‚  - K: å…¶ä»–ä½ç½®æœ‰ä»€ä¹ˆä¿¡æ¯ï¼Ÿ                â”‚
â”‚  - V: å–å›ç›¸å…³ä¿¡æ¯                        â”‚
â”‚  - è¾“å‡º: èåˆäº†ä¸Šä¸‹æ–‡çš„è¡¨ç¤º               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼ (+ æ®‹å·®è¿æ¥)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MLP: "å¤„ç†è¿™äº›ä¿¡æ¯"                      â”‚
â”‚                                          â”‚
â”‚  - å¯¹æ¯ä¸ªä½ç½®ç‹¬ç«‹åšéçº¿æ€§å˜æ¢             â”‚
â”‚  - æå–æ›´é«˜å±‚æ¬¡çš„ç‰¹å¾                     â”‚
â”‚  - æ³¨å…¥"ä¸–ç•ŒçŸ¥è¯†"ï¼ˆå­˜å‚¨åœ¨æƒé‡ä¸­ï¼‰         â”‚
â”‚  - è¾“å‡º: æ›´ä¸°å¯Œçš„è¡¨ç¤º                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼ (+ æ®‹å·®è¿æ¥)
       â”‚
       â–¼
è¾“å‡º Token è¡¨ç¤º
```

#### ä¸ºä»€ä¹ˆè¿™ç§åˆ†å·¥æœ‰æ•ˆï¼Ÿ

**1. ä¿¡æ¯æµåŠ¨ vs ä¿¡æ¯å¤„ç†**

```
Attention åªåš"åŠ æƒå¹³å‡"ï¼Œæ˜¯çº¿æ€§æ“ä½œï¼ˆå¯¹ V è€Œè¨€ï¼‰
â†’ éœ€è¦ MLP çš„éçº¿æ€§æ¥æå‡è¡¨è¾¾èƒ½åŠ›

MLP åªçœ‹å•ä¸ªä½ç½®ï¼Œæ— æ³•è·å–ä¸Šä¸‹æ–‡
â†’ éœ€è¦ Attention æ¥æ”¶é›†å…¶ä»–ä½ç½®çš„ä¿¡æ¯
```

**2. "çŸ¥è¯†å­˜å‚¨"çš„åˆ†å·¥**

ç ”ç©¶è¡¨æ˜ï¼š
- **Attention**ï¼šä¸»è¦å­¦ä¹ "å¦‚ä½•ç»„åˆä¿¡æ¯"ï¼ˆè¯­æ³•ç»“æ„ã€æŒ‡ä»£å…³ç³»ç­‰ï¼‰
- **MLP**ï¼šä¸»è¦å­˜å‚¨"äº‹å®çŸ¥è¯†"ï¼ˆå·´é»æ˜¯æ³•å›½é¦–éƒ½ã€æ°´çš„åŒ–å­¦å¼æ˜¯ Hâ‚‚O ç­‰ï¼‰

è¿™ä¹Ÿæ˜¯ä¸ºä»€ä¹ˆ **MLP å‚æ•°é‡è¿œå¤§äº Attention** â€” éœ€è¦å­˜å‚¨å¤§é‡ä¸–ç•ŒçŸ¥è¯†ï¼

**3. å±‚æ•°å †å çš„æ•ˆæœ**

```
Layer 1:  Attention â†’ MLP
             â†“
Layer 2:  Attention â†’ MLP    æ¯å±‚éƒ½åœ¨å‰ä¸€å±‚çš„åŸºç¡€ä¸Š
             â†“               è¿›ä¸€æ­¥æå–å’ŒæŠ½è±¡ç‰¹å¾
Layer 3:  Attention â†’ MLP
            ...

```

#### ä¸€å¥è¯æ€»ç»“

> **Attention è´Ÿè´£"çœ‹åˆ°"ä¸Šä¸‹æ–‡ï¼ŒMLP è´Ÿè´£"ç†è§£"çœ‹åˆ°çš„å†…å®¹ã€‚ä¸¤è€…äº’è¡¥ï¼Œç¼ºä¸€ä¸å¯ã€‚**

---

### 11) å®Œæ•´çš„ Transformer å±‚

æŠŠä¸Šé¢çš„ç»„ä»¶ç»„åˆèµ·æ¥ï¼Œä¸€ä¸ªå®Œæ•´çš„ Transformer å±‚ï¼ˆPre-Norm æ¶æ„ï¼‰å¦‚ä¸‹ï¼š

```
Input Hidden States
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ RMSNorm â”‚ â†â”€â”€ input_layernorm
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
        â”‚
        â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚           Self-Attention             â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
   â”‚  â”‚ Q/K/V Proj â”€â”€â–º RoPE â”€â”€â–º Attn â”‚    â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
   â”‚               â”‚                      â”‚
   â”‚               â–¼ O Proj               â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€ + Residual â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                   â”‚
        â–¼                                   â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
   â”‚ RMSNorm â”‚ â†â”€â”€ post_attention_layernorm â”‚
   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                              â”‚
        â”‚                                   â”‚
        â–¼                                   â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
   â”‚      MLP        â”‚                      â”‚
   â”‚    (SwiGLU)     â”‚                      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
            â”‚                               â”‚
            â”œâ”€â”€â”€â”€â”€â”€â”€â”€ + Residual â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
     Output Hidden States
```

**ä»£ç å®ç°**ï¼š

```python
class TransformerBlock(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.input_layernorm = RMSNorm(config.hidden_size)
        self.self_attn = Attention(config)
        self.post_attention_layernorm = RMSNorm(config.hidden_size)
        self.mlp = MLP(config)

    def forward(self, x, attention_mask=None, position_ids=None):
        # 1. Self-Attention (with residual)
        residual = x
        x = self.input_layernorm(x)
        x = self.self_attn(x, attention_mask, position_ids)
        x = residual + x

        # 2. MLP (with residual)
        residual = x
        x = self.post_attention_layernorm(x)
        x = self.mlp(x)
        x = residual + x

        return x
```

---

### 12) å‚æ•°é‡ä»å“ªé‡Œæ¥ï¼Ÿ

ç†è§£ LLM çš„å‚æ•°é‡åˆ†å¸ƒå¯¹äºæ¨¡å‹éƒ¨ç½²ã€æ˜¾å­˜ä¼°ç®—å’Œæ€§èƒ½ä¼˜åŒ–è‡³å…³é‡è¦ã€‚ä¸‹é¢æˆ‘ä»¬ä»¥ **Qwen3-8B** ä¸ºä¾‹ï¼Œé€å±‚è¯¦ç»†è®¡ç®—æ¯ä¸ªç»„ä»¶çš„å‚æ•°é‡ã€‚

#### Qwen3-8B æ¨¡å‹é…ç½®

é¦–å…ˆï¼Œåˆ—å‡º Qwen3-8B çš„å…³é”®é…ç½®å‚æ•°ï¼š

```python
# Qwen3-8B é…ç½®
vocab_size = 151936          # è¯è¡¨å¤§å°
hidden_size = 4096           # éšè—å±‚ç»´åº¦ (d_model)
num_hidden_layers = 36       # Transformer å±‚æ•°
num_attention_heads = 32     # Q çš„æ³¨æ„åŠ›å¤´æ•°
num_key_value_heads = 8      # K/V çš„æ³¨æ„åŠ›å¤´æ•° (GQA)
head_dim = 128               # æ¯ä¸ªå¤´çš„ç»´åº¦ (hidden_size / num_attention_heads)
intermediate_size = 14336    # MLP ä¸­é—´å±‚ç»´åº¦
rms_norm_eps = 1e-6          # RMSNorm epsilon
```

#### å‚æ•°é‡è®¡ç®—å…¬å¼

å¯¹äº Linear å±‚ï¼ˆæ—  biasï¼‰ï¼š**å‚æ•°é‡ = input_dim Ã— output_dim**

---

#### ç¬¬ä¸€éƒ¨åˆ†ï¼šEmbedding å±‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Embedding å±‚                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  embed_tokens: vocab_size Ã— hidden_size                      â”‚
â”‚              = 151936 Ã— 4096                                 â”‚
â”‚              = 622,329,856 â‰ˆ 622.3M                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**è®¡ç®—è¯¦è§£**ï¼š
- è¯è¡¨ä¸­æ¯ä¸ª token éœ€è¦ä¸€ä¸ª `hidden_size` ç»´çš„å‘é‡
- æ€»å‚æ•° = 151,936 Ã— 4,096 = **622,329,856** (çº¦ 622M)

---

#### ç¬¬äºŒéƒ¨åˆ†ï¼šå•ä¸ª Transformer å±‚

æ¯ä¸ª Transformer å±‚åŒ…å« **Self-Attention** å’Œ **MLP** ä¸¤å¤§éƒ¨åˆ†ã€‚

##### 2.1 Self-Attention éƒ¨åˆ†

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Self-Attention å±‚                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â‘  Q Projection (q_proj)                                    â”‚
â”‚     è¾“å…¥: hidden_size = 4096                                 â”‚
â”‚     è¾“å‡º: num_heads Ã— head_dim = 32 Ã— 128 = 4096            â”‚
â”‚     å‚æ•°: 4096 Ã— 4096 = 16,777,216 â‰ˆ 16.78M                 â”‚
â”‚                                                              â”‚
â”‚  â‘¡ K Projection (k_proj) â€” GQA: åªæœ‰ 8 ä¸ª KV heads          â”‚
â”‚     è¾“å…¥: hidden_size = 4096                                 â”‚
â”‚     è¾“å‡º: num_kv_heads Ã— head_dim = 8 Ã— 128 = 1024          â”‚
â”‚     å‚æ•°: 4096 Ã— 1024 = 4,194,304 â‰ˆ 4.19M                   â”‚
â”‚                                                              â”‚
â”‚  â‘¢ V Projection (v_proj) â€” GQA: åªæœ‰ 8 ä¸ª KV heads          â”‚
â”‚     è¾“å…¥: hidden_size = 4096                                 â”‚
â”‚     è¾“å‡º: num_kv_heads Ã— head_dim = 8 Ã— 128 = 1024          â”‚
â”‚     å‚æ•°: 4096 Ã— 1024 = 4,194,304 â‰ˆ 4.19M                   â”‚
â”‚                                                              â”‚
â”‚  â‘£ Output Projection (o_proj)                               â”‚
â”‚     è¾“å…¥: num_heads Ã— head_dim = 32 Ã— 128 = 4096            â”‚
â”‚     è¾“å‡º: hidden_size = 4096                                 â”‚
â”‚     å‚æ•°: 4096 Ã— 4096 = 16,777,216 â‰ˆ 16.78M                 â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Attention æ€»è®¡: 16.78M + 4.19M + 4.19M + 16.78M            â”‚
â”‚                = 41,943,040 â‰ˆ 41.94M                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**GQA (Grouped Query Attention) è¯´æ˜**ï¼š
```
æ ‡å‡† MHA:  Q=32å¤´, K=32å¤´, V=32å¤´  â†’  K/V å„éœ€ 16.78M å‚æ•°
GQA-8:    Q=32å¤´, K=8å¤´,  V=8å¤´   â†’  K/V å„åªéœ€ 4.19M å‚æ•°
                                      â†“
                              èŠ‚çœ 75% çš„ KV å‚æ•°ï¼
                              åŒæ—¶å‡å°‘ KV cache å¤§å°
```

##### 2.2 MLP éƒ¨åˆ† (SwiGLU)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       MLP å±‚ (SwiGLU)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â‘¤ Gate Projection (gate_proj)                              â”‚
â”‚     è¾“å…¥: hidden_size = 4096                                 â”‚
â”‚     è¾“å‡º: intermediate_size = 14336                          â”‚
â”‚     å‚æ•°: 4096 Ã— 14336 = 58,720,256 â‰ˆ 58.72M                â”‚
â”‚                                                              â”‚
â”‚  â‘¥ Up Projection (up_proj)                                  â”‚
â”‚     è¾“å…¥: hidden_size = 4096                                 â”‚
â”‚     è¾“å‡º: intermediate_size = 14336                          â”‚
â”‚     å‚æ•°: 4096 Ã— 14336 = 58,720,256 â‰ˆ 58.72M                â”‚
â”‚                                                              â”‚
â”‚  â‘¦ Down Projection (down_proj)                              â”‚
â”‚     è¾“å…¥: intermediate_size = 14336                          â”‚
â”‚     è¾“å‡º: hidden_size = 4096                                 â”‚
â”‚     å‚æ•°: 14336 Ã— 4096 = 58,720,256 â‰ˆ 58.72M                â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  MLP æ€»è®¡: 58.72M Ã— 3 = 176,160,768 â‰ˆ 176.16M               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä¸ºä»€ä¹ˆ intermediate_size = 14336ï¼Ÿ**
```
ä¼ ç»Ÿ FFN: intermediate = 4 Ã— hidden = 4 Ã— 4096 = 16384
SwiGLU:   å¤šäº†ä¸€ä¸ª gate_projï¼Œä¸ºä¿æŒè®¡ç®—é‡ä¸€è‡´ï¼Œ
          intermediate = (4 Ã— hidden Ã— 2) / 3 â‰ˆ 14336
          (å› ä¸º SwiGLU æœ‰ 3 ä¸ªæŠ•å½±çŸ©é˜µè€Œä¸æ˜¯ 2 ä¸ª)
```

##### 2.3 RMSNorm å±‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       RMSNorm å±‚                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â‘§ input_layernorm (Attention å‰)                           â”‚
â”‚     å‚æ•°: hidden_size = 4096                                 â”‚
â”‚                                                              â”‚
â”‚  â‘¨ post_attention_layernorm (MLP å‰)                        â”‚
â”‚     å‚æ•°: hidden_size = 4096                                 â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  RMSNorm æ€»è®¡: 4096 Ã— 2 = 8,192 â‰ˆ 8.19K                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

##### 2.4 å•å±‚æ±‡æ€»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              å•ä¸ª Transformer å±‚å‚æ•°é‡æ±‡æ€»                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚        ç»„ä»¶          â”‚     å‚æ•°é‡        â”‚       å æ¯”        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ q_proj               â”‚   16,777,216     â”‚       7.69%       â”‚
â”‚ k_proj               â”‚    4,194,304     â”‚       1.92%       â”‚
â”‚ v_proj               â”‚    4,194,304     â”‚       1.92%       â”‚
â”‚ o_proj               â”‚   16,777,216     â”‚       7.69%       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ **Attention å°è®¡**   â”‚ **41,943,040**   â”‚    **19.22%**     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ gate_proj            â”‚   58,720,256     â”‚      26.91%       â”‚
â”‚ up_proj              â”‚   58,720,256     â”‚      26.91%       â”‚
â”‚ down_proj            â”‚   58,720,256     â”‚      26.91%       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ **MLP å°è®¡**         â”‚ **176,160,768**  â”‚    **80.73%**     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ input_layernorm      â”‚        4,096     â”‚       ~0%         â”‚
â”‚ post_attn_layernorm  â”‚        4,096     â”‚       ~0%         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ **RMSNorm å°è®¡**     â”‚     **8,192**    â”‚      **~0%**      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ **å•å±‚æ€»è®¡**         â”‚ **218,112,000**  â”‚     **100%**      â”‚
â”‚                      â”‚   **(â‰ˆ218.1M)**  â”‚                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**å…³é”®å‘ç°**ï¼šMLP å å•å±‚å‚æ•°çš„ **~81%**ï¼ŒAttention åªå  **~19%**ï¼

---

#### ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ‰€æœ‰ Transformer å±‚

```
36 å±‚ Ã— 218,112,000 = 7,852,032,000 â‰ˆ 7.852B
```

---

#### ç¬¬å››éƒ¨åˆ†ï¼šè¾“å‡ºå±‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        è¾“å‡ºå±‚                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â‘© Final RMSNorm (norm)                                     â”‚
â”‚     å‚æ•°: hidden_size = 4096                                 â”‚
â”‚                                                              â”‚
â”‚  â‘ª Language Model Head (lm_head)                            â”‚
â”‚     è¾“å…¥: hidden_size = 4096                                 â”‚
â”‚     è¾“å‡º: vocab_size = 151936                                â”‚
â”‚     å‚æ•°: 4096 Ã— 151936 = 622,329,856 â‰ˆ 622.3M              â”‚
â”‚                                                              â”‚
â”‚     æ³¨: å¾ˆå¤šæ¨¡å‹ä¼šè®© lm_head ä¸ embed_tokens å…±äº«æƒé‡        â”‚
â”‚     (tie_word_embeddings=True)ï¼Œè¿™æ ·å¯èŠ‚çœ 622M å‚æ•°         â”‚
â”‚                                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  è¾“å‡ºå±‚æ€»è®¡: 4096 + 622,329,856 = 622,333,952 â‰ˆ 622.3M      â”‚
â”‚  (å¦‚æœæƒé‡å…±äº«ï¼Œåˆ™åªæœ‰ 4096)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### å®Œæ•´æ¨¡å‹å‚æ•°é‡æ±‡æ€»

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Qwen3-8B å®Œæ•´å‚æ•°é‡è®¡ç®—                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚            ç»„ä»¶              â”‚          å‚æ•°é‡              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ embed_tokens                 â”‚       622,329,856            â”‚
â”‚                              â”‚        (622.3M)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 36 Ã— Transformer Layer       â”‚     7,852,032,000            â”‚
â”‚   â”œâ”€ 36 Ã— Attention          â”‚     1,509,949,440 (1.51B)    â”‚
â”‚   â”œâ”€ 36 Ã— MLP                â”‚     6,341,787,648 (6.34B)    â”‚
â”‚   â””â”€ 36 Ã— RMSNormÃ—2          â”‚           294,912 (0.3M)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Final norm                   â”‚             4,096            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ lm_head (è‹¥ä¸å…±äº«)            â”‚       622,329,856            â”‚
â”‚                              â”‚        (622.3M)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ **æ€»è®¡ (ä¸å…±äº«æƒé‡)**         â”‚   **9,096,695,808**          â”‚
â”‚                              â”‚      **(â‰ˆ9.1B)**             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ **æ€»è®¡ (å…±äº«æƒé‡)**           â”‚   **8,474,365,952**          â”‚
â”‚                              â”‚      **(â‰ˆ8.47B)**            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æ³¨**ï¼šå®é™… Qwen3-8B ä½¿ç”¨ `tie_word_embeddings=False`ï¼ˆä¸å…±äº«ï¼‰ï¼Œæ‰€ä»¥æ€»å‚æ•°çº¦ **8.3B**ã€‚

---

#### å‚æ•°é‡åˆ†å¸ƒå¯è§†åŒ–

```
å‚æ•°é‡åˆ†å¸ƒï¼ˆä¸å…±äº«æƒé‡ï¼Œçº¦ 9.1Bï¼‰:

Embedding (622M)    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 6.8%

Attention (1.51B)   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 16.6%

MLP (6.34B)         â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 69.7%

lm_head (622M)      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 6.8%

å…¶ä»– (<1M)          â– ~0%
```

---

#### éªŒè¯è®¡ç®—

ä½ å¯ä»¥é€šè¿‡ä»£ç éªŒè¯ä¸Šè¿°è®¡ç®—ï¼š

```python
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-8B",
    torch_dtype="auto",
    trust_remote_code=True
)

# ç»Ÿè®¡å„éƒ¨åˆ†å‚æ•°é‡
def count_parameters(model):
    total = 0
    embed = 0
    attn = 0
    mlp = 0
    norm = 0
    lm_head = 0

    for name, param in model.named_parameters():
        num = param.numel()
        total += num

        if 'embed_tokens' in name:
            embed += num
        elif 'lm_head' in name:
            lm_head += num
        elif any(x in name for x in ['q_proj', 'k_proj', 'v_proj', 'o_proj']):
            attn += num
        elif any(x in name for x in ['gate_proj', 'up_proj', 'down_proj']):
            mlp += num
        elif 'norm' in name:
            norm += num

    print(f"Total:     {total:,} ({total/1e9:.2f}B)")
    print(f"Embedding: {embed:,} ({embed/1e6:.1f}M)")
    print(f"Attention: {attn:,} ({attn/1e9:.2f}B)")
    print(f"MLP:       {mlp:,} ({mlp/1e9:.2f}B)")
    print(f"Norm:      {norm:,} ({norm/1e3:.1f}K)")
    print(f"LM Head:   {lm_head:,} ({lm_head/1e6:.1f}M)")

count_parameters(model)
```

---

#### å…³é”®ç»“è®º

| å‘ç° | è¯´æ˜ |
|------|------|
| **MLP æ˜¯å‚æ•°å¤§æˆ·** | å æ€»å‚æ•°çš„ ~70%ï¼Œä¸»è¦ç”¨äºå­˜å‚¨çŸ¥è¯† |
| **Attention ç›¸å¯¹è½»é‡** | GQA è¿›ä¸€æ­¥å‡å°‘äº† KV æŠ•å½±çš„å‚æ•° |
| **Embedding ä¸å¯å¿½è§†** | è¯è¡¨å¤§ï¼Œembedding å æ¯” ~7% |
| **RMSNorm å‡ ä¹ä¸å ** | æ¯å±‚åªæœ‰ 8K å‚æ•° |
| **æƒé‡å…±äº«èŠ‚çœå‚æ•°** | å…±äº« embed/lm_head å¯çœ ~600M |

---
## å‚è€ƒèµ„æ–™

- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [The Illustrated GPT-2](https://jalammar.github.io/illustrated-gpt2/)


