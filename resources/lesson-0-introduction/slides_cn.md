---
marp: true
theme: default
paginate: true
size: 16:9
style: |
  section {
    font-family: 'PingFang SC', 'Microsoft YaHei', sans-serif;
  }
  pre {
    font-size: 0.7em;
  }
  h1 {
    color: #2563eb;
  }
  h2 {
    color: #1e40af;
  }
---

<!-- _class: lead -->
# AIOS: 从零构建 LLM 的"操作系统"

## 第0课：LLM概览和课程介绍

**GitHub 仓库**: [https://github.com/wyann22/aios](https://github.com/wyann22/aios)

📚 所有课件、代码和资源都会在此仓库持续更新

---

# 你是否想过？

- ChatGPT 是如何生成回复的？
- 70亿参数的模型如何在 GPU 上运行？
- 为什么推理优化如此重要？

**本课程目标**：亲手从零构建 LLM 推理框架

---

# 本课程适合谁？

| 角色 | 收获 |
|------|------|
| **软件工程师** | 理解 AI/ML 系统架构 |
| **AI infra从业者*** | 深入理解大模型推理 |
| **学生** | 可实践、可落地的项目，找工作|

---

# 你将收获什么
- 理解大模型架构
- 从零加载和运行 Llama/Qwen等模型
- 实现可运行的 LLM 推理引擎
- 掌握 KV-cache、TP/PP、AI编译等优化技术
...
---

<!-- _class: lead -->
# Part 1
## LLM 本质也是软件

---

# 软件的本质

```
┌─────────────────────────────────────────────────┐
│  传统软件                                        │
│  ┌────┐      ┌──────────┐      ┌────┐          │
│  │输入│ ───► │ 代码逻辑  │ ───► │输出│          │
│  └────┘      └──────────┘      └────┘          │
└─────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────┐
│  LLM                                            │
│  ┌────────┐   ┌──────────┐   ┌──────────┐      │
│  │Token输入│ ─► │神经网络层│ ─► │ 概率分布 │      │
│  └────────┘   └──────────┘   └──────────┘      │
└─────────────────────────────────────────────────┘
```

---

# 通用软件模型

**所有软件都遵循：输入 → 处理 → 输出**

| | 传统软件 | LLM |
|--|---------|-----|
| **输入** | 结构化数据（JSON、SQL） | Token序列（自然语言→数字） |
| **处理** | 执行代码逻辑 | 矩阵运算穿过神经网络层 |
| **输出** | 结构化数据 | 概率分布 → 下一个token |

---

# 输入/输出对比

| 方面 | 传统软件 | LLM |
|------|---------|-----|
| 输入格式 | API参数、数据库查询 | Token ID、Embeddings |
| 输出生成 | 一次性完整响应 | 自回归：逐个token生成 |
| 确定性 | 相同输入=相同输出 | 相同输入≠相同输出（除非temp=0） |

---

# 关键相似点

两者在硬件层面都是**确定性系统**：

- **传统软件**：CPU顺序执行指令
- **LLM**：GPU并行执行矩阵乘法

> LLM 中的"随机性"来自**采样策略**（温度、top-p），而非计算本身。
> 当 temperature=0 时，LLM 是完全确定的。

---

<!-- _class: lead -->
# Part 2
## LLM vs 传统软件

LLM也是运行在计算机上的软件，跟传统软件有何不同？

![llama2-70b](./images/llama-2-70b.png)

> "魔法"都在权重里

---

# Llama3: 典型的 LLM 架构

![Llama3](./images/Llama3.png)

> 💡 访问 [Hugging Face](https://hf-mirror.com/meta-llama/Meta-Llama-3-8B) 查看完整模型卡片

<!-- 如需添加实际图片，将图片放在 images/ 目录，然后使用: -->
<!-- ![Llama2 架构](images/llama2-architecture.png) -->

---

# 传统软件范式

```
输入 ──► 规则（代码） ──► 输出
```

**示例：拼写检查器**

```
"helo" ──► 编辑距离 + 字典查找 ──► "hello"
```

**特点**：
- ✓ 确定性（相同输入 = 相同输出）
- ✓ 可解释（可追踪代码路径）
- ✓ 易于调试和测试

---

# LLM 范式

```
输入 ──► 学习到的权重（参数）+ 网络架构 ──► 概率分布 ──► 输出
```

**示例：词义消歧**

```
"I deposited money at the bank" ──► LLM ──► bank = 银行 ✓（非河岸）
"The river bank was steep"      ──► LLM ──► bank = 河岸 ✓（非银行）
```

**特点**：
- ✗ 非确定性（温度、采样）
- ✗ 黑盒（权重编码模式）
- ✓ 能泛化到未见过的输入
- ✓ 自然处理歧义

---

# 并排对比

| 方面 | 传统软件 | LLM |
|------|---------|-----|
| 决策方式 | if-else、规则 | 矩阵乘法 + 学习到的权重 |
| 知识存储 | 数据库、文件 | 模型参数（权重/偏置） |
| 处理新情况 | 添加新代码/规则 | 已经能泛化 |
| 调试 | 断点、日志 | 注意力分析、探针 |
| 失败模式 | 崩溃、错误 | 幻觉、错误答案 |

---

# 具体示例：情感分析

## 传统方法

```python
positive_words = {"好", "棒", "优秀"}
negative_words = {"差", "糟糕", "难受"}

def analyze_sentiment(text):
    pos_count = sum(1 for word in positive_words if word in text)
    neg_count = sum(1 for word in negative_words if word in text)
    return "正面" if pos_count > neg_count else "负面"

# 局限性："这部电影还不错" → 可能误判
```

## LLM 方法

```python
prompt = f"分析以下文本的情感：'{text}'"
logits = model.forward(tokenizer.encode(prompt))
token_id = logits.argmax()
sentiment = tokenizer.decode(token_id)

# 能处理："这部电影还不错" → 正面
```

---

# 理解"黑盒"

**传统程序**：
```python
if word in dictionary:
    return correct_spelling(word)  # ← 你可以读懂这个！
```

**LLM**：
```python
weights = [0.0234, -0.1567, 0.8921, ...]  # 70亿个数字
output = input @ weights[layer1]
output = activation(output)  # ← 这些数字是什么意思？
```

**"知识"分布在数十亿个数字中，而非可读的代码**

---

<!-- _class: lead -->
# Part 3
## 计算和内存：根本性差异

---

# CPU vs GPU 架构对比

| 特性 | CPU | GPU |
|------|-----|-----|
| 设计理念 | 🎯 通用处理 | ⚡ 并行计算 |
| 核心数 | 8-64 个强大核心 | 10,000+ 个简单核心 |
| 缓存 | L1/L2/L3 大缓存 (MB级) | 小缓存 + HBM 高带宽内存 |
| 优化目标 | 低延迟、分支预测 | 高吞吐、数据并行 |
| 适用场景 | 传统软件、I/O密集 | LLM推理、矩阵运算 |

<!-- 如需添加架构图，将图片放在 images/ 目录，然后使用: -->
<!-- ![bg right:40%](images/cpu-vs-gpu.png) -->

---

# 传统软件：计算轻量

**典型 Web 服务器请求**：

| 步骤 | CPU周期 | 内存占用 |
|------|--------|---------|
| 解析HTTP请求 | ~1,000 | ~10 KB |
| 数据库查询 | ~10,000 | ~50 KB |
| 业务逻辑 | ~5,000 | ~20 KB |
| 渲染响应 | ~2,000 | ~30 KB |
| **总计** | **~20,000 (~7微秒)** | **~100 KB** |

**瓶颈**：I/O（磁盘、网络），而非计算

---

# LLM 推理：计算密集

**单个Token生成（Llama 7B）**：

| 每层操作 | 运算量 | 权重内存占用 (FP16) |
|---------|--------|-------------------|
| 注意力 QKV | 5000万 | ~200 MB |
| 注意力输出 | 1700万 | ~67 MB |
| FFN 上投影 | 4500万 | ~180 MB |
| FFN 下投影 | 4500万 | ~180 MB |
| **每层总计** | **~1.6亿** | **~630 MB** |
| **32层总计** | **~50亿** | **~20 GB** |

**额外内存需求**：
- KV缓存（seq_len=2048）：~2 GB
- 激活值和中间结果：~1-2 GB
- **总内存占用**：**~24 GB**

**瓶颈**：内存带宽 AND 计算

---

# 量化对比

| 指标 | 传统软件 | LLM推理 |
|------|---------|--------|
| 每请求计算量 | 数千 CPU周期 | 数十亿 FLOPs |
| 内存占用 | MB级别 | GB到TB级别 |
| 内存带宽需求 | 10-50 GB/s | **1-3 TB/s** |
| 计算模式 | 不规则、多分支 | 规则、密集矩阵乘法 |
| 并行性 | 任务级（线程） | 数据级（数千核心） |
| 功耗 | 50-200W | 300-700W（每GPU） |

---

<!-- _class: lead -->
# Part 4
## 硬件平台：CPU vs GPU

---

# CPU：为传统软件优化

```
┌──────────────────────────────────────────┐
│  Core 0    Core 1    Core 2    Core N    │
│  ┌─────┐  ┌─────┐   ┌─────┐   ┌─────┐   │
│  │ ALU │  │ ALU │   │ ALU │   │ ALU │   │
│  │ L1$ │  │ L1$ │   │ L1$ │   │ L1$ │   │
│  └─────┘  └─────┘   └─────┘   └─────┘   │
│         共享 L3 缓存 (30-100 MB)          │
└──────────────────────────────────────────┘
```

**特点**：少量强大核心（8-64个）、大缓存、分支预测、乱序执行

**适合**：不规则工作负载、低延迟、多分支代码

---

# GPU：为并行计算优化

```
┌──────────────────────────────────────────┐
│  SM 0      SM 1      SM 2    ...  SM N   │
│  ┌─────┐  ┌─────┐   ┌─────┐     ┌─────┐ │
│  │█████│  │█████│   │█████│     │█████│ │
│  │█████│  │█████│   │█████│     │█████│ │
│  └─────┘  └─────┘   └─────┘     └─────┘ │
│        Total: 10,000+ 简单核心           │
│         HBM 内存 (80GB @ 3TB/s)          │
└──────────────────────────────────────────┘
```

**特点**：数千简单核心、高带宽HBM、Tensor核心

**适合**：规则工作负载、大规模并行、GEMM

---

# 硬件对比

| 特性 | CPU | GPU/NPU |
|------|-----|---------|
| 核心数量 | 8-64 | 10,000+ |
| 核心复杂度 | 高（乱序、分支预测） | 低（简单ALU） |
| 时钟频率 | 3-5 GHz | 1-2 GHz |
| 内存带宽 | 50-200 GB/s | **1-3 TB/s** |
| 峰值算力 (FP16) | 1-5 TFLOPS | **300-1000 TFLOPS** |

---

<!-- _class: lead -->
# Part 5
## 推理引擎 = LLM 的"操作系统"

---

# 软件栈类比

| 传统计算 | AI计算 |
|---------|--------|
| 应用程序 (Chrome, Word) | LLM 模型 (Llama, Qwen) |
| ↓ | ↓ |
| **操作系统** (Linux, Windows) | **推理引擎** (vLLM, TGI) |
| ↓ | ↓ |
| CPU 硬件 | GPU/NPU 硬件 |

> **操作系统是软件和硬件之间的桥梁**
> **推理引擎是LLM和GPU之间的桥梁**

---

# 角色对照

| 操作系统角色 | 推理引擎角色 |
|-------------|-------------|
| 内存分配 | GPU内存管理 |
| 进程调度 | 请求批处理与调度 |
| 虚拟内存 | KV缓存分页（PagedAttention） |
| 设备驱动 | CUDA算子优化 |
| 文件系统缓存 | 前缀缓存 |
| 多进程隔离 | 多租户服务 |

---

# 没有推理引擎 vs 有推理引擎

## 朴素方法
```python
for request in requests:
    output = model.generate(request)  # 一次一个
# GPU利用率：10-20%  |  吞吐量：10 tokens/秒
```

## 使用推理引擎
```python
engine = InferenceEngine(
    model="llama-7b", tensor_parallel=2, max_batch_size=64
)
# GPU利用率：80%+  |  吞吐量：1000+ tokens/秒
```

---

<!-- _class: lead -->
# Part 6
## 课程目标

---

# 从零构建 LLM 的"操作系统"

| Layer | 内容 |
|-------|------|
| **Layer 1** | 模型加载与执行：加载权重、前向传播、分词与生成 |
| **Layer 2** | 内存管理：KV缓存、PagedAttention |
| **Layer 3** | 调度与批处理：持续批处理、请求调度、抢占策略 |
| **Layer 4** | 并行与优化：TP、PP、投机解码等|

**最终目标**：构建你自己的 "AIOS" — AI操作系统

---

# 为什么从零构建？

1. **深度理解**
   使用 vLLM 很容易；理解*为什么*它能工作让你不可替代

2. **调试能力**
   当生产环境出问题时，你需要理解每一层

3. **创新能力**
   下一个突破可能来自你

4. **职业发展**
   理解内部原理的AI infra工程师需求旺盛

---

# 学习路径

```
第0课：入门介绍 ← 你在这里！
    ↓
第1课：LLM基础（Transformer、注意力机制）
    ↓
第2课：使用PyTorch运行Llama/Qwen
    ↓
[未来] KV缓存 → 批处理 → 量化 → 高级优化
```

---

# 前置要求

**必需**：
- Python 编程（类、函数、数据结构）
- 基础线性代数（矩阵乘法、向量）

**有帮助但非必需**：
- PyTorch 基础
- 神经网络基础

**环境**：
- Python 3.8+ / PyTorch 2.0+
- CUDA GPU（16GB+ 显存）多卡环境

---

<!-- _class: lead -->
# 欢迎加入

---

# 资源获取

**GitHub 仓库**: [https://github.com/wyann22/aios](https://github.com/wyann22/aios)

所有内容包括：
- 📖 课程讲义和幻灯片
- 💻 完整代码实现
- 📝 练习题和解答
- 🔗 参考资料链接

**下一课**：[第1课 - LLM基础](../lesson-1-llm-basics/README.md)
