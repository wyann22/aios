---
marp: true
theme: default
paginate: true
size: 16:9
style: |
  section {
    font-family: 'PingFang SC', 'Microsoft YaHei', sans-serif;
  }
  pre {
    font-size: 0.7em;
  }
  h1 {
    color: #2563eb;
  }
  h2 {
    color: #1e40af;
  }
---

<!-- _class: lead -->
# AIOS: LLM 推理学习项目

## 第0课：LLM推理入门

从零构建 LLM 的"操作系统"

---

# 你是否想过？

- ChatGPT 是如何生成回复的？
- 70亿参数的模型如何在 GPU 上运行？
- 为什么推理优化如此重要？

**本课程目标**：亲手从零构建 LLM 推理框架

---

# 本课程适合谁？

| 角色 | 收获 |
|------|------|
| **软件工程师** | 理解 AI/ML 系统架构 |
| **ML从业者** | 深入理解推理而非仅仅训练 |
| **系统工程师** | GPU 编程和性能优化 |
| **学生** | 可实践、可落地的知识 |

---

# 你将构建什么

- 使用 PyTorch 实现可运行的 LLM 推理引擎
- 从零加载和运行 Llama/Qwen 模型
- 理解 Transformer 架构的每一层
- 掌握 KV-cache、TP/PP 等优化技术

---

<!-- _class: lead -->
# Part 1
## LLM vs 传统软件

---

# 传统软件范式

```
输入 ──► 规则（代码） ──► 输出
```

**示例：拼写检查器**

```
"helo" ──► 字典查找 + 编辑距离 ──► "hello"
```

**特点**：
- ✓ 确定性（相同输入 = 相同输出）
- ✓ 可解释（可追踪代码路径）
- ✓ 资源使用可预测
- ✓ 易于调试和测试

---

# LLM 范式

```
输入 ──► 学习到的权重（参数） ──► 概率分布 ──► 输出
```

**示例：语言理解**

```
"河岸很陡" ──► 神经网络（上下文分析） ──► "riverbank"
```

**特点**：
- ✗ 非确定性（温度、采样）
- ✗ 黑盒（权重编码模式）
- ✓ 能泛化到未见过的输入
- ✓ 自然处理歧义

---

# 并排对比

| 方面 | 传统软件 | LLM |
|------|---------|-----|
| 决策方式 | if-else、规则 | 矩阵乘法 + 学习到的权重 |
| 知识存储 | 数据库、文件 | 模型参数（权重/偏置） |
| 处理新情况 | 添加新代码/规则 | 已经能泛化 |
| 调试 | 断点、日志 | 注意力分析、探针 |
| 失败模式 | 崩溃、错误 | 幻觉、错误答案 |

---

# 具体示例：情感分析

## 传统方法

```python
positive_words = {"好", "棒", "优秀"}
negative_words = {"差", "糟糕", "难受"}

# 局限性："这部电影还不错" → 可能误判
```

## LLM 方法

```python
prompt = f"分析以下文本的情感：'{text}'"
logits = model.forward(tokenizer.encode(prompt))

# 能处理："这部电影还不错" → 正面
```

---

# 理解"黑盒"

**传统程序**：
```python
if word in dictionary:
    return correct_spelling(word)  # ← 你可以读懂这个！
```

**LLM**：
```python
weights = [0.0234, -0.1567, 0.8921, ...]  # 70亿个数字
output = input @ weights[layer1]
output = activation(output)  # ← 这个数字是什么意思？
```

**"知识"分布在数十亿个数字中，而非可读的代码**

---

<!-- _class: lead -->
# Part 2
## LLM 本质也是软件

---

# 通用软件模型

**所有软件都遵循：输入 → 处理 → 输出**

| | 传统软件 | LLM |
|--|---------|-----|
| **输入** | 结构化数据（JSON、SQL） | Token序列（自然语言→数字） |
| **处理** | 执行代码逻辑 | 矩阵运算穿过神经网络层 |
| **输出** | 结构化数据 | 概率分布 → 下一个token |

---

# 输入/输出对比

| 方面 | 传统软件 | LLM |
|------|---------|-----|
| 输入格式 | API参数、数据库查询 | Token ID、Embeddings |
| 输入大小 | 可变，通常KB级 | 固定上下文窗口（4K-128K） |
| 输出生成 | 一次性完整响应 | 自回归：逐个token生成 |
| 确定性 | 相同输入=相同输出 | 相同输入≠相同输出（除非temp=0） |

---

# 关键相似点

两者在硬件层面都是**确定性系统**：

- **传统软件**：CPU顺序执行指令
- **LLM**：GPU并行执行矩阵乘法

> LLM 中的"随机性"来自**采样策略**（温度、top-p），而非计算本身。
> 当 temperature=0 时，LLM 是完全确定的。

---

<!-- _class: lead -->
# Part 3
## 计算和内存：根本性差异

---

# 传统软件：计算轻量

**典型 Web 服务器请求**：

| 步骤 | CPU周期 |
|------|--------|
| 解析HTTP请求 | ~1,000 |
| 数据库查询 | ~10,000 |
| 业务逻辑 | ~5,000 |
| 渲染响应 | ~2,000 |
| **总计** | **~20,000 (~7微秒)** |

**瓶颈**：I/O（磁盘、网络），而非计算

---

# LLM 推理：计算密集

**单个Token生成（Llama 7B）**：

| 每层操作 | 运算量 |
|---------|--------|
| 注意力 QKV | 5000万 |
| 注意力输出 | 1700万 |
| FFN 上投影 | 4500万 |
| FFN 下投影 | 4500万 |
| **每层总计** | **~1.6亿** |
| **32层总计** | **~50亿** |

**瓶颈**：内存带宽 AND 计算

---

# 量化对比

| 指标 | 传统软件 | LLM推理 |
|------|---------|--------|
| 每请求计算量 | 数千 CPU周期 | 数十亿 FLOPs |
| 内存占用 | MB级别 | GB到TB级别 |
| 内存带宽需求 | 10-50 GB/s | **1-3 TB/s** |
| 计算模式 | 不规则、多分支 | 规则、密集矩阵乘法 |
| 并行性 | 任务级（线程） | 数据级（数千核心） |
| 功耗 | 50-200W | 300-700W（每GPU） |

---

# 优化策略对照

| 传统软件 | LLM 推理 |
|----------|----------|
| 缓存优化 | KV-cache复用 |
| 查询优化 | 批处理策略 |
| 连接池 | 持续批处理 |
| CDN / 负载均衡 | 张量并行 |
| 异步I/O | 投机解码 |

---

<!-- _class: lead -->
# Part 4
## 硬件平台：CPU vs GPU

---

# CPU：为传统软件优化

```
┌──────────────────────────────────────────┐
│  Core 0    Core 1    Core 2    Core N    │
│  ┌─────┐  ┌─────┐   ┌─────┐   ┌─────┐   │
│  │ ALU │  │ ALU │   │ ALU │   │ ALU │   │
│  │ L1$ │  │ L1$ │   │ L1$ │   │ L1$ │   │
│  └─────┘  └─────┘   └─────┘   └─────┘   │
│         共享 L3 缓存 (30-100 MB)          │
└──────────────────────────────────────────┘
```

**特点**：少量强大核心（8-64个）、大缓存、分支预测、乱序执行

**适合**：不规则工作负载、低延迟、多分支代码

---

# GPU：为并行计算优化

```
┌──────────────────────────────────────────┐
│  SM 0      SM 1      SM 2    ...  SM N   │
│  ┌─────┐  ┌─────┐   ┌─────┐     ┌─────┐ │
│  │█████│  │█████│   │█████│     │█████│ │
│  │█████│  │█████│   │█████│     │█████│ │
│  └─────┘  └─────┘   └─────┘     └─────┘ │
│        Total: 10,000+ 简单核心           │
│         HBM 内存 (80GB @ 3TB/s)          │
└──────────────────────────────────────────┘
```

**特点**：数千简单核心、高带宽HBM、Tensor核心

**适合**：规则工作负载、大规模并行、GEMM

---

# 硬件对比

| 特性 | CPU | GPU/NPU |
|------|-----|---------|
| 核心数量 | 8-64 | 10,000+ |
| 核心复杂度 | 高（乱序、分支预测） | 低（简单ALU） |
| 时钟频率 | 3-5 GHz | 1-2 GHz |
| 内存带宽 | 50-200 GB/s | **1-3 TB/s** |
| 峰值算力 (FP16) | 1-5 TFLOPS | **300-1000 TFLOPS** |

---

<!-- _class: lead -->
# Part 5
## 推理引擎 = LLM 的"操作系统"

---

# 软件栈类比

| 传统计算 | AI计算 |
|---------|--------|
| 应用程序 (Chrome, Word) | LLM 模型 (Llama, Qwen) |
| ↓ | ↓ |
| **操作系统** (Linux, Windows) | **推理引擎** (vLLM, TGI) |
| ↓ | ↓ |
| CPU 硬件 | GPU/NPU 硬件 |

> **操作系统是软件和硬件之间的桥梁**
> **推理引擎是LLM和GPU之间的桥梁**

---

# 角色对照

| 操作系统角色 | 推理引擎角色 |
|-------------|-------------|
| 内存分配 | GPU内存管理 |
| 进程调度 | 请求批处理与调度 |
| 虚拟内存 | KV缓存分页（PagedAttention） |
| 设备驱动 | CUDA算子优化 |
| 文件系统缓存 | 前缀缓存 |
| 多进程隔离 | 多租户服务 |

---

# 没有推理引擎 vs 有推理引擎

## 朴素方法
```python
for request in requests:
    output = model.generate(request)  # 一次一个
# GPU利用率：10-20%  |  吞吐量：10 tokens/秒
```

## 使用推理引擎
```python
engine = InferenceEngine(
    model="llama-7b", tensor_parallel=2, max_batch_size=64
)
# GPU利用率：80%+  |  吞吐量：1000+ tokens/秒
```

---

<!-- _class: lead -->
# Part 6
## 课程目标

---

# 从零构建 LLM 的"操作系统"

| Layer | 内容 |
|-------|------|
| **Layer 1** | 模型加载与执行：加载权重、前向传播、分词与生成 |
| **Layer 2** | 内存管理：KV缓存、PagedAttention |
| **Layer 3** | 调度与批处理：持续批处理、请求调度、抢占策略 |
| **Layer 4** | 并行与优化：TP、PP、量化、投机解码 |

**最终目标**：构建你自己的 "AIOS" — AI操作系统

---

# 为什么从零构建？

1. **深度理解**
   使用 vLLM 很容易；理解*为什么*它能工作让你不可替代

2. **调试能力**
   当生产环境出问题时，你需要理解每一层

3. **创新能力**
   下一个突破可能来自你

4. **职业发展**
   理解内部原理的AI基础设施工程师需求旺盛

---

# 学习路径

```
第0课：入门介绍 ← 你在这里！
    ↓
第1课：LLM基础（Transformer、注意力机制）
    ↓
第2课：使用PyTorch运行Llama/Qwen
    ↓
[未来] KV缓存 → 批处理 → 量化 → 高级优化
```

---

# 前置要求

**必需**：
- Python 编程（类、函数、数据结构）
- 基础线性代数（矩阵乘法、向量）

**有帮助但非必需**：
- PyTorch 基础
- 神经网络基础

**环境**：
- Python 3.8+ / PyTorch 2.0+
- CUDA GPU（8GB+ 显存）

---

<!-- _class: lead -->
# 练习 0.1

**问题**：将 "Hello, how are you?" 翻译成法语

1. 传统编程如何解决？
   - 需要什么字典/规则？语法怎么办？

2. LLM 如何处理？
   - 它学到了什么模式？
   - 为什么不需要显式语法规则？

---

<!-- _class: lead -->
# 欢迎加入
## 让我们一起构建不凡

下一课：[第1课 - LLM基础](../lesson-1-llm-basics/README.md)
